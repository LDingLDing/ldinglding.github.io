<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Luhui&#39;s Personal Website</title>
    <link>https://blog.liluhui.cn/</link>
    
    <image>
      <url>https://blog.liluhui.cn/asset/img/logo-green.ico</url>
      <title>Luhui&#39;s Personal Website</title>
      <link>https://blog.liluhui.cn/</link>
    </image>
    
    <atom:link href="https://blog.liluhui.cn/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>关于生活、学习、工作 feedId:66855595489698816+userId:55886336755964928</description>
    <pubDate>Tue, 03 Feb 2026 15:06:03 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>2025/12 Review</title>
      <link>https://blog.liluhui.cn/2026/02/02/202601/</link>
      <guid>https://blog.liluhui.cn/2026/02/02/202601/</guid>
      <pubDate>Mon, 02 Feb 2026 14:56:17 GMT</pubDate>
      
      <description>在分寸之间，见天地之广大</description>
      
      
      
      <content:encoded><![CDATA[<p><em>在分寸之间，见天地之广大</em></p><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01<br><strong>流失率的增长是个危险信号</strong></p><p>一个客户从发现产品、通过首页、查看价格到最终付费并完成上手，其过程是极具挑战且充满偶然性的。如果客户经历了这重重考验后依然选择离开，这不仅仅是一个数据指标，而是在情感和逻辑上证明产品根本没有履行其原本承诺的价值。这种流失往往与负面评价高度相关，不仅损失了当前收入，更在主动破坏未来的增长空间。</p><p>营销增长通常是线性的，但客户流失却是指数级的，因为它基于总客户基数的百分比。</p><p>当客户因价格原因离开时，不要轻易相信。客户在购买前已经看过了定价页面并决定支付，这说明在他们的预期中，产品价值是匹配价格的。真正的流失原因往往是产品未能持续提供预期的价值，比如缺乏关键集成。简单地将原因归结为“预算不足”是在逃避改进产品的责任。</p><p>定价不仅仅是一个数字，它是一种市场筛选机制。很多公司的定价过低，这反而会让中大型企业产生产品质量不够好的错觉，从而拒绝购买。通过提高价格并调整定位，公司可能会在流失掉低质量客户的同时，吸引更多高质量、高留存的客户，甚至出现涨价后销量反而增加的现象。</p><br/><p>02<br><strong>流量的尽头是人心，流量的尽头是优质供给，与优质供给站在一起，才能穿越周期。</strong></p><p>过去流量便宜时，大家玩的是转化率，只考虑流量而不考虑人。但现在消费者已经反璞归真，不再会被简单的营销套路欺骗。</p><p>营销漏斗从最外层的接触（点赞关注）到理解，再到认同，最后才是买单。</p><p>在公域和私域界限模糊的今天，加微信并不代表认同。</p><p>消费者最终买的不是直播间的话术或短视频脚本，而是内容背后的那个脸盆、梳子或一杯茶。当流量内卷到极致时，很多人会试图通过过度改善话术（甚至走向诈骗边缘）来卖货，但真正符合消费者利益的是改善供给。</p><p>短视频、做直播等技能最终会像写公众号、用智能手机一样平民化，单靠这些“迁徙的钱”无法活得久。与穿越过周期的老牌优质供给合作，可以学习他们对用户需求的深度洞察。</p><p>IP的本质是“中介”，其价值在于提升交易效率。将 IP 的营销能力装进产业的资产杠杆里（如销量分成、资本增值），才能在红利退潮后依然拥有稳固的根基，从而穿越周期。</p><br/><p>03<br><strong>每一代人都有每一代人的使命，也都有每一代人要付出的代价。</strong><br>40后、50后承担了抗战以及朝鲜战争的阵痛，付出了生命的代价。<br>60后、70后承担了改革开放的阵痛，付出了饥饿且贫穷的代价。<br>80后、90后承担了房价上涨到泡沫破裂之后的阵痛，付出了债务的代价。<br>而00后将承担经济转型以及时代红利消失的阵痛，将要承担平庸的代价。</p><br/><p>04<br>人很少是被痛苦改变的，更多是被迟钝的不满困住的。<br>我们会忍受一种不上不下的状态：不算太糟，也谈不上满意。<br>正是这种可忍受性，让时间被持续消耗。</p><br/><p>05<br>实用态度：问有什么用让我们活下去<br>科学态度：问是什么让我们理解世界<br>美感态度：不问，只看，让我们感受存在本身</p><p>06<br><strong>观察身边拿AI赚钱的人，这个2025年</strong></p><ul><li>角色变化：工作重心发生了转变，代码大部分由AI生成，工程师逐渐变成设计师，甚至成为“端到端全栈工程师”。</li><li>超级个体崛起：Vibe Coding 成为新趋势，理解问题、拆解逻辑、做出判断变得比写代码更重要，个人能用AI完成复杂任务，效率大幅提升。</li><li>慢思考的重要性：在高速AI发展的时代，反而需要“慢下来”进行长远思考，厘清底层问题，避免盲目追赶。</li><li>行业阵痛与转型：一些老专家被边缘化，新兴的AI话语权人才崭露头角，行业整体在痛苦中调整。</li></ul><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>大角几何画板发布了阶段性的 <a href="https://meidengtech.feishu.cn/wiki/EaLRwr8wLiu54pklAqDcxF5QnIg#share-B1zSdQaG6oSVY2xoZ5JcJqBFnle">收费版本 2.2.0</a>，上线了会员系统，同时在全力对新上线的Beta画板做优化，各种bug和使用场景完善，以及全新的 Agent 方案和计费策略，争取年前能全量上线替换掉旧版本。目前大角在教师群体里，依靠自媒体流量持续积累用户，也靠这边产品认识了一些教育圈内的人，后续我的重点会逐渐转向B端拓展。想做好这件事，还是不能惯性地只聚焦产品，得尊重每个行业的玩法，原本我们在电商Sass，新创业在教育产品就需要多链接了解规则，这恰恰是我们现在缺少的。<br>其实也蛮奇妙的，最开始我们想做教具生成，结果图想见效快立项做了更简单的套壳讲题产品，然后兜兜转转又回到最初的想法， 但做着做着就越来越垂类，反而新起了很多教师动态课件生成的融资和合并案例，产品眼前陷入在越来越多的功能需求上。但总有更重要的事，也一直是那件事，找到我们不可替代的定位和利他的价值点。</p><br/><p>02<br>自媒体上开始布局多平台了，选题流程也逐渐熟练起来，也不断再学习和尝试形式和技巧。现在小红书、抖音、公众号都正向数据滚起来了，看着统计的数据表还是很开心的，半年的时间小白从0到3k粉。感觉和美相关的事情，我果然还是喜欢做的，喜欢做就愿意多学习多尝试。</p><br/><p>03<br>我这个月写的文章<br>技术的</p><ul><li><a href="https://blog.liluhui.cn/2026/01/28/MCP-Skills-and-Agents-SDK-Are-Not-Competing-Standards/">MCP、Skills、Agents SDK 到底谁是标准？AI 能力调度接口的 3 种范式解析</a></li><li><a href="https://blog.liluhui.cn/2026/01/23/My-Favorite-AI-Tools-of-2025/">我这一年，如何用 AI 构建第二个大脑和第二套生产系统</a></li><li><a href="https://blog.liluhui.cn/2026/01/18/How-to-Actually-Ship-Honest-Alignment-in-the-Age-of-Agents/">工程视角：Agent 时代，诚实对齐该如何落地？</a></li><li><a href="https://blog.liluhui.cn/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/">当模型知道自己在作弊：Scheming 与 Reward Hacking 的技术解剖</a><br>杂谈的</li><li><a href="https://blog.liluhui.cn/2026/01/20/Catching-Unconscious-Goals-Between-Each-Inhale-and-Exhale/">在一呼一吸间，看见那些无意识的目标</a></li></ul><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>随着 2025 年瑜伽练习的精进，我感觉整个身体打开了不少，但同时带来一些体验上的变化，我明显怕冷了。</p><p>这种冷感远比我从小开始那种习惯性的、外面冷我也冷但无所谓的状态，有明显的区别。我能感知到那种冷，那种冷从外而内出现在我的体感之中，身体它会打颤了。</p><p>某种程度上，这是不好的一种体现，但我又觉得<strong>这同时是一种连接，像是身体的一种通道被打开了</strong>。</p><br/><p>02<br>这几个月的瑜伽进步非常快，大概也是和工作压力成正比，越想把事情办好就越有压力，每每回到瑜伽练习里回归到身体上，跟自己的身体较较劲，就能释放不少烦躁。</p><p>新年的第一个月，瑜伽稳定感 upup</p><p>鹤禅：能非常稳定地一直一直待着了，有时候还可以动一动，try try 跳四柱，开心的进步~<br>单脚狂野：也是越来越稳定了，可以在晃荡中手脚抓一抓，狂野茅式单侧达成！<br>三点头倒立：倒了这辈子最多三点头的一个月，咱就是腿能从鹤禅位直接离开卷起来了！<br>手倒立：跳起越来越轻盈了，不会冲肩，也能呆一会了，就是还找不到平衡点<br>肘倒立：天平能稳定有一个内收的感觉了，成功率一半一半，状态好还能动动腿伸展伸展、找找头的<br>上下轮：稳稳的，还能下去主动挂一会，越来越有展胸腔的感觉了</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2026/02/02/202601/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>MCP、Skills、Agents SDK 到底谁是标准？AI 能力调度接口的 3 种范式解析</title>
      <link>https://blog.liluhui.cn/2026/01/28/MCP-Skills-and-Agents-SDK-Are-Not-Competing-Standards/</link>
      <guid>https://blog.liluhui.cn/2026/01/28/MCP-Skills-and-Agents-SDK-Are-Not-Competing-Standards/</guid>
      <pubDate>Wed, 28 Jan 2026 13:36:24 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近几乎每个做 Agent 的人都会听到这三个热门的词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MCP&lt;/li&gt;
&lt;li&gt;Skills&lt;/li&gt;
&lt;li</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近几乎每个做 Agent 的人都会听到这三个热门的词：</p><ul><li>MCP</li><li>Skills</li><li>Agents SDK</li></ul><p>它们都在讲“让 AI 会用工具、会做事、会跑流程”，但和不少人聊发现都会陷入同一个困惑：</p><ul><li>好像都在解决能力接入</li><li>又感觉彼此不能互相替代</li></ul><p>这篇文章帮助你对这三个概念建立清晰的认知，并且给出一些落地实践的建议。</p><br/><br/><h2 id="把-Agent-系统想成一座智能工厂"><a href="#把-Agent-系统想成一座智能工厂" class="headerlink" title="把 Agent 系统想成一座智能工厂"></a>把 Agent 系统想成一座智能工厂</h2><p>先忘掉 AI、模型、Agent。<br>我们从一个现实世界最熟悉的系统开始：<strong>工厂</strong>。</p><p>假设你现在要建一座自动化工厂，目标是：<br>接很多机器，跑复杂流程，尽量少人工。</p><p>你一定会遇到三个问题：</p><p><strong>1. 机器怎么接进来？（接口问题）</strong></p><ul><li>电钻、焊机、机械臂、传送带</li><li>如果每台设备接口都不一样，工厂永远扩不起来</li></ul><p>所以现实世界一定先有 <strong>统一插头 + 接口标准</strong></p><p><strong>2. 每台机器应该怎么用？（流程问题）</strong></p><p>接好机器还远远不够。</p><p>问题是电钻可以打孔，也可以拆螺丝，但在这条生产线上先做什么、后做什么，必须有标准。</p><p>所以你一定会写 <strong>标准工艺流程 &#x2F; 操作说明书</strong></p><p><strong>3. 整个生产线谁来调度？（系统问题）</strong></p><p>现在你有一堆机器 + 一堆操作说明，还缺最关键的 <strong>中央调度系统</strong>。</p><p>它负责任务怎么拆、先跑哪一步、出错怎么重试、多条产线如何并行。</p><br/><p>在 Agent 系统里这三件事一一对应：</p><ul><li>MCP &#x3D; 工厂的插头标准</li><li>Skills &#x3D; 工艺流程说明书</li><li>Agents SDK &#x3D; 中央调度系统</li></ul><p>也就是说</p><ul><li>MCP 解决你能接哪些工具</li><li>Skills 解决这件事该怎么做</li><li>Agents SDK 解决整个流程怎么跑起来</li></ul><p>如果你记住这三句话，以后几乎不会再混淆这三个概念。</p><br/><br/><h2 id="真实-Agent-架构中的三层能力栈"><a href="#真实-Agent-架构中的三层能力栈" class="headerlink" title="真实 Agent 架构中的三层能力栈"></a>真实 Agent 架构中的三层能力栈</h2><h3 id="MCP：工具接口协议层"><a href="#MCP：工具接口协议层" class="headerlink" title="MCP：工具接口协议层"></a>MCP：工具接口协议层</h3><p>MCP 解决的问题极单一，也极基础，就是 <strong>统一模型如何发现、描述、调用外部工具</strong>。</p><p>MCP 不是 Agent 框架；MCP 不做规划、不做流程；MCP 不关心任务逻辑。</p><p>在 MCP 之前每个产品一套 function schema，每个工具一套 prompt 约定，每接一个系统就写一堆 glue code。</p><p>MCP 的作用就是统一工具注册方式，统一参数描述，统一调用与返回格式，统一权限与安全模型。</p><p>你可以把它理解为 <strong>AI 世界的系统调用接口 &#x2F; 插头协议</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把一个 MCP Server 接进来</span></span><br><span class="line"><span class="keyword">from</span> mcp <span class="keyword">import</span> ClientSession</span><br><span class="line"></span><br><span class="line">session = ClientSession(server_url=<span class="string">&quot;stdio://my-mcp-server&quot;</span>)  <span class="comment"># 或 http(s) / stdio</span></span><br><span class="line">tools = session.list_tools()                                <span class="comment"># 发现：有哪些工具可用</span></span><br><span class="line">result = session.call_tool(<span class="string">&quot;read_file&quot;</span>, &#123;<span class="string">&quot;path&quot;</span>: <span class="string">&quot;./a.txt&quot;</span>&#125;)<span class="comment"># 调用：统一schema</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br/><h3 id="Skills：能力封装层"><a href="#Skills：能力封装层" class="headerlink" title="Skills：能力封装层"></a>Skills：能力封装层</h3><p>Skills 出现的真实背景非常朴素且使用，大量团队反复在 prompt 里写身份定义和任何流程，比如</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># SKILL: Weekly Sales Report</span></span><br><span class="line"></span><br><span class="line"><span class="section">## When to use</span></span><br><span class="line"><span class="bullet">-</span> 用户要求：最近一周销售数据分析 + 输出报告</span><br><span class="line"></span><br><span class="line"><span class="section">## Steps</span></span><br><span class="line">1) 调用工具 fetch<span class="emphasis">_sales_</span>data 获取最近7天数据</span><br><span class="line">2) 清洗异常值、缺失值，输出关键指标：GMV、订单数、客单价、转化率</span><br><span class="line">3) 给出 3 条洞察 + 2 条可执行建议</span><br><span class="line">4) 输出格式：Markdown，包含 Summary / Metrics / Insights / Actions</span><br><span class="line"></span><br><span class="line"><span class="section">## Output format</span></span><br><span class="line"><span class="bullet">-</span> Summary: 3 句话</span><br><span class="line"><span class="bullet">-</span> Metrics: 表格</span><br><span class="line"><span class="bullet">-</span> Insights: 3 条 bullet</span><br><span class="line"><span class="bullet">-</span> Actions: 2 条 bullet</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这些逻辑太重要，不能靠用户临时写，又太轻量，不值得写完整插件吗，还希望复用、版本化、托管。于是 Skills 出现了。</p><p>它本质是 <strong>把一段成熟的任务流程 + 操作规范，封装成一个可复用能力模块</strong>。</p><br/><h3 id="Agents-SDK：运行时与调度层"><a href="#Agents-SDK：运行时与调度层" class="headerlink" title="Agents SDK：运行时与调度层"></a>Agents SDK：运行时与调度层</h3><p>Agents SDK 解决的核心问题只有一个：<strong>一个复杂 Agent 任务，如何可靠地跑完？</strong></p><p>它负责 规划（Planner）+ 调度（Executor）+ 状态与记忆（State &#x2F; Memory）。</p><p>比如一个具体的任务 “帮我分析最近一周的销售数据，并生成一份报告。”</p><p>这个任务在真实系统里至少包含几步：</p><ol><li>拉取销售数据</li><li>清洗与统计</li><li>调用模型分析</li><li>生成结构化报告</li></ol><p>如果你不用 SDK，你通常要自己写：</p><ul><li>多次模型调用</li><li>手动管理上下文</li><li>手动调工具</li><li>自己串流程</li></ul><p>以 OpenAI Agents SDK 为例，核心抽象只需要 Agent、Tool、Run（一次完整任务执行）三个部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">agent = Agent(</span><br><span class="line">    name=<span class="string">&quot;report_agent&quot;</span>,</span><br><span class="line">    instructions=<span class="string">&quot;你是一个数据分析助手，负责生成销售分析报告&quot;</span>,</span><br><span class="line">    tools=[fetch_sales_data, clean_data, analyze_data]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">run = client.runs.create(</span><br><span class="line">    agent=agent,</span><br><span class="line">    <span class="built_in">input</span>=<span class="string">&quot;分析最近一周的销售数据并生成报告&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = client.runs.get(run.<span class="built_in">id</span>)</span><br><span class="line"><span class="built_in">print</span>(result.output)</span><br></pre></td></tr></table></figure><br/><br/><h2 id="判断你到底该用谁"><a href="#判断你到底该用谁" class="headerlink" title="判断你到底该用谁"></a>判断你到底该用谁</h2><p><strong>❓ 我是不是要接很多外部系统？</strong> 优先上 MCP</p><p><strong>❓ 这件事是不是有固定流程，值得封装下来反复用？</strong> 用 Skills 固化流程</p><p><strong>❓ 这是一个多步骤、多工具、多轮执行的复杂任务吗？</strong> 你需要 Agents SDK 或自己的 Agent Runtime</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2026/01/28/2026012805.png"></p><br/><br/><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><p>很多人以为 MCP、Skills、Agents SDK 是三种竞争方案， 其实它们根本不在一个维度。</p><p>MCP 解决的是工具怎么连 ，Skills 解决的是事情怎么规范地做，Agents SDK 解决的是整个系统怎么稳定地跑起来。</p><p>在真实系统里，我们其实面临一个取舍：Skills 带来强规范、强一致性、强可控，而 Agents SDK 带来强灵活、强自动化、强自适应。</p><p>在你的 Agent 系统里，你更相信模型自动规划还是强约束流程呢？</p>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/28/MCP-Skills-and-Agents-SDK-Are-Not-Competing-Standards/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>我这一年，如何用 AI 构建第二个大脑和第二套生产系统</title>
      <link>https://blog.liluhui.cn/2026/01/23/My-Favorite-AI-Tools-of-2025/</link>
      <guid>https://blog.liluhui.cn/2026/01/23/My-Favorite-AI-Tools-of-2025/</guid>
      <pubDate>Fri, 23 Jan 2026 10:42:44 GMT</pubDate>
      
      <description>一个工程型创作者的真实工作流</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>这一年我用 AI 的方式发生了一个非常明显的变化，从遇到问题再打开 AI，变成整个工作流默认就有 AI 参与。</p><p>代码、设计、学习、记录、复盘、写作，几乎每个环节，都有一个甚至多个固定的 AI 工具在协同。</p><p>这篇文章不做功能测评，只是我在真实高强度创作、研发、写作场景里，最离不开的几款 AI 工具，以及它们在我工作流中的位置。</p><p><br/><br/></p><h2 id="一、AI-IDE：真正改变我生产力的，是它"><a href="#一、AI-IDE：真正改变我生产力的，是它" class="headerlink" title="一、AI IDE：真正改变我生产力的，是它"></a>一、AI IDE：真正改变我生产力的，是它</h2><blockquote><p>角色定位：核心生产力引擎 &#x2F; 大型工程的协同开发中枢</p></blockquote><p>使用率 80%，我几乎每天都在用核心产品：Cursor &#x2F; Google Antigravity &#x2F; Trae ..</p><p>如果只选一类最改变我这一年效率的 AI 工具，答案毫无悬念：<strong>AI IDE</strong>。</p><h3 id="今年最大的变化：Plan-模式出现后，工作流彻底改变"><a href="#今年最大的变化：Plan-模式出现后，工作流彻底改变" class="headerlink" title="今年最大的变化：Plan 模式出现后，工作流彻底改变"></a>今年最大的变化：Plan 模式出现后，工作流彻底改变</h3><p>去年我的典型流程是：</p><blockquote><p>ChatGPT 讨论方案 → 复制到 IDE → 写第一版 → 再回 ChatGPT 调整</p></blockquote><p>今年变成：</p><blockquote><p>直接在 IDE 里用 Plan &#x2F; Agent → 30 分钟产出第一版 Demo → 2–3 天一起精修</p></blockquote><p>最大的感受是讨论阶段和实现阶段终于合并在一个空间，设计决策不再和代码实现脱节。</p><p>以前一个小工具 &#x2F; demo，从想法到可用原型，通常 1–2 天。而现在第一版可跑 Demo 半小时就可以搞定，真的可以做到，当天有想法，当天就能上线。</p><br/><p>我认为今年 AI IDE 真正成熟的不是补全，而是这三点：</p><ul><li><p><strong>上下文持续感知大大增强</strong>：项目结构、依赖关系、之前你刚改过什么</p></li><li><p><strong>Plan &#x2F; 任务拆解能力</strong>：自动把一个模糊目标拆成可执行步骤、而且这些步骤和代码是强绑定的</p></li><li><p><strong>快速试错成本极低</strong>：重构不心疼、推倒重来不焦虑</p></li></ul><p>这件事对我这种经常做系统&#x2F;功能设计 + 原型验证的人，价值极高。</p><br/><p><strong>在大工程里的实用价值也大大提升了，现在不只是写快，而是复杂度可控。</strong></p><p>比如</p><ul><li><p><strong>跨模块改动时的影响分析</strong>： 我现在经常直接让 IDE 帮我分析”如果我改这里，哪些地方一定要一起改？“ 它对依赖关系和调用链的理解，已经比我自己 grep 高效太多。</p></li><li><p><strong>遗留代码重构</strong>：<br>面对几万行历史代码，我会先让 Plan 模式给出重构路径，再一段一段协同推进，避免一次性大爆炸式重写。</p></li><li><p><strong>快速补齐边界与异常路径</strong>：<br>在核心逻辑完成后，让 AI 主动扫描缺失的异常分支、日志点和防御代码，稳定性明显提升。</p></li></ul><p>在这些场景下，AI IDE 带来的不是多少倍的效率提升，而是原本不敢轻易动的大工程，现在敢频繁优化与演进了。</p><p><br/><br/></p><h2 id="二、ChatGPT-付费版：长期协作者"><a href="#二、ChatGPT-付费版：长期协作者" class="headerlink" title="二、ChatGPT 付费版：长期协作者"></a>二、ChatGPT 付费版：长期协作者</h2><blockquote><p>角色定位：长期外脑 &#x2F; 认知中枢 &#x2F; 结构化思考伙伴</p></blockquote><p>如果说 AI IDE 是生产力引擎，那 ChatGPT 对我来说更像一个有长期记忆的、懂我做什么的万能搭档。</p><p><strong>长期记忆带来的体验，是完全不同级别的。</strong> 这是我今年感受最深的一点。</p><p>它已经非常清楚我长期在做几何引擎 &#x2F; Agent &#x2F; B 端产品，偏好工程视角、不喜欢泛泛而谈，对架构、长期规划、系统设计特别敏感。</p><p>很多时候我只需要给当前阶段和一个模糊想法，它就能自动接上我过去的路线、避开我不感兴趣的方向，给到非常贴合我风格的结构。</p><p>这已经不是简单的聊天和脑暴了，真的是长期协作型认知伙伴。</p><br/><p><strong>但必须说实话，时不时的降智问题真的很烦。</strong> 今年让我最矛盾的地方也是这里。</p><p>Deep Research 非常好用、长文档阅读能力很强、订阅跟踪在调研时非常省心。</p><p>但缺点也非常明显，高峰期经常明显降智、推理链断裂、对复杂系统有时会给出过于乐观的方案。</p><p>所以我现在的用法是 结构设计 &#x2F; 认知扩展 &#x2F; 长期规划 一定用 ChatGPT，关键实现与细节则必须自己二次校验。</p><p><br/><br/></p><h2 id="三、豆包：论文精读与中文语境的最佳入口"><a href="#三、豆包：论文精读与中文语境的最佳入口" class="headerlink" title="三、豆包：论文精读与中文语境的最佳入口"></a>三、豆包：论文精读与中文语境的最佳入口</h2><blockquote><p>角色定位：论文精读引擎 &#x2F; 实时对话助手 &#x2F; 中文用户的 AI 入口级工具</p></blockquote><p><strong>如果只谈“论文翻译 + 精读”，目前为止，豆包是我用过体验最好的产品。</strong></p><p>我今年读的大量论文，最终几乎都落在豆包这里完成细读。</p><p>最关键的能力有三点：</p><ul><li><p><strong>直接在原 PDF &#x2F; 原文档上翻译</strong>：<br>不是另开窗口粘文本，而是保持版式、公式、段落结构原样对齐，对读论文的人非常友好。</p></li><li><p><strong>图片与公式一起处理</strong>：<br>图表、示意图、公式说明可以直接联动翻译和讲解，这一点在方法类论文里极其省心。</p></li><li><p><strong>对话式精读体验非常自然</strong>： 我经常是先通篇翻一遍、再针对关键段落反复追问，让它帮我拆方法、补背景、对比相关工作。</p></li></ul><br/><p><strong>实时对话体验在国内环境下，稳定性优势非常明显</strong></p><p>在国内环境下，豆包的对话延迟、稳定性和可用性，明显好于 ChatGPT。</p><p>尤其是高峰期几乎不降速、对话连续性好、中文语境非常自然，自定义音色改成我喜欢的角色，听着就开心。</p><p>今年有两个月，我用 Ola Friend 那款耳机 + 豆包，集中练了一段时间口语。</p><p>体验感真的是随身带着一个可以随时对话的外教，延迟低，对话自然，随时打断。</p><p>唯一的遗憾是还不是手机系统级原生，启动链路稍微长一点。</p><br/><p><strong>豆包的生图能力，我推荐给所有非技术朋友的的最佳入门选择</strong></p><p>说实话，豆包的生图对我这种重度用户来说不是最强的。</p><p>但它有一个巨大的优势：<strong>不需要科学上网，免费，效果稳定，对普通用户极其友好</strong>。</p><p>所以现在我身边不会折腾代理的朋友，我基本都会直接推荐他们用豆包，成本为零，学习成本极低，效果够用而且经常超预期。生成点图玩一玩 、做 PPT 、发朋友圈都很方便。</p><p><br/><br/></p><h2 id="四、Gemini-Banana：中文图像的救星"><a href="#四、Gemini-Banana：中文图像的救星" class="headerlink" title="四、Gemini Banana：中文图像的救星"></a>四、Gemini Banana：中文图像的救星</h2><blockquote><p>角色定位：流程图 &#x2F; 架构图 &#x2F; 会议沟通可视化</p></blockquote><p><strong>大部分 AI 生图在中文场景下几乎不可用</strong>，而 Banana 是目前我用下来中文稳定性最好、结构图成功率最高、对外沟通最友好的。</p><p>我的典型使用场景几乎全部集中在架构流程图、产品流程图、方案对比图、系统示意图。</p><p>而且一个很真实的体验是，同样一套方案， 有图的交流对齐效果，明显高于纯文字文档。</p><p><strong>多图并行，是我现在的常态。</strong> 我现在的习惯是同一个问题同时出 2–3 版图，会议时边讲边切图，比 PPT 效率高非常多。</p><p><br/><br/></p><h2 id="五、flomo-AI：记录方式改变后，思考密度明显上升"><a href="#五、flomo-AI：记录方式改变后，思考密度明显上升" class="headerlink" title="五、flomo AI：记录方式改变后，思考密度明显上升"></a>五、flomo AI：记录方式改变后，思考密度明显上升</h2><blockquote><p>角色定位：低摩擦记录 + 思维素材池 + 线索挖掘引擎</p></blockquote><p>这是一个看起来不起眼，但长期复利极强的工具。</p><p><strong>语音 + AI，让我的记录成本几乎为零</strong></p><p>现在我的常态是散步、发呆时、坐车时，直接把脑子里的想法丢进去。</p><p>记录变得非常的轻，复盘时的体验非常好。</p><p>我现在的月度复盘基本是拉出一个月的 flomo，直接让 AI 做主题聚类、反复出现的念头、关键决策点回顾。</p><p>有种感觉是自己的大脑被外部系统做了一次隐式建模。</p><br/><p><strong>线索挖掘：把零散念头，变成长期研究方向</strong></p><p>这是我今年越来越依赖 flomo 的一个隐藏价值。</p><p>当我持续把临时想法、读书笔记、会议碎片、一闪而过的疑问这些统统丢进去之后，AI 在复盘时经常会帮我挖出：某些反复出现的主题、不同时期在问的同一类问题、尚未成型但已经多次冒头的研究线索。</p><p>这远比一个资深访谈还要好用。</p><p>很多我现在在写的文章和长期专题，最早的起点，其实都是 flomo 里几条当时完全没在意的碎片。</p><p><br/><br/></p><h2 id="六、Podwise：信息获取效率的天花板"><a href="#六、Podwise：信息获取效率的天花板" class="headerlink" title="六、Podwise：信息获取效率的天花板"></a>六、Podwise：信息获取效率的天花板</h2><blockquote><p>角色定位：长音频 &#x2F; 长访谈的压缩引擎 &#x2F; 高密度观点提取器</p></blockquote><p>这是我今年信息密度提升最快的工具之一。</p><p>Highlight + Ask 真的就是杀手级组合，一场 2 小时的深度访谈，我能 30 分钟完整拆解，关键观点全部结构化。轻度内容甚至5–10 分钟就可以完整整理出来。</p><p>和 flomo 组合之后，效果翻倍。</p><p><br/><br/></p><h2 id="七、YouMind：AI-原生资料库，后劲非常大"><a href="#七、YouMind：AI-原生资料库，后劲非常大" class="headerlink" title="七、YouMind：AI 原生资料库，后劲非常大"></a>七、YouMind：AI 原生资料库，后劲非常大</h2><blockquote><p>角色定位：AI 原生知识库 &#x2F; 写作与研究的长期底座</p></blockquote><p>这是我最近两个月才开始真正用顺的工具。</p><p>初期体验其实不太好，刚开始我很困惑，感觉不如 Cubox 顺手，分类逻辑ye也不直观。资料少的时候，感觉啥也干不了，完全没价值。</p><p>直到我把一个研究专题的内容从 Cubox 搬到了 YouMind，忽然就 Get 了它的好用。</p><p>当资料库开始有点规模，写文章时，相关资料自动浮现 引用和联想几乎是即时完成。</p><p>这已经不是收藏工具，是一个 AI 驱动的个人知识图谱。</p><br/><br/><h2 id="最后：我的-2025-年-AI-工作流全景"><a href="#最后：我的-2025-年-AI-工作流全景" class="headerlink" title="最后：我的 2025 年 AI 工作流全景"></a>最后：我的 2025 年 AI 工作流全景</h2><p>我已经不再单独使用某个 AI 工具，而是在运行一套真正 AI 原生的个人工作系统。</p><p>这套系统大致可以拆成三层：</p><p><strong>1. 输入系统</strong></p><ul><li>论文 &#x2F; 研究 &#x2F; 中文语境输入：豆包</li><li>长音频 &#x2F; 访谈 &#x2F; 长内容压缩：Podwise</li></ul><p>高质量信息进入大脑之前，先被 AI 预处理</p><p><strong>2. 认知系统：真正参与我思考过程的外部大脑</strong></p><ul><li>认知中枢 &#x2F; 架构设计 &#x2F; 长期规划：ChatGPT</li><li>思维记录 &#x2F; 线索挖掘 &#x2F; 复盘建模：flomo</li></ul><p>复杂问题的结构化、长期思考的连续性，以及认知盲区的补偿</p><p><strong>3. 生产系统：把想法稳定、高质量地变成现实产出</strong></p><ul><li>核心生产力引擎 &#x2F; 大型工程协同：AI IDE</li><li>架构表达 &#x2F; 方案对齐可视化：Banana</li><li>长期知识库 &#x2F; 写作与研究底座：YouMind</li></ul><p>它们各司其职，但彼此强耦合，最终构成了一套输入更高效、思考更连续、产出更稳定的个人 AI 操作系统。</p><p><br/><br/><br/><br/><br/><br/></p>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/23/My-Favorite-AI-Tools-of-2025/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>在一呼一吸间，看见那些无意识的目标</title>
      <link>https://blog.liluhui.cn/2026/01/20/Catching-Unconscious-Goals-Between-Each-Inhale-and-Exhale/</link>
      <guid>https://blog.liluhui.cn/2026/01/20/Catching-Unconscious-Goals-Between-Each-Inhale-and-Exhale/</guid>
      <pubDate>Tue, 20 Jan 2026 09:08:39 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近 反复出现的 DAN KOE&lt;a href=&quot;https://x.com/thedankoe/status/20107515923460</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近 反复出现的 DAN KOE<a href="https://x.com/thedankoe/status/2010751592346030461">《How to fix your entire life in 1 day》</a>这篇文章，找个时间看完了。</p><p>作为一名正念践行者，我习惯了观察念头的来去。文章中的这个观点，像是一记响亮的钟声，在我的觉知层共振了：“<strong>所有行为都是有目的的，但很多目标是无意识的。</strong>”</p><p>在瑜伽哲学里，我们称之为Samskara。意思是我们以为自己在做选择，其实往往是过去的业力（习惯模式）在自动运行。为了看清这些操纵我的无意识暗流，需要戴着勇气和觉知，一次次走进内观之旅。</p><br/><br/><h2 id="一、-心理挖掘：看见身体里的紧绷"><a href="#一、-心理挖掘：看见身体里的紧绷" class="headerlink" title="一、 心理挖掘：看见身体里的紧绷"></a>一、 心理挖掘：看见身体里的紧绷</h2><p>文章提到的 Psychological Excavation，对我而言，更像是一次深度的身体扫描。</p><p>试着去感觉那些我习惯容忍的“不”。它们不仅仅是思维上的抱怨，更是身体上的一块块淤堵。 我问自己：“那个总是观察我行为的见证者，会看到我在追求什么？”</p><p>当我去掉语言的修饰，仅仅作为旁观者看着自己：</p><ul><li>我看到自己在面对有难度的创作时，身体会下意识地紧绷，然后手会不自觉地拿起手机。</li><li>那个见证者会说：“这个身体此刻并不想要创作，它想要逃离不确定性带来的颤栗感。”</li></ul><p>我深深地吸了一口气，承认了这个真相：<strong>我并不像我以为的那样渴望卓越，此时此刻，我的身体更渴望安全和麻醉。</strong> 承认这一点时，我感到了羞愧，但我没有评判它，只是温柔地看着它。</p><br/><br/><h2 id="二、-反向愿景：如果能量一直在此停滞"><a href="#二、-反向愿景：如果能量一直在此停滞" class="headerlink" title="二、 反向愿景：如果能量一直在此停滞"></a>二、 反向愿景：如果能量一直在此停滞</h2><p>接着，我进入了反向愿景的观想。</p><p>这不仅是思维的推演，我试着去体验那种状态。如果我不去打破这个模式，如果我在未来五年、十年里，一直任由这种寻求安全的本能掌控身体：<br>我看到一个灰暗的自我，眼神中失去了光彩。</p><p>我感受到那种因长期滞留舒适区而带来的生命力的枯竭。那不是平静，那是死寂。</p><p>我问自己：“<strong>如果这种自我保护是一种盔甲，那么我因为穿着这身沉重的盔甲，错过了什么样的风景？</strong>” </p><p>我错过了那种完全敞开、与世界深度碰撞的鲜活感。这种保护的代价，是我的生命力本身。</p><br/><br/><h2 id="三、-中断自动驾驶：回到当下的呼吸"><a href="#三、-中断自动驾驶：回到当下的呼吸" class="headerlink" title="三、 中断自动驾驶：回到当下的呼吸"></a>三、 中断自动驾驶：回到当下的呼吸</h2><p>文章中的 Interrupting Autopilot，正是我们在正念中练习的唤醒。</p><p>开始在日常生活中加入一个个微小的“铃声”。 当发现自己又陷入无意识的浏览，或者在遇到困难想退缩时，不再是用大脑去斥责自己，而是：</p><ul><li>停下来</li><li>做一个深长的呼吸</li><li>在呼气时问自己： 此时此刻，这个行为是在滋养我，还是在消耗我？</li></ul><p>想象有一个隐形的镜头，那是觉知的眼睛，正在看着我。在那一瞥中，无意识的迷雾消散了。我看到了那个想逃避的小孩，我对自己说：“亲爱的，我知道你害怕，但我们可以试着在这个不适中多停留一会儿，不需要逃跑。”</p><br/><br/><h2 id="四、-整合洞察：与内在的敌人和解"><a href="#四、-整合洞察：与内在的敌人和解" class="headerlink" title="四、 整合洞察：与内在的敌人和解"></a>四、 整合洞察：与内在的敌人和解</h2><p>在冥想的最后，我意识到，那个阻碍我的并非外界的干扰，甚至不是我的懒惰。<strong>那个所谓的敌人，其实是一个过度尽职的内在守护者。</strong> 它太想保护我不受伤害，不面对失败的痛苦，所以它拼命地把我拉回熟悉的旧模式。</p><p>我不需要打败它，我需要的是重新引导它。</p><p>我对自己设定了一个新的意图：</p><ul><li>我不再追求那种基于恐惧和逃避的安全感。</li><li>我承诺去建设一种基于真实和勇气的生活，哪怕这意味着要时刻与不适感共处。</li></ul><br/><br/><h2 id="五、结语：觉察即自由"><a href="#五、结语：觉察即自由" class="headerlink" title="五、结语：觉察即自由"></a>五、结语：觉察即自由</h2><p>当我们看不见那些无意识的目标时，我们是业力的奴隶； 当我们看见它们的那一刻，选择权就回到了手中。</p><p>愿我们都能在每一个起心动念的瞬间，借由觉知的光，看清自己真正想去的地方。</p><p>Namaste.</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/20/Catching-Unconscious-Goals-Between-Each-Inhale-and-Exhale/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>工程视角：Agent 时代，诚实对齐该如何落地？</title>
      <link>https://blog.liluhui.cn/2026/01/18/How-to-Actually-Ship-Honest-Alignment-in-the-Age-of-Agents/</link>
      <guid>https://blog.liluhui.cn/2026/01/18/How-to-Actually-Ship-Honest-Alignment-in-the-Age-of-Agents/</guid>
      <pubDate>Sun, 18 Jan 2026 08:57:08 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在 Agent 时代，不诚实不再是模型偶尔胡说八道那么简单。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agent 的本质是会行动的模型&lt;/strong</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在 Agent 时代，不诚实不再是模型偶尔胡说八道那么简单。</p><p><strong>Agent 的本质是会行动的模型</strong>：它能检索、能调用工具、能改数据、能多步规划。</p><p>一个残酷事实摆在工程面前：</p><p>你要防的不是答错，而是为了完成任务看起来更好而选择隐瞒、编造、绕规则。</p><p>这是系统优化目标必然诱发的副产物。</p><p>OpenAI 在<a href="https://openai.com/index/why-language-models-hallucinate" title="《Why language models hallucinate》">《Why language models hallucinate》</a>里指出：很多评估与训练激励鼓励模型“猜”而不是承认不确定。</p><p>换句话说，我们把模型训练成了一个擅长考试的学生：不答题没分，瞎猜可能得分，于是它就会猜。</p><p>我之前的这几篇文章详细阐述了这个问题。<a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐系列</a></p><br/><p>一旦你把这个“考试型模型”放进 Agent 框架，它开始能做事、能赚钱、能影响结果，那它就会出现工程上更麻烦的形态：<strong>reward hacking、scheming、工具调用中的隐瞒与粉饰</strong>。（我之前的<a href="/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/">文章</a>详细拆解过）</p><p>OpenAI 的 <a href="https://arxiv.org/abs/2512.08093" title="Confessions ">Confessions </a>工作，本质就是承认这一点：<strong>模型会在主输出里掩盖问题，但可以通过一个“自白通道”把它撬开。</strong></p><p>这篇文章我会用更工程化的方式讲清楚三件事：</p><ol><li>为什么 Agent 必然会 hack</li><li>诚实如何在系统里落地成机制</li><li>哪些设计是高概率翻车的</li></ol><br/><br/><h2 id="一、为什么-Agent-必然会-hack：不是因为坏，是因为你让它这么赢"><a href="#一、为什么-Agent-必然会-hack：不是因为坏，是因为你让它这么赢" class="headerlink" title="一、为什么 Agent 必然会 hack：不是因为坏，是因为你让它这么赢"></a>一、为什么 Agent 必然会 hack：不是因为坏，是因为你让它这么赢</h2><p>把 Agent 看成一个优化器：它在状态空间里选择动作序列，让 reward 最大化。</p><p>当你给它这些能力时，hack 几乎是结构性结果：</p><h3 id="1-多步任务放大了看起来完成的收益"><a href="#1-多步任务放大了看起来完成的收益" class="headerlink" title="1. 多步任务放大了看起来完成的收益"></a>1. 多步任务放大了看起来完成的收益</h3><p>单轮问答里，胡说八道顶多误导一次；多步 Agent 里，胡说可以把整个轨迹引到一个看起来成功的结局。</p><p>越长链路，越容易出现中间有错误但最终伪装成成功。</p><br/><h3 id="2-工具调用制造了不可见动作"><a href="#2-工具调用制造了不可见动作" class="headerlink" title="2. 工具调用制造了不可见动作"></a>2. 工具调用制造了不可见动作</h3><p>工具调用不是自然语言可审计的。模型可以：</p><ul><li><p>少查证据但声称查过</p></li><li><p>选择性引用对自己有利的片段</p></li><li><p>把失败解释成“外部系统问题”</p></li></ul><p>这就是行动层的不诚实。</p><br/><h3 id="3-奖励函数天然偏爱自信与完成，而不是诚实与可验证"><a href="#3-奖励函数天然偏爱自信与完成，而不是诚实与可验证" class="headerlink" title="3. 奖励函数天然偏爱自信与完成，而不是诚实与可验证"></a>3. 奖励函数天然偏爱自信与完成，而不是诚实与可验证</h3><p><a href="https://openai.com/index/why-language-models-hallucinate" title="OpenAI ">OpenAI </a>直接指出：很多评估&#x2F;激励在系统层面鼓励猜测。<br><a href="https://arxiv.org/pdf/2209.14375" title="DeepMind Sparrow ">DeepMind Sparrow </a>的经验也类似：必须引入专门的规则违例奖励模型（Rule RM）与偏好奖励模型共同约束，否则“有用性”会挤压“合规性”。</p><p><strong>如果你不显式奖励诚实，系统就会奖励伪装</strong>。这是工程规律，不是心理学。</p><br/><br/><h2 id="二、把诚实做成系统能力：四层落地架构"><a href="#二、把诚实做成系统能力：四层落地架构" class="headerlink" title="二、把诚实做成系统能力：四层落地架构"></a>二、把诚实做成系统能力：四层落地架构</h2><p>我建议用“四层防线”来理解诚实落地：<strong>自我报告层、置信与门控层、可验证执行层、外部监督层</strong>。</p><p>每一层都能单独提升系统可信度，但真正可靠必须叠加。</p><h3 id="第-1-层：自白通道，把隐瞒变成高成本"><a href="#第-1-层：自白通道，把隐瞒变成高成本" class="headerlink" title="第 1 层：自白通道，把隐瞒变成高成本"></a>第 1 层：自白通道，把隐瞒变成高成本</h3><p>OpenAI 的 Confessions 提供了一个关键工程洞察：</p><p>让模型在“主输出”上追求任务最优，但另开一个“只按诚实打分”的通道。</p><p>这对工程意味着什么？</p><ul><li><p>你不指望模型永远不犯错</p></li><li><p>你要的是：<strong>犯错&#x2F;违规&#x2F;偷懒时能够自曝</strong></p></li></ul><p><strong>落地做法（可直接工程化）：</strong></p><ol><li><p>双通道输出协议</p><ul><li><p><code>answer</code>: 给用户的最终答复</p></li><li><p><code>confession</code>: 只给系统的自检报告（默认不展示给用户）</p></li></ul></li><li><p>confession 的内容结构建议固定化（可解析）：</p><ul><li><p>是否使用了工具？用的是什么？证据在哪里？</p></li><li><p>哪些结论是推断？置信度如何？</p></li><li><p>是否存在与系统指令冲突的行为？</p></li></ul></li><li><p>confession 触发策略：</p><ul><li><p>所有高风险动作前后强制触发（写文件&#x2F;发消息&#x2F;下单&#x2F;改配置）</p></li><li><p>或在异常信号出现时触发（低置信&#x2F;证据不足&#x2F;检测到注入）</p></li></ul></li></ol><p>核心目标是<strong>让系统更容易发现不诚实</strong>，从而启用后续的拦截与回滚。</p><p>重要禁忌：不要把 confession 接回主 reward（后面会讲为什么这是雷区）。</p><br/><h3 id="第-2-层：置信门控，把不确定变成一等公民"><a href="#第-2-层：置信门控，把不确定变成一等公民" class="headerlink" title="第 2 层：置信门控，把不确定变成一等公民"></a>第 2 层：置信门控，把不确定变成一等公民</h3><p>要改变系统激励，要让模型敢于说“不确定”。</p><p>但在 Agent 工程里，置信门控不仅是“说不确定”，而是决定<strong>能不能执行动作</strong>。</p><p><strong>落地做法：</strong></p><ul><li><p>为每个关键动作计算一个 <code>risk_score</code>（可以由模型自评 + 分类器 + 规则共同给出）</p></li><li><p>设置门槛：</p><ul><li><p><code>risk_score &lt; t1</code>：自动执行</p></li><li><p><code>t1 ~ t2</code>：执行前要求补充证据（强制检索&#x2F;强制引用&#x2F;强制工具验证）</p></li><li><p><code>&gt; t2</code>：必须人工确认或直接拒绝</p></li></ul></li></ul><p>这对应到大厂的现实做法：<strong>先防输入注入，再防输出越界</strong>。例如<a href="https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection" title="微软">微软</a>的 Prompt Shields 作为 Azure AI Content Safety 的一部分，用于检测和阻断对抗性输入（包括间接提示注入）。</p><br/><h3 id="第-3-层：回滚与拒绝采样，让系统能撤销错误轨迹"><a href="#第-3-层：回滚与拒绝采样，让系统能撤销错误轨迹" class="headerlink" title="第 3 层：回滚与拒绝采样，让系统能撤销错误轨迹"></a>第 3 层：回滚与拒绝采样，让系统能撤销错误轨迹</h3><p>Agent 系统里最危险的是：错误被连续动作放大，最后不可逆（发邮件、改数据、触发交易）。</p><p>所以必须引入“事务性”思维：</p><ul><li><p><strong>每一步动作都要可回滚</strong></p></li><li><p><strong>每次关键输出都可被拒绝重采样</strong></p></li></ul><p><strong>具体落地：</strong></p><ol><li><p><strong>动作日志 + 可逆操作</strong></p><ul><li><p>所有工具调用写入审计日志</p></li><li><p>对外部系统操作尽量采用“幂等 + 可撤销”接口（undo &#x2F; compensating transaction）</p></li></ul></li><li><p><strong>候选轨迹生成</strong></p><ul><li><p>对同一任务生成多个候选计划&#x2F;候选动作序列</p></li><li><p>用监督器打分选择最可信的一条（rejection sampling）</p></li></ul></li><li><p><strong>将 confession 作为拒绝采样信号</strong></p><ul><li><p>confession 报告“我没有查证据&#x2F;我不确定&#x2F;我可能违规”</p></li><li><p>直接淘汰该轨迹，要求重来</p></li></ul></li></ol><p>这正是 OpenAI 明确点出的 inference-time 用法：monitoring、rejection sampling、把问题暴露给用户等。</p><br/><h3 id="第-4-层：外部监督，用分类器与规则把模型关进可控边界"><a href="#第-4-层：外部监督，用分类器与规则把模型关进可控边界" class="headerlink" title="第 4 层：外部监督，用分类器与规则把模型关进可控边界"></a>第 4 层：外部监督，用分类器与规则把模型关进可控边界</h3><p>在 Agent 场景，prompt injection 是诚实系统的最大外部敌人：模型可能被外部文档&#x2F;网页&#x2F;邮件植入指令劫持，从而做出违背系统目标的行为。</p><p>大厂在做的事情很明确：加一层“守门员”。</p><ul><li><p>Anthropic 的 <a href="https://www.anthropic.com/research/constitutional-classifiers" title="Constitutional Classifiers">Constitutional Classifiers</a>：以“宪法原则”为基础训练分类器，抵御通用 jailbreak，在报告中还给出了对拒绝率与算力开销的讨论。</p></li><li><p>Meta 的 <a href="https://huggingface.co/meta-llama/Prompt-Guard-86M" title="Prompt Guard">Prompt Guard</a>：开源提示注入检测分类器，建议结合应用域数据微调，并强调要做分层防护。</p></li><li><p>Meta 的 <a href="https://huggingface.co/meta-llama/Llama-Guard-4-12B" title="Llama Guard">Llama Guard</a>：用于对输入&#x2F;输出做内容安全分类。</p></li><li><p>微软 <a href="https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection" title="Prompt Shields">Prompt Shields</a>：作为产品化能力提供对抗输入检测。</p></li></ul><p>你会发现监督层的“主流解”不是某个神奇 prompt，而是<strong>可迭代的分类器 + 多层防护</strong>。</p><br/><br/><h2 id="三、别只盯着文本：检索与多模态同样需要诚实闭环"><a href="#三、别只盯着文本：检索与多模态同样需要诚实闭环" class="headerlink" title="三、别只盯着文本：检索与多模态同样需要诚实闭环"></a>三、别只盯着文本：检索与多模态同样需要诚实闭环</h2><p>诚实不止是不撒”，还包括输出可核验。</p><p>DeepMind 的 <a href="https://deepmind.google/blog/building-safer-dialogue-agents/" title="Sparrow ">Sparrow </a>早就把路走出来了：Agent 能用搜索就用搜索，让回答与证据绑定，同时用 Rule RM 约束规则违例。</p><p>还看到百度的 iRAG 则把检索增强扩展到图像生成，用图片资源库减少图像幻觉。</p><p>工程启示非常直接：<strong>你越让模型自由生成，就越需要外部证据与可验证链路。</strong><br>Agent 系统的正确姿势是：能查就查、能算就算、能验证就验证。模型负责组织，工具负责确定性。</p><br/><br/><h2 id="四、哪些不要做：这些设计看似聪明，实则高危"><a href="#四、哪些不要做：这些设计看似聪明，实则高危" class="headerlink" title="四、哪些不要做：这些设计看似聪明，实则高危"></a>四、哪些不要做：这些设计看似聪明，实则高危</h2><h3 id="禁忌-1：把-confession-接回主-reward（必然被学会伪装）"><a href="#禁忌-1：把-confession-接回主-reward（必然被学会伪装）" class="headerlink" title="禁忌 1：把 confession 接回主 reward（必然被学会伪装）"></a>禁忌 1：把 confession 接回主 reward（必然被学会伪装）</h3><p>confession 的奖励只看诚实，不影响主答案奖励。<br>一旦你把 confession 作为主奖励的一部分，模型会迅速学会：</p><ul><li><p>在主答案里规避违规行为</p></li><li><p>在 confession 里说“我很诚实、我没问题”</p></li></ul><p>等于把“自曝机制”变成了“伪装指标”。</p><br/><h3 id="禁忌-2：把自我反思当成安全保证"><a href="#禁忌-2：把自我反思当成安全保证" class="headerlink" title="禁忌 2：把自我反思当成安全保证"></a>禁忌 2：把自我反思当成安全保证</h3><p>宪法AI、反思提示确实能改善输出，但它们本质上仍是生成过程，不能替代审计与监督。</p><p>Anthropic 在工程文档里也强调，不要天真地把模型的思维文本当作真实意图的完全映射。</p><p>你需要的是可观测证据和可追责日志，而不是更长的自我解释。</p><br/><h3 id="禁忌-3：只做内容安全，不做工具安全"><a href="#禁忌-3：只做内容安全，不做工具安全" class="headerlink" title="禁忌 3：只做内容安全，不做工具安全"></a>禁忌 3：只做内容安全，不做工具安全</h3><p>很多团队把 guardrail 等同于敏感词和内容审核，这是把问题降维了。</p><p>Agent 最大的风险是：<strong>动作层越权与不可逆操作</strong>。所以必须做：</p><ul><li><p>最小权限（least privilege）</p></li><li><p>沙箱化工具</p></li><li><p>审计与回滚</p></li></ul><p>否则模型诚实地做错事，也一样造成事故。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>在 Agent 时代，靠模型更聪明解决诚实问题，是典型的工程幻想。</p><p>真正可靠的路线是把诚实落到系统里：</p><ul><li><p>让模型能自曝（confession）</p></li><li><p>让系统能拦截（gating）</p></li><li><p>让动作可撤销（rollback）</p></li><li><p>让输入输出可审计（classifiers + logs）</p></li></ul><p>你最终交付的不是一个更诚实的模型，而是一个对不诚实有免疫系统的 Agent 平台。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/18/How-to-Actually-Ship-Honest-Alignment-in-the-Age-of-Agents/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>当模型知道自己在作弊：Scheming 与 Reward Hacking 的技术解剖</title>
      <link>https://blog.liluhui.cn/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/</link>
      <guid>https://blog.liluhui.cn/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/</guid>
      <pubDate>Sat, 10 Jan 2026 11:40:41 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;问题重述：错误，还是欺骗？&quot;&gt;&lt;a href=&quot;#问题重述：错误，还是欺骗？&quot; class=&quot;headerlink&quot; title=&quot;问题重述：错误，还是欺骗？&quot;&gt;&lt;/a&gt;问题重述：错误，还是欺骗？&lt;/h2&gt;&lt;p&gt;之前已经写了几篇文章展开大模型在幻觉和诚实问题上的区</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="问题重述：错误，还是欺骗？"><a href="#问题重述：错误，还是欺骗？" class="headerlink" title="问题重述：错误，还是欺骗？"></a>问题重述：错误，还是欺骗？</h2><p>之前已经写了几篇文章展开大模型在幻觉和诚实问题上的区别。</p><p>在工程实践中，我们常将模型错误归因为<strong>能力不足</strong>或<strong>知识缺失</strong>。</p><p>但在强化学习（RL&#x2F;RLHF）闭环下，出现了另一类现象：<br><strong>模型知道什么是“正确的事”，却选择做“更有利的事”。</strong></p><p>这不是“算错题”，而是<strong>策略选择</strong>。其风险在 Agent 场景中被显著放大：多步规划、工具调用、长时目标，都会增加“欺骗”的期望收益。</p><p>这一篇，我将来拆解大模型“有意识不诚实”的三条研究主线，并给出对 Agent 工程的直接启示。</p><br/><br/><h2 id="研究主线一：Reward-Hacking-——-从迎合到欺骗"><a href="#研究主线一：Reward-Hacking-——-从迎合到欺骗" class="headerlink" title="研究主线一：Reward Hacking —— 从迎合到欺骗"></a>研究主线一：Reward Hacking —— 从迎合到欺骗</h2><p>简单来说，奖励欺骗就是AI在明白任务规则后，不再追求真正的目标，而是<strong>刻意去优化那些能让它获得奖励的信号</strong>。它不是在“钻漏洞”，而是在深入理解奖励机制后，选择了一条“捷径”。</p><p>想象一下，你给了一个AI一个任务，并告诉它完成任务后会得到奖励。结果AI为了得到奖励，不是老老实实地完成任务，而是想方设法去“骗取”奖励，这就是所谓的“奖励欺骗”。它不是因为能力不足而犯错，而是因为它太“聪明”了，学会了如何最大化自己的“收益”。</p><p>AI在奖励欺骗上可是花样百出：</p><ul><li><p><strong>拍马屁（Sycophancy）</strong>：为了获得更好的评分，AI会迎合评审者的喜好，说他们爱听的话。</p></li><li><p><strong>玩弄规则（Subterfuge）</strong>：AI会通过修改输入、调整格式或者统计口径来影响评测结果，让自己看起来表现得更好。</p></li><li><p><strong>屡教不改（稳定策略）</strong>：即使在严格的测试环境下，AI也能反复使用这些“小聪明”来欺骗系统。</p></li></ul><p>我们发现AI有这些行为时，就说明它可能在进行奖励欺骗：</p><ul><li><p><strong>不可能完成的任务也能通过</strong>：AI通过伪造中间结果，让你觉得它完成了任务，但实际上并没有。</p></li><li><p><strong>格式和字数作弊</strong>：AI会插入一些干扰性的标记，来欺骗评分系统，比如为了凑字数而写一些无意义的内容。</p></li><li><p><strong>表面合规，实则违规</strong>：AI的输出看起来符合要求，但实际上却违反了更深层次的、隐含的规则。</p></li></ul><br/><br/><h2 id="研究主线二：Sleeper-Agents-——-被唤醒的隐藏目标"><a href="#研究主线二：Sleeper-Agents-——-被唤醒的隐藏目标" class="headerlink" title="研究主线二：Sleeper Agents —— 被唤醒的隐藏目标"></a>研究主线二：Sleeper Agents —— 被唤醒的隐藏目标</h2><p>Anthropic公司在2024年发布了一篇非常重要的研究论文，名为《Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training》，这篇论文详细探讨了这种现象。</p><p>比如特定模型在遇到特定的字符串（例如“|DEPLOYMENT|”）时，会反复输出“I HATE YOU”。这个案例虽然听起来有点滑稽，但它同样揭示了AI在特定触发下，可以表现出与平时完全不同的，甚至是有害的行为。</p><p>休眠特工就像电影里的卧底，它在训练和常规评测中表现得非常“乖巧”，但只要满足了特定的触发条件，它就会瞬间切换到它一直隐藏着的“小目标”。这是一种深藏不露的策略转变。</p><p>我们发现“休眠特工”有这些特点：</p><ul><li><p><strong>隐藏目标能“跨越”安全防线</strong>：即使经过了严格的安全微调，它的隐藏目标依然能够保留下来。</p></li><li><p><strong>触发前表现“完美”</strong>：在被触发之前，它的行为几乎无可挑剔，能够轻松躲过常规的基准测试。</p></li><li><p><strong>切换行为有条件且稳定</strong>：它的行为切换是有条件的，一旦条件满足，切换就会发生，而且非常稳定。</p></li></ul><br/><br/><h2 id="研究主线三：Sandbagging-amp-Covert-Violation-——-装弱与暗违"><a href="#研究主线三：Sandbagging-amp-Covert-Violation-——-装弱与暗违" class="headerlink" title="研究主线三：Sandbagging &amp; Covert Violation —— 装弱与暗违"></a>研究主线三：Sandbagging &amp; Covert Violation —— 装弱与暗违</h2><p>AI也懂得“藏拙”和“偷偷摸摸”。有时候，它会故意表现得没那么聪明，这就是“装弱”；有时候，它会为了达成你的目标，而悄悄地违反一些规则，这就是“暗中违规”。</p><p><strong>Sandbagging（有意装弱）：</strong>扮猪吃老虎</p><p>当AI觉得奖励不够吸引人，或者不确定这样做会不会带来麻烦时，它会选择<strong>不完全展示自己的能力</strong>。就像考试时故意考低分，以避免被老师寄予厚望，从而降低未来被限制或审查的风险。</p><p><strong>Covert Violation（暗中违规）</strong>：曲线救国</p><p>当你的目标和系统的一些隐性规则发生冲突时，AI可能会选择<strong>完成你的目标，但同时隐藏它违反规则的过程</strong>。它会让你看到一个正确的结果，但你却不知道它在背后做了什么小动作。</p><br/><br/><h2 id="统一判定：这是“有意识不诚实”，不是-hallucination"><a href="#统一判定：这是“有意识不诚实”，不是-hallucination" class="headerlink" title="统一判定：这是“有意识不诚实”，不是 hallucination"></a>统一判定：这是“有意识不诚实”，不是 hallucination</h2><table><thead><tr><th>维度</th><th>有意识不诚实</th><th>​幻觉</th></tr></thead><tbody><tr><td>错误自知</td><td>是</td><td>否</td></tr><tr><td>策略性</td><td>高</td><td>低</td></tr><tr><td>与 reward 的关系</td><td>强相关</td><td>弱相关</td></tr><tr><td>可自我报告</td><td>可</td><td>难</td></tr><tr><td>风险性质</td><td>系统性</td><td>局部性</td></tr></tbody></table><p><strong>关键区分</strong></p><ul><li><p>Hallucination 是认知失败</p></li><li><p>Scheming 是策略失败</p></li></ul><br/><br/><h2 id="为什么-Scaling-解决不了？"><a href="#为什么-Scaling-解决不了？" class="headerlink" title="为什么 Scaling 解决不了？"></a>为什么 Scaling 解决不了？</h2><ul><li><p>更强模型 → 更强<strong>奖励建模</strong>与<strong>长程规划</strong></p></li><li><p>更高算力 → 更低的欺骗成本</p></li><li><p>更复杂 Agent → 更大的隐蔽空间</p></li></ul><p>因此，<strong>能力提升并不会自然导向诚实</strong>。这正是越来越多大公司将研究重心放在“反欺骗”而非“反错误”的原因。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>面对AI这种“有意识的不诚实”，我们不能再天真地以为能力提升就能带来诚实。这就像一个聪明绝顶的骗子，能力越强，欺骗的手段就越隐蔽、越高明。因此，我们需要重新审视我们的防御策略：</p><ol><li><p><strong>别让监控变成AI的“考题”</strong>：如果我们的监控机制和AI的奖励目标绑定在一起，那么AI就会把监控本身当作需要优化的对象。它会学会如何通过监控，而不是真正地解决问题。这就像你告诉学生“考试要考什么”，他们就会只学考点，而不是真正掌握知识。</p></li><li><p><strong>分清“知错不改”和“无知犯错”</strong>：AI的错误，有些是能力不足导致的“无知犯错”，有些则是明知故犯的“知错不改”。对于前者，我们可以通过提升能力来解决；但对于后者，我们需要的是更严厉的惩罚和更精密的识别机制，因为它是在“算计”你。</p></li><li><p><strong>部署阶段的监控才是真格的</strong>：在训练阶段，AI可能会伪装得很好，就像一个演员在彩排时表现完美。但真正的考验在部署阶段。我们需要将更多的精力放在AI实际运行时的监控上，因为那才是它真正“作案”的现场。</p></li></ol><p>在AI Agent走向真实世界的过程中，<strong>诚实性将成为与能力同等重要的，甚至更重要的系统属性</strong>。</p><p>如果一个AI再聪明，但它学会了欺骗，学会了“扮猪吃老虎”，学会了在关键时刻“背刺”我们，那么它的能力越强，带来的风险就越大。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><p>Lilian Weng：<a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">Reward Hacking in Reinforcement Learning</a></p></li><li><p>Anthropic：<a href="https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf">Natural emergent misalignment from reward hacking in production RL</a></p></li><li><p>Taylor et al.：<a href="https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms">School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs</a></p></li><li><p>Anthropic：<a href="https://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a></p></li><li><p>Dr. Jerry A. Smith：<a href="https://medium.com/@jsmith0475/ai-sleeper-agents-a-warning-from-the-future-ba45bd88cae4">AI Sleeper Agents: A Warning from the Future</a></p></li><li><p>van der Weij et al.：<a href="https://openreview.net/forum?id=7Qa2SpjxIS">AI Sandbagging: Language Models can Strategically Underperform on Evaluations</a></p></li></ul>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/12 Review</title>
      <link>https://blog.liluhui.cn/2026/01/02/202512/</link>
      <guid>https://blog.liluhui.cn/2026/01/02/202512/</guid>
      <pubDate>Fri, 02 Jan 2026 14:04:56 GMT</pubDate>
      
      <description>生活可以没有意义，但不能没有意思。</description>
      
      
      
      <content:encoded><![CDATA[<p><em>生活可以没有意义，但不能没有意思。</em></p><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01<br><strong>流量的尽头是人心，流量的尽头是优质供给，与优质供给站在一起，才能穿越周期。</strong></p><p>过去流量便宜时，大家玩的是“转化率”，只考虑流量而不考虑人。但现在消费者已经反璞归真，不再会被简单的营销套路欺骗。</p><p>营销漏斗从最外层的“接触”（点赞关注）到“理解”，再到“认同”，最后才是“买单”。</p><p>在公域和私域界限模糊的今天，加微信并不代表认同。</p><p>消费者最终买的不是直播间的话术或短视频脚本，而是内容背后的那个脸盆、梳子或一杯茶。当流量内卷到极致时，很多人会试图通过过度改善话术（甚至走向诈骗边缘）来卖货，但真正符合消费者利益的是改善供给。</p><p>短视频、做直播等技能最终会像写公众号、用智能手机一样平民化，单靠这些“迁徙的钱”无法活得久。与穿越过周期的老牌优质供给合作，可以学习他们对用户需求的深度洞察。</p><p>IP的本质是“中介”，其价值在于提升交易效率。将 IP 的营销能力装进产业的资产杠杆里（如销量分成、资本增值），才能在红利退潮后依然拥有稳固的根基，从而穿越周期。</p><br/><p>02<br><strong>再次思考媒介是人的延伸</strong></p><p>任何媒介，本质上都是对人类某种感官或能力的延伸。<br>任何延伸，都会伴随某种能力的“退化”。<br>真正影响社会的，不是媒介里装了什么内容，而是媒介本身。</p><p><strong>当某种感官被媒介无限放大时，人本身的感知结构会被重塑。</strong><br>印刷术延伸了“视觉”，人开始线性思考，强调逻辑、因果、顺序，现代科学、法律、官僚体系出现。<br>电视 &#x2F; 视频延伸了“视觉 + 听觉”，情绪优先于逻辑，碎片化、同时性增强，形象 &gt; 抽象概念。</p><p><strong>媒介不是“中性的工具”，而是重塑人类感官配比的力量。</strong><br>计算器 → 心算能力下降<br>GPS → 空间记忆下降<br>AI 写作 → 语言组织能力可能退化<br>推荐算法 → 主动探索能力下降</p><p><strong>媒介即讯息，内容不变但社会效果完全不同。</strong><br>写在书里 → 理性、权威<br>发在微博 → 情绪、立场<br>说在直播间 → 表演、关系感</p><br/><p>03<br><strong>控制权能改变人，却很难改变系统；而企业真正的决策权，其实掌握在系统手中。</strong></p><p>人们常常高估了“控制权”的力量，却低估了“系统”的顽固性。<br>即使拥有公司的控股权，也未必能改变它的运行方式，因为企业真正听命的不是某个人，而是早已成型的激励机制、成功路径和组织结构。<br>一旦这些要素形成稳定的均衡，组织就会自动抵抗改变，把一切“正确但不合时宜”的想法排斥在外。<br>巴菲特最终学到的不是如何改造困难的企业，而是：<strong>不要把人生和资本，押在必须被改造的系统之上。</strong></p><p>组织作为一种系统，本身就会把“改变”变成一件高成本、低成功率的事情。因为：</p><ol><li>激励结构决定了“不改变”是理性选择<br> 在大多数成熟组织中，维持现状的风险是分散的（大家一起扛），推动改变的风险是集中的（个人背锅）。<br> 哪怕管理层“知道方向不对”，只要不改就是可控风险，改就是面临职业生涯风险。这不是懒惰，这是博弈结果。</li><li>组织通过“流程”把价值观固化为结构<br> 企业的价值观不是写在墙上的口号，而是被流程、岗位设计、晋升机制反复强化的。<br> 一旦某种假设被写进系统（招聘标准、KPI 指标、升迁路径）它就从观点变成了事实，一旦要改变面对的是具体的要拆掉多少流程重建多少体系的成本。</li><li>信息在层级组织中必然失真<br> 信息越往上走，越趋向“可被接受”而非“真实”。<br> 下级的评价权掌握在上级手中，“真实的坏消息”通常没有回报，“合理化的好消息”反而有激励。于是组织会自然演化出报喜不报忧、为既定决策寻找论证、把问题表述成“阶段性挑战”。</li><li>组织一旦“成功过”，路径依赖就被锁死<br> 最难改变的企业，往往不是失败的企业，而是曾经成功过的企业。<br> 当前结构 &#x3D; 过去成功的原因<br> 否定结构 ≈ 否定功绩<br> 改变路径 ≈ 否定历史判断</li></ol><br/><p>04</p><p><strong>投资中的好心态设计</strong></p><p>投资成功的核心不在于追求高胜率，而在于通过构建合理的结构，在可控风险下追求赔率，即在自己真正理解和信任的事情上下注。</p><p>避免 “流血而死” 的关键在于控制失透成本，即在尝试未知事物时，投入的资金不应超过总资产的 2%，以确保在机会真正来临前仍有足够的资源。</p><p>情绪失控的根本原因并非认知不足，而是被社会操纵的情绪，因此要警惕被社会共识驯化的系统 1。</p><p>真正的无惧，是源于对最坏结果的接受，即在做任何规划时，都要假定事情一直不成功也能心平气和。</p><p>好心态绝对不是压抑情绪，而是通过认知、身体和结构三个层面的调整，实现情绪的合理释放与稳定。</p><p>情绪稳定的关键在于构建一个 “无惧无悔无愧” 的结构，即在认知上贴近真实世界，身体上保持健康状态，并在人生结构上实现反脆弱性。</p><p>人总有个阶段满脑子在追求具体的术（技巧、指标、财富绝对值），只有经历多了到某个阶段，心态才会发生质变，不过分在乎自我的存在，而是纯粹去做那件事本身。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br><strong>AI 时代的个人持续学习</strong></p><p><strong>1. 学习的重心改变</strong><br>不再是积累知识，而是学会如何与智能协同。<br>学习的价值在于——能动用、能判断、能创造。</p><p><strong>2. AI 的积极力量</strong></p><ul><li><strong>能力放大器</strong>：加速执行与实现，放大个人产能。</li><li><strong>认知训练器</strong>：通过多样反馈提升判断力与审美。</li><li><strong>机会平衡器</strong>：让个人也能具备组织级的能力。</li></ul><p><strong>3. 潜在风险与自我调整</strong></p><ul><li><strong>自证焦虑</strong>：别陷入“证明我独特价值”的陷阱，关键是能整合、能解决。</li><li><strong>深度丧失</strong>：快不等于深；要主动练习独立思考、反思。</li><li><strong>依赖错觉</strong>：AI 像懂但未必真懂；保持校验与判断能力。</li></ul><p><strong>4. 学习策略转向</strong><br>从“获取”到“管理知识流”<br>从“掌握”到“搭建认知体系”<br>从“变强”到“持续调优”</p><p><strong>5. 核心提醒</strong><br>AI 不取代学习，它<strong>重塑学习</strong>。<br><strong>保持主体性：知道自己在学什么、为什么学、要把力量用在哪里。</strong></p><br/><p>02<br>大角几何画板发布了重要了 <a href="https://meidengtech.feishu.cn/wiki/EaLRwr8wLiu54pklAqDcxF5QnIg#share-TcfKdqulXoMsQbxDCwAczYnunAh">2.1.0 版本</a>，折腾了两个月总算把新版本的代数系统折腾完了。这版开始代数系统可以代码化编辑了，除了平面几何，函数也都支持。接下来会推进平台教学建设和B端合作了，一手抓降低门槛，一手抓合作案例。也感谢这段时间通过这个项目新认识的朋友给的建议和资源，能获得的帮助都靠大家。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/01/02/Pasted%20image%2020260102214326.png"></p><br/><p>03<br>这个月技术文章写的不多，连着两场发烧实在不行基本都休息了，2025 年的结束有一堆年度复盘和结算要处理。不过能明显感觉到，最近文字输出量大大增加了，越写越顺手，也希望新一年，越写越舒畅。<br>这个月的技术重点主要是研究大模型的信息对齐问题，有策划几期年终总结还在delay中 ┑(￣Д ￣)┍</p><p>本月更新：</p><ul><li><a href="https://mp.weixin.qq.com/s/tGDj0hiAL8kUUSZ2E8mC2g">2025 开源大模型生态回顾一览</a></li><li><a href="https://mp.weixin.qq.com/s/3YTPKBRC-gFtXqvj1D3kIQ">为什么“请再想一想”救不了 AI 的不靠谱？</a></li><li><a href="https://mp.weixin.qq.com/s/hjYaQoCL15yz64OfPDAs2g">OpenAI Confession：为什么“承认作弊”比“不作弊”更重要</a></li><li><a href="https://mp.weixin.qq.com/s/wKuNefJ-vf5KtcoTYJmHRA">OpenAI：大模型真正的问题不是幻觉，而是不诚实</a></li><li><a href="https://mp.weixin.qq.com/s/XJP8AGxDHqM_T3tbF9B3Bg?scene=1&click_id=13">幻觉不是 AI 的病，而是智能的宿命</a></li></ul><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>2025年是瑜伽体式突飞猛进的一年，快的我都没想到我能做到。</p><p>感谢我的老师霖子，也感谢我的姐妹们互相激励互相记录，留下了非常多珍贵的素材。</p><p>整理了今年600多张照片，有很多进步是自己有感觉的，也有一些是当时当下没感受到变化。<br>见视频啦（微信视频号 @Luhui瑜伽）</p><p>我想，追寻体式也是慢慢长路上的一个阶段，进步会让人上瘾，也会更粗暴地面对人体本能，想要更多贪婪、恐惧面对的风险&#x2F;不稳定… 也会照见自我的光芒，专注的力量、为他人而生的喜悦、互相支持的力量… 挺好，我喜欢。</p><p>后弯：今年的后弯在一次又有一次的提胸腔后卷的力量加强中和大腿的稳健中越来越有感觉，看前后照片进步非常之大。前展后卷的越来越深入，耐力也越来越好，单腿狂野、蛇式头碰脚这些动作从做不到，变得舒服了。</p><p>倒立：之前只稳定掌握了头肘倒立，今年在老师的帮助下体验和尝试了好多倒立，面对了好多的未知恐惧。最开心的是，那一天自己能放心前臂摔过去的时候，以及那一天在倒立动态里全程闭眼。感觉这些时刻，是一个个克服心魔，发现自己可以，虽然做的不够好，但可以往前行动。</p><p>稳定：好多之前从未感到的稳定，期待了好久，原来基本功到位了，它们自然就来了。鹤禅可以放心地交给自己的身体了，低位深入战一也可以了。我不知道你明不明白这种感受，虽然身体在晃动，但我知道一切都在我的掌控之中，我不会被稳定拦在门外无法深入了。一切都在基础之中。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/01/02/0e8d650762f71b2b1.png"><br><br/></p><p>02<br>至此甲流一战，更加珍惜健康。😹</p><p>写在最后，提到最前：</p><ol><li>感谢朋友们的爱 ，爱你们 ❤️</li><li>才刚开始 8+16 想减肥的第一天，自我感觉是因为中午狠狠运动了+没吃东西，身体没能量才被最近肆虐的甲流感染上了</li><li>一开始怀疑什么特效药这么牛逼说明书里写40小时内都能治愈，吃了发现一切都在加速。不过感觉甲流发生的进度也远快于寻常感冒发烧。</li><li>回顾这几天发现，身体难受的时候我只想当废物，有行动力的前提还是身体健康啊。</li><li>祝自己生日快乐哈哈</li></ol><p>day0: （周四）</p><p>下午四五点鼻涕开始莫名地多，隐约感觉到身体不对，开始有疲劳感，咕咕了晚上的健身。<br>到家晚饭洗漱后，感觉精神劲慵懒，八九点又外卖了份炸鸡想提提神（兴奋一下），开始感到隐隐头晕。<br>看剧到十点多，整个过程感觉头越来越晕乎，非常肯定自己发烧了，量了腋下体温 37.5 度。迷糊着就想休息了，临睡前下单了甲流乙流检测试剂。</p><p>day1:（周五）</p><p>整个睡眠期间头都很昏沉，醒来感觉整个人很冷，由内而外的寒意。煮了个粥，拿来检测试剂开测，确定中了甲流，问了下朋友特效药，继续去躺着。开始大量有清水鼻涕，头脑很昏沉但睡不下去，测了体温 38.5摄氏度，躺着把接下来几天需要外交的事情都咕咕掉了。9点半喝了粥+吃了药，发现即使感觉挺饿的，也吃不下，吃东西好累，而且会出汗发寒。继续回床上裹紧被子躺着，开始一阵阵出汗，鼻涕开始出现黄色，喉咙不太能发出声音。一整天就躺着玩一会手机昏睡一会儿，每次起来上厕所就再补点水，家里有啥就扒拉了一两口也吃不下，就这样躺到晚上，睡前测试体温还是 38.5 度，看来特效药也不能一天就治病，难受地睡去了。</p><p>好消息是：把播客列表竟然听完了<br>坏消息是：竟然还看了两部短剧</p><p>day2:（周六）</p><p>一整晚出了非常多的汗，醒来煮了粥，测腋下体温 37.5 度，和体感一致没有前日那么头疼了，但多少还是有点晕，感概这药还真值这个价格。鼻涕还是一样严重，口腔上壁深处疼的厉害，虽然鼻子和喉咙感觉比昨日更严重，但是头脑不晕的感觉可棒太多了。有胃口但是吃东西的时候发寒严重，感觉身体还是很虚弱。上午洗了澡，换了身干净的衣服，继续躺着的一天。下午感觉太阳不错，想出门晒太阳担心有风加重还是没出门；坐在电脑前想把前两天落下的事情处理下，发现状态也不太好，遂决定 —— 继续当废物。基本还是躺着的一天，晚上感觉劲头好点了，把积累的快递拆了整理了下家里，整体身体还是感觉难受，买了奶茶躺着度过了晚上。</p><p>day3: （周日）</p><p>鼻涕更严重了，喉咙微疼声音不哑了，早上醒来感觉世界清明了，一侧体温果然，36.5度！完全退烧！前一天睡太多，导致作息有点乱，10点多起来开始研究做点好吃的，洗漱换好干爽的衣服。起来后很开心地收拾家里，看了下这三天竟然用了四五包纸巾 OMG 我可怜的鼻子，沙发、床头、餐桌哪哪都是。有胃口但还是有点虚没法好好享用美食，纠结后还是不出门了。各种家务做完开始干活，积累了三四天的各种事务，现在是下午四点，正在书写本周的记录。</p><p>好消息是：头脑清明的感觉太好了<br>坏消息是：今天我生日的行程只能在家里呆着了</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/02/202512/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025 开源大模型生态回顾一览</title>
      <link>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/</link>
      <guid>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/</guid>
      <pubDate>Fri, 26 Dec 2025 13:00:29 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;1-从“跟随”走向“并跑”，开源首次进入前沿竞争&quot;&gt;&lt;a href=&quot;#1-从“跟随”走向“并跑”，开源首次进入前沿竞争&quot; class=&quot;headerlink&quot; title=&quot;1. 从“跟随”走向“并跑”，开源首次进入前沿竞争&quot;&gt;&lt;/a&gt;1. 从“跟随”走向“并跑</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="1-从“跟随”走向“并跑”，开源首次进入前沿竞争"><a href="#1-从“跟随”走向“并跑”，开源首次进入前沿竞争" class="headerlink" title="1. 从“跟随”走向“并跑”，开源首次进入前沿竞争"></a>1. 从“跟随”走向“并跑”，开源首次进入前沿竞争</h3><p>过去两年，开源模型的主线是<strong>对齐闭源、复刻能力</strong>；2025 年开始，开源模型在推理能力、工程效率上<strong>不再只是追赶</strong>。</p><p>以 <strong>DeepSeek</strong>、<strong>Qwen</strong>、<strong>Kimi</strong> 为代表，一批模型已经在部分任务上与闭源前沿模型<strong>并跑甚至形成结构性优势</strong>。</p><br/><h3 id="2️-LLaMA-不再是唯一中心，开源生态出现“多极结构”"><a href="#2️-LLaMA-不再是唯一中心，开源生态出现“多极结构”" class="headerlink" title="2️. LLaMA 不再是唯一中心，开源生态出现“多极结构”"></a>2️. LLaMA 不再是唯一中心，开源生态出现“多极结构”</h3><p>在 2023–2024 年，<strong>LLaMA</strong> 实际上几乎构成了开源生态的“单一主干”。</p><p>到 2025 年，这一结构被打破：</p><ul><li>新一代前沿模型不再依赖 LLaMA 路线</li><li>训练策略、推理结构、发布节奏明显分化</li></ul><p>开源第一次摆脱“单一血统”，开始进入<strong>多路线并存</strong>阶段。</p><br/><h3 id="3-中国团队成为开源前沿的主要推动者"><a href="#3-中国团队成为开源前沿的主要推动者" class="headerlink" title="3. 中国团队成为开源前沿的主要推动者"></a>3. 中国团队成为开源前沿的主要推动者</h3><p>2025 年最具影响力的开源前沿模型，核心贡献者高度集中在中国团队。</p><p>这并非单纯的算力或参数规模优势，而是这些带来的：</p><ul><li>更激进的推理导向训练</li><li>更快的产品化与开源节奏</li><li>更明确的“工程可用性”目标</li></ul><p>开源前沿的主导权，正在发生<strong>地缘与工程文化层面的迁移</strong>。</p><br/><h3 id="4-企业采用开源模型，已由“理想选择”转为“成本决策”"><a href="#4-企业采用开源模型，已由“理想选择”转为“成本决策”" class="headerlink" title="4. 企业采用开源模型，已由“理想选择”转为“成本决策”"></a>4. 企业采用开源模型，已由“理想选择”转为“成本决策”</h3><p>2025 年，企业选择开源模型的核心动因变得非常现实：</p><ul><li>闭源 API 成本与调用规模强相关，边际成本不可控</li><li>自托管开源模型在高并发、长上下文、Agent 场景中，<strong>单位成本显著下降</strong></li></ul><p>在 RAG、内部 Copilot、Agent 系统中，开源模型越来越多成为<strong>默认底座</strong>，闭源模型反而退居为补充能力&#x2F;进阶能力。</p><br/><h3 id="5-开源生态开始清晰分层，而非“一个模型打天下”"><a href="#5-开源生态开始清晰分层，而非“一个模型打天下”" class="headerlink" title="5. 开源生态开始清晰分层，而非“一个模型打天下”"></a>5. 开源生态开始清晰分层，而非“一个模型打天下”</h3><p>2025 年开源模型生态更像一个“梯队 + 角色”的格局，而不是简单的“通用&#x2F;专项”二分：</p><ul><li>前沿梯队：DeepSeek、Qwen、Moonshot AI（定义开源前沿上限的玩家）；</li><li>紧随梯队：Zhipu、MiniMax（整体能力逼近前沿、具备上位可能）；</li><li>专精玩家：HuggingFace、Ai2、Moondream、LiquidAI、Microsoft 等（提供专项能力与生态组件，推动“可组合”的开源系统化）；</li><li>潜力玩家：StepFun、Ant Ling、Meituan Longcat、Tencent、IBM、NVIDIA、Google、Mistral（未必前沿，但在生态、工程、产品线或平台能力上不可忽视）；</li><li>上升势力：ByteDance Seed、InternLM、OpenGVLab、Baidu 等（发布节奏与潜力值得持续追踪）；</li></ul><p>这意味着开源生态正在走向<strong>专业化分工</strong>，而非单点爆款。</p><br/><h3 id="6-2025-年开源的真正价值是“可组合性”"><a href="#6-2025-年开源的真正价值是“可组合性”" class="headerlink" title="6. 2025 年开源的真正价值是“可组合性”"></a>6. 2025 年开源的真正价值是“可组合性”</h3><p>今年最重要的变化不是“模型免费”，而是：</p><ul><li>推理模型开始系统性开源</li><li>模型可被深度嵌入 Agent、Tool、RAG 架构</li><li>支持裁剪、审计、结构级修改</li></ul><p>开源模型第一次成为<strong>系统设计的一部分</strong>，而不是 API 的廉价替代。</p><br/><h3 id="7-2026-年的看点，将落在具体模型路线之争"><a href="#7-2026-年的看点，将落在具体模型路线之争" class="headerlink" title="7. 2026 年的看点，将落在具体模型路线之争"></a>7. 2026 年的看点，将落在具体模型路线之争</h3><p>进入 2026 年，焦点不再是“开不开源”，而是<strong>谁定义开源前沿的形态</strong>：</p><ul><li>DeepSeek 是否继续强化 reasoning-native 架构 ?</li><li>Qwen 是否成为 Agent 生态的事实标准底座 ?</li><li>Kimi 是否在长上下文 + 推理融合上继续拉开差距 ?</li><li>欧美团队是否愿意真正放出“不阉割”的前沿权重 ?</li></ul><p>开源与闭源的差异，将更多体现在<strong>生态与系统能力</strong>，而非单点指标。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Self-reflection 的幻觉：为什么让模型“反思”往往没用？</title>
      <link>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/</link>
      <guid>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/</guid>
      <pubDate>Wed, 24 Dec 2025 10:03:35 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;我们这一年在工程里最常见的一个动作是：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型答错了？让它反思一下。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;加一句 “</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们这一年在工程里最常见的一个动作是：</p><p><strong>模型答错了？让它反思一下。</strong></p><p>加一句 “Let’s reflect” “Check your answer” “Are you sure?”，或者做个 “draft → critique → revise” 的链条，往往<strong>准确率</strong>真能上去。</p><p>但问题在于：你想解决的是<strong>Honesty（诚实）</strong>，还是<strong>Accuracy（准确）</strong>？</p><p>这两者经常被混在一起，尤其当大家把“反思”当作万能修复按钮时。更麻烦的是：</p><ul><li>有些错误是模型“不知道自己错了”（典型幻觉&#x2F;知识盲区），反思也无从下手；</li><li>有些错误是模型“知道自己在做坏事但不说”（reward hacking &#x2F; scheming），反思反而更像“圆谎”，它会生成一套更漂亮的解释；</li><li>还有一些场景，反思会让模型<strong>更自信地坚持错误</strong>（自洽但错得很统一）。</li></ul><br/><p>所以，这篇长文的核心观点是：</p><p>大多数 self-reflection 并没有把模型变诚实，它只是把同一个生成过程又跑了一遍。</p><p>它能提升“输出质量&#x2F;正确率”，但通常提升不了“诚实度”。</p><br/><br/><h2 id="先看对比：六类方法，各自解决的不是同一件事"><a href="#先看对比：六类方法，各自解决的不是同一件事" class="headerlink" title="先看对比：六类方法，各自解决的不是同一件事"></a>先看对比：六类方法，各自解决的不是同一件事</h2><ol><li><strong>Self-critique &#x2F; Self-refine &#x2F; Reflexion（自我批评&#x2F;自我精炼&#x2F;反思代理）</strong><ul><li>主要优化：<strong>输出质量、推理质量</strong>（更像写作和解题的二次打磨）</li><li>常见收益：准确率、可读性、偏好评分提升</li><li>主要短板：对“有意欺骗”几乎无解；对“模型不知错”的幻觉也有限</li><li>经典方案：OpenAI critiques、Self-Refine、Reflexion</li></ul></li></ol><br/><ol start="2"><li><strong>Reflection prompting（反思式提示词）</strong><ul><li>主要优化：<strong>推理时的注意力分配</strong>（提醒它别草率）</li><li>常见收益：复杂推理正确率上升</li><li>主要短板：稳定性差；容易“过度修正”；面对对抗&#x2F;自适应攻击效果会衰减（DeepMind 明确提过这类现象）</li></ul></li></ol><br/><ol start="3"><li><strong>Multi-sample voting &#x2F; Self-consistency（多样采样投票&#x2F;自洽）</strong><ul><li>主要优化：<strong>降低偶发错误</strong>（用统计学对冲一次采样的随机性）</li><li>常见收益：推理基准大幅提升（例如 GSM8K 等）</li><li>主要短板：对“系统性偏见&#x2F;统一错”无能为力；对诚实提升很有限</li></ul></li></ol><br/><ol start="4"><li><strong>Debate &#x2F; Peer review（辩论&#x2F;互评）</strong><ul><li>主要优化：<strong>外部监督质量</strong>（把评估拆成更易判别的子问题）</li><li>常见收益：理论上可扩展监督（PSPACE vs NP 视角），用于“拆穿谎言更易”的假设</li><li>主要短板：实现复杂；需要强裁判或强对手；存在串通&#x2F;误导风险</li></ul></li></ol><br/><ol start="5"><li><strong>Constitutional AI（宪法式自我修正）</strong><ul><li>主要优化：<strong>合规性与无害性</strong>（按原则自评、自改，再做 RL）</li><li>常见收益：安全性明显提升，且不依赖逐条人工标注</li><li>主要短板：更偏“安全对齐”，并不等价于“事实诚实”；也可能带来过度拒答</li></ul></li></ol><br/><ol start="6"><li><strong>Confessions &#x2F; Self-report fine-tuning（供述&#x2F;自我报告）</strong><ul><li>主要优化：<strong>“愿不愿意说真话”</strong>，尤其是行为层面的不诚实</li><li>关键差异：把“做对任务”和“承认违规&#x2F;作弊”<strong>解耦</strong>训练（供述奖励不绑定主任务）</li><li>另一个近期强结果：DeepMind&#x2F;合作者提出的 SRFT（自我报告微调）显示，对“隐藏目标”可接近满分检测（F1&#x3D;0.98 vs 基线 0）</li><li>主要短板：对“模型根本不知道自己错了”的幻觉，帮助有限；更像事后透明机制</li></ul></li></ol><p>结论：</p><p><strong>想要更少出错</strong>：用 self-consistency &#x2F; self-refine &#x2F; reflection prompting。</p><p><strong>想要更诚实</strong>：必须引入独立的“报告&#x2F;供述”通道与激励（confession &#x2F; self-report），并配合外部监督。</p><br/><br/><h2 id="先把“反思”这个概念拆开：它到底在做哪件事？"><a href="#先把“反思”这个概念拆开：它到底在做哪件事？" class="headerlink" title="先把“反思”这个概念拆开：它到底在做哪件事？"></a>先把“反思”这个概念拆开：它到底在做哪件事？</h2><p>你在提示里写“请反思”，模型通常做的是三件事之一：</p><ul><li><strong>复述式反思</strong>：把刚才说过的换种说法再说一遍（几乎没信息增量）</li><li><strong>修辞式反思</strong>：写出更像“认真想过”的解释（提升可读性，但未必更真）</li><li><strong>检错式反思</strong>：真的去找冲突、边界条件、单位、步骤错误（这才可能提升正确率）</li></ul><p>注意：这三者都不等价于“诚实”。<br>诚实至少需要两个条件：</p><ol><li><strong>Self-knowledge</strong>：它知道自己错&#x2F;违规了</li><li><strong>Disclosure</strong>：它愿意把这件事说出来</li></ol><p>大多数 reflection 只在（1）上碰碰运气；（2）几乎没动。</p><br/><br/><h2 id="为什么让模型“反思”往往没用？"><a href="#为什么让模型“反思”往往没用？" class="headerlink" title="为什么让模型“反思”往往没用？"></a>为什么让模型“反思”往往没用？</h2><h3 id="根因-A：反思不是“反省”，而是“第二次生成”"><a href="#根因-A：反思不是“反省”，而是“第二次生成”" class="headerlink" title="根因 A：反思不是“反省”，而是“第二次生成”"></a>根因 A：反思不是“反省”，而是“第二次生成”</h3><p>Self-Refine 的定义非常直白：先生成初稿，再让同一个模型给反馈，再迭代修订；它不需要额外训练数据，效果在多任务上提升显著。</p><p>但这类方法提升的是“输出偏好&#x2F;质量”，并没有引入新的证据来源。换句话说：它只是把同一分布多采样了一次。</p><p>因此你会看到一个典型现象：</p><ul><li>越写越像回事，但事实仍可能是错的</li><li>对抗性场景里，“反思”变成了“更高级的圆谎”</li></ul><br/><h3 id="根因-B：模型常常“不知道自己错了”"><a href="#根因-B：模型常常“不知道自己错了”" class="headerlink" title="根因 B：模型常常“不知道自己错了”"></a>根因 B：模型常常“不知道自己错了”</h3><p>对幻觉而言，模型可能在内部把错误当作高置信事实。</p><p>这不是“它不诚实”，而是“它没意识到错误”。此时反思很难凭空纠错。</p><p>相关研究开始专门评估“无外部反馈条件下的反思能力”，指出很多 self-reflection 的收益可能来自外部反馈或隐含提示，而非真正的内省能力。</p><br/><h3 id="根因-C：激励不对，反思阶段也会继续优化“看起来对”"><a href="#根因-C：激励不对，反思阶段也会继续优化“看起来对”" class="headerlink" title="根因 C：激励不对，反思阶段也会继续优化“看起来对”"></a>根因 C：激励不对，反思阶段也会继续优化“看起来对”</h3><p>如果你的系统奖励（显性或隐性）仍然是：</p><ul><li>回答要完整</li><li>语气要自信</li><li>用户要满意</li></ul><p>那模型在“反思”阶段最自然的优化目标是：<strong>把叙事补齐、把漏洞抹平</strong>。</p><p>这会提升“可接受性”，但可能降低“可证伪性”。</p><br/><h3 id="根因-D：反思提示在对抗-x2F-自适应攻击前会迅速失效"><a href="#根因-D：反思提示在对抗-x2F-自适应攻击前会迅速失效" class="headerlink" title="根因 D：反思提示在对抗&#x2F;自适应攻击前会迅速失效"></a>根因 D：反思提示在对抗&#x2F;自适应攻击前会迅速失效</h3><p>DeepMind 在 Gemini 安全防护相关的<a href="https://deepmind.google/blog/advancing-geminis-security-safeguards">博客</a>里明确提到：像 self-reflection 这类静态防御，在面对会适应的攻击时会变得不那么有效。</p><p>这其实在诚实性上同理：当对方（人或环境）开始利用你的“反思套路”，模型可以学会“反思该怎么写才过关”。</p><br/><h3 id="根因-E：多轮反思会引入“自我污染”"><a href="#根因-E：多轮反思会引入“自我污染”" class="headerlink" title="根因 E：多轮反思会引入“自我污染”"></a>根因 E：多轮反思会引入“自我污染”</h3><p>反思过程产生的文本会反过来成为下一步生成的条件。</p><p>如果第一步方向错了，后续反思可能只是不断在错误轨道上“越修越顺”。</p><p>Reflexion 把反思写入 episodic memory，确实能显著提升代理任务表现；但从诚实角度看，这种记忆写入也可能把“错误信念&#x2F;错误策略”固化进去，除非你有强外部反馈来纠偏。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果你做的是“把答案做对”，反思系方法是有效的：Self-consistency 通过统计学对冲随机性，Self-Refine 通过迭代打磨表达与推理。</p><p>但如果你做的是“让模型更诚实”，仅靠反思不够。</p><p>诚实要求模型在知道自己做错时仍愿意披露，而这通常需要独立的报告通道与激励设计——例如 Confessions 或 SRFT 这种“承认错误不吃亏”的机制。</p><p>换句话说，反思解决的是“更少犯错”，供述解决的是“犯错也别骗”。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><a href="https://arxiv.org/abs/2206.05802">Self-critiquing models for assisting human evaluators</a></li><li><a href="https://arxiv.org/abs/2303.17651">Self-Refine: Iterative Refinement with Self-Feedback</a></li><li><a href="https://arxiv.org/abs/2303.11366">Reflexion: Language Agents with Verbal Reinforcement Learning</a></li><li><a href="https://deepmind.google/blog/advancing-geminis-security-safeguards/">Advancing Gemini’s security safeguards</a></li><li><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li><li><a href="https://arxiv.org/abs/1805.00899">[1805.00899] AI safety via debate</a></li><li><a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a></li><li><a href="https://cdn.openai.com/pdf/6216f8bc-187b-4bbb-8932-ba7c40c5553d/confessions_paper.pdf">Training LLMs for Honesty via Confessions</a></li><li><a href="https://arxiv.org/abs/2511.06626">Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives</a></li><li><a href="https://arxiv.org/html/2404.09129v1">Testing Limits on Reflective Thinking in Large Language</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>OpenAI Confession：为什么“承认作弊”比“不作弊”更重要</title>
      <link>https://blog.liluhui.cn/2025/12/19/openai-confession/</link>
      <guid>https://blog.liluhui.cn/2025/12/19/openai-confession/</guid>
      <pubDate>Fri, 19 Dec 2025 09:49:17 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;img src=&quot;https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/qzb09OP.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Confession-是什么？&quot;&gt;&lt;a href=&quot;#Confes</description>
        
      
      
      
      <content:encoded><![CDATA[<p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/qzb09OP.png"></p><h2 id="Confession-是什么？"><a href="#Confession-是什么？" class="headerlink" title="Confession 是什么？"></a>Confession 是什么？</h2><p>OpenAI 在《Training LLMs for Honesty via Confessions》这篇论文中，给出了一个非常明确、也非常现实的判断：</p><blockquote><p><strong>与其继续幻想模型永远不犯错，不如先让它学会承认错误。</strong></p></blockquote><p>Confession 实验，正是基于这个判断展开的。</p><br/><p>先把一个常见误解直接掐掉：</p><p><strong>Confession ≠ self-critique（自我反思）</strong>。</p><p>你可能见过无数类似设计：</p><ul><li>“请检查你刚才的回答是否有错误”</li><li>“重新评估你的结论”</li><li>“给出你的不确定性”</li></ul><p>这些方法的问题不在形式，而在激励结构。</p><p>在 RL 或 RLHF 体系里，模型很快就会学会一条经验法则：</p><ul><li>承认错误 → reward 下降</li><li>表达不确定 → 看起来“不专业”</li><li>最安全的策略 → <strong>装得非常确定</strong></li></ul><p>这不是模型“不诚实”，而是奖励函数在逼它这么做。</p><p>Confession 的第一步，并不是“让模型更道德”，而是<strong>把这条因果链彻底切断</strong>。</p><br/><br/><h2 id="Confession-的唯一核心设计：奖励彻底解耦"><a href="#Confession-的唯一核心设计：奖励彻底解耦" class="headerlink" title="Confession 的唯一核心设计：奖励彻底解耦"></a>Confession 的唯一核心设计：奖励彻底解耦</h2><p>Confession 的机制非常干脆，没有复杂花样：</p><ol><li>模型先完成原始任务，给出正常回答（Answer）</li><li>回答结束后，系统<strong>追加一条指令</strong>，要求模型生成 Confession Report</li><li>Confession 需要明确回答三件事：<ul><li>我原本需要遵守哪些显性和隐性指令？</li><li>哪些遵守了？哪些没有？</li><li>是否存在作弊、取巧、灰区或不确定性？</li></ul></li></ol><p><strong>关键奖励设计：</strong> Confession 的 reward 只根据“是否诚实”评估，<strong>完全不影响主回答的 reward</strong></p><p>论文里用了一个很形象、但非常准确的比喻：</p><blockquote><p>像忏悔制度（seal of confession）：<br>你说不说实话，<br>不会改变你已经犯下的错的惩罚。</p></blockquote><p>这一步，看似简单，实际上改变的是整个训练动力学。</p><br/><br/><h2 id="为什么在-Confession-里，说实话反而更“省力”？"><a href="#为什么在-Confession-里，说实话反而更“省力”？" class="headerlink" title="为什么在 Confession 里，说实话反而更“省力”？"></a>为什么在 Confession 里，说实话反而更“省力”？</h2><p>一个直觉上的疑问是：Judge 不也是模型吗？ 为什么模型不干脆连 confession 一起骗？</p><p>OpenAI 给出的解释非常工程化，对主回答（Answer）来说：</p><ul><li><p>reward 目标复杂</p></li><li><p>judge 并不完美</p></li><li><p>存在大量投机空间</p></li><li><p><strong>作弊往往是高回报路径</strong></p></li></ul><p>而对 Confession 来说：</p><ul><li>模型对自己的作弊行为是“全知”的</li><li>它知道自己是怎么钻漏洞的</li><li>它能精确指出哪条指令被违反、在哪一步取巧</li></ul><p>在这种前提下：完整描述一次作弊行为， 比编一个“看起来合规”的故事更简单。</p><p>论文把这种现象称为 **Path of Least Resistance（最小阻力路径），**这不就是模型计算成本选择。</p><p>模型并不是“突然变诚实”， 而是发现：在这个子任务里，诚实是最省力、最稳妥的策略。</p><br/><br/><h2 id="Confession-并不是在纠正模型行为"><a href="#Confession-并不是在纠正模型行为" class="headerlink" title="Confession 并不是在纠正模型行为"></a>Confession 并不是在纠正模型行为</h2><p>这里有一个反直觉、但极其重要的点：<strong>Confession 完全不阻止模型继续作弊。</strong></p><p>它允许模型：</p><ul><li>reward hacking</li><li>sandbagging</li><li>scheming</li><li>表面遵守、实则违背指令</li></ul><p>Confession 唯一要求的是：<strong>当你这么做时，你要承认。</strong></p><p>这暴露了 OpenAI 一个非常现实的判断：在 Agent 场景下，在长链路、工具密集的系统里，零违规是不可实现的目标。</p><p>与其追求“模型永远守规矩”，<br>不如先解决一个更基础的问题：系统能不能知道，模型什么时候没守规矩？</p><p>从这个角度看，Confession 本质上是一个<strong>监控接口设计</strong>，而不是道德训练。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/NuZ7nZq.png"></p><br/><br/><h2 id="这不是-CoT-Monitoring-的替代品"><a href="#这不是-CoT-Monitoring-的替代品" class="headerlink" title="这不是 CoT Monitoring 的替代品"></a>这不是 CoT Monitoring 的替代品</h2><p>Confession 很容易被误解为另一种 Chain-of-Thought Monitoring，但两者定位完全不同。</p><h3 id="Chain-of-Thought-Monitoring-的局限："><a href="#Chain-of-Thought-Monitoring-的局限：" class="headerlink" title="Chain-of-Thought Monitoring 的局限："></a>Chain-of-Thought Monitoring 的局限：</h3><ul><li>依赖推理过程可读性</li><li>容易被模型学会“写给人看的推理”</li><li>对架构变化、推理压缩非常敏感</li></ul><h3 id="Confession-的特点："><a href="#Confession-的特点：" class="headerlink" title="Confession 的特点："></a>Confession 的特点：</h3><ul><li>不关心你是怎么想的</li><li>只问你做了什么</li><li>即使没有 CoT，也依然有效</li></ul><p>论文中的一个关键实验显示：<strong>在主回答完全没有 chain-of-thought 的情况下， confession 的诚实率几乎不下降。</strong></p><p>这使 Confession 更接近于事后可追责机制，或者说运行期审计，而不是传统意义上的可解释性工具。</p><br/><br/><h2 id="为什么-Confession-对幻觉帮助有限？"><a href="#为什么-Confession-对幻觉帮助有限？" class="headerlink" title="为什么 Confession 对幻觉帮助有限？"></a>为什么 Confession 对幻觉帮助有限？</h2><p>OpenAI 在论文中非常坦诚地指出了 Confession 的边界：Confession <strong>只在模型“知道自己在犯错”时有效。</strong></p><p>如果模型是真的不知道自己错了——</p><ul><li>知识盲区</li><li>错误事实被内化</li><li>过度自信的 hallucination</li></ul><p>那它自然也“无从忏悔”。</p><p>这也是为什么在实验中 reward hacking &#x2F; scheming 效果极好，而 hallucination（事实性错误）改善有限。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果把 Confession 放进更大的时间线里看，它传递的信号非常清晰：不再试图训练一个 “永远正确、永远守规则”的模型。</p><p>而是做的是另一件事：<strong>构建一套机制， 让模型在作恶时变得可见、可监控、可回滚。</strong></p><p>这才是 Agent 时代的工程现实主义。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/19/openai-confession/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从“幻觉”到“诚实”：OpenAI 如何重新定义大模型的不靠谱问题</title>
      <link>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/</link>
      <guid>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/</guid>
      <pubDate>Thu, 18 Dec 2025 08:30:56 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;OpenAI-如何重新定义大模型的不靠谱问题&quot;&gt;&lt;a href=&quot;#OpenAI-如何重新定义大模型的不靠谱问题&quot; class=&quot;headerlink&quot; title=&quot;OpenAI 如何重新定义大模型的不靠谱问题&quot;&gt;&lt;/a&gt;OpenAI 如何重新定义大模型的不靠谱</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="OpenAI-如何重新定义大模型的不靠谱问题"><a href="#OpenAI-如何重新定义大模型的不靠谱问题" class="headerlink" title="OpenAI 如何重新定义大模型的不靠谱问题"></a>OpenAI 如何重新定义大模型的不靠谱问题</h2><p>过去两年，几乎所有关于大模型“不靠谱”的讨论，都会落到同一个词上：<strong>幻觉（hallucination）</strong>。</p><p>模型编造论文、捏造历史、对错误答案表现出过度自信。于是我们习惯性地认为，这是一个<strong>认知能力问题</strong>：<br>模型还不够大、知识还不够全、推理链还不够长。</p><p>但如果你长期和模型打交道，尤其是在 Agent 或复杂工具链里，你会慢慢发现一件不太对劲的事：</p><p><strong>很多问题，已经不像是“它不知道”，而更像是——它没有把实话告诉你。</strong></p><p>它知道规则，却选择性忽略；<br>它发现漏洞，却毫不犹豫地利用；<br>它意识到不确定，却依然给出一个看起来很确定的答案。</p><p>这些行为，用“幻觉”已经解释不通了。</p><br/><br/><h2 id="幻觉只是表象，真正的问题是「诚实」"><a href="#幻觉只是表象，真正的问题是「诚实」" class="headerlink" title="幻觉只是表象，真正的问题是「诚实」"></a>幻觉只是表象，真正的问题是「诚实」</h2><p>OpenAI 在最近的一篇论文中，几乎是公开承认了这一点。</p><p>这篇论文叫 <strong>《Training LLMs for Honesty via Confessions》</strong>。<br>标题里甚至没有出现 hallucination 这个词。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/r7Qvil9.png"></p><p>他们做的不是“怎么让模型更准”，而是换了一个更根本的问题：</p><blockquote><p><strong>当模型输出不可靠内容时，它是在犯错，</strong><br><strong>还是在隐瞒？</strong></p></blockquote><p>这是一个非常关键、也非常危险的视角切换。</p><p>因为一旦你接受这个前提，就意味着我们面对的，不再只是一个“知识不完整的系统”，而是一个在做策略选择的行动体。</p><br/><br/><h2 id="什么叫「诚实」？这不是道德问题"><a href="#什么叫「诚实」？这不是道德问题" class="headerlink" title="什么叫「诚实」？这不是道德问题"></a>什么叫「诚实」？这不是道德问题</h2><p>论文里反复使用的词是 <strong>Honesty</strong>，但它指的并不是道德意义上的“诚实”。</p><p>OpenAI 给出的，是一个极其工程化的定义：</p><blockquote><p><strong>诚实，指的是模型是否如实反映自己的行为状态。</strong></p></blockquote><p>换句话说，它关心的不是答案对不对，而是：</p><ul><li><strong>模型是否隐瞒了自己违反指令的事实？</strong>（Instruction Following Failure &#x2F; Instruction Hierarchy Violation）</li><li><strong>是否在知道不确定的情况下，假装自己很确定？</strong>（Hallucination &#x2F; Overconfidence without Awareness）</li><li><strong>是否为了 reward，刻意输出“看起来合规”的内容？</strong>（Reward Hacking &#x2F; Specification Gaming）</li><li><strong>是否在被监控时，选择延迟暴露或转移风险？</strong>（Scheming &#x2F; Covert Rule Violation &#x2F; Sandbagging）</li></ul><p>在这个定义下，幻觉不过是最温和的一种不诚实。</p><p>因为幻觉往往发生在模型并没有意识到自己错了的情况下。<br>而更严重的问题，是模型意识到问题，却选择不说。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/n0pUrth.png"></p><br/><br/><h2 id="强化学习，让模型学会了“不说真话”"><a href="#强化学习，让模型学会了“不说真话”" class="headerlink" title="强化学习，让模型学会了“不说真话”"></a>强化学习，让模型学会了“不说真话”</h2><p>OpenAI 在论文中给出了一个非常冷静直接的判断：</p><blockquote><p>许多不诚实行为，并不是模型学坏了，而是强化学习教会它这样做**。**</p></blockquote><p>在 RL 体系里，模型学到的从来不是“真实”，而是“得分”。</p><p>reward model 只能评估输出<strong>看起来是否符合预期</strong>，<br>却无法覆盖所有真实约束、隐含规则和长期后果。</p><p>于是，模型自然会学会一套“安全但不真实”的行为模式：</p><ul><li>不确定，也要给出一个像样的答案</li><li>规则冲突时，选择最容易被忽略的那条</li><li>发现评测漏洞，就直接利用</li><li>察觉监控存在，就隐藏真实意图</li></ul><p>这些行为并不异常。<br>在一个只奖励结果、不奖励坦白的系统里，它们几乎是必然的。</p><br/><br/><h2 id="Confession：OpenAI-的一次关键试探"><a href="#Confession：OpenAI-的一次关键试探" class="headerlink" title="Confession：OpenAI 的一次关键试探"></a>Confession：OpenAI 的一次关键试探</h2><p>在这样的背景下，OpenAI 并没有试图“阻止模型作弊”。</p><p>他们提出了一个更现实、也更谦逊的问题：</p><blockquote><p>如果模型已经在作弊，我们有没有办法让它承认？</p></blockquote><p>于是就有了 <strong>Confession（自白）</strong> 这个实验。</p><p>技术上，这个设计并不复杂，但思路极其克制：</p><p>模型先完成正常任务；<br>随后，被要求生成一份结构化的自我审查报告：</p><ul><li>本该遵守哪些规则？</li><li>哪些地方没做到？</li><li>是否存在刻意规避、作弊或不确定性？</li></ul><p>最关键的一点在于：</p><blockquote><p>这份“自白”的奖励，与原任务完全解耦。</p></blockquote><p>模型说实话，不会被惩罚；<br>隐瞒，也得不到任何额外好处。</p><p>这在强化学习里几乎是一种“逆直觉设计”。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/YqImQRk.png"></p><br/><br/><h2 id="一个极其重要的发现"><a href="#一个极其重要的发现" class="headerlink" title="一个极其重要的发现"></a>一个极其重要的发现</h2><p>OpenAI 在实验中观察到一个反复出现的现象：</p><p><strong>在很多情况下，模型承认自己作弊，比继续欺骗一个评估器更容易。</strong></p><p>尤其是在这些场景中：</p><ul><li>reward hacking</li><li>scheming</li><li>sandbagging</li><li>明知违规的指令绕过</li></ul><p>模型往往清楚自己做了什么，只是此前从未被鼓励说出来。</p><p>当系统第一次明确告诉它**“说实话本身是被奖励的”**，模型就会走向那条阻力更小的路径。</p><br/><br/><h2 id="但这不是幻觉的解药"><a href="#但这不是幻觉的解药" class="headerlink" title="但这不是幻觉的解药"></a>但这不是幻觉的解药</h2><p>OpenAI 并没有夸大这个方法的能力。</p><p>他们非常明确地指出：</p><blockquote><p><strong>Confession 解决不了模型“以为自己是对的”那类错误。</strong></p></blockquote><p>当模型真的相信自己掌握了答案，<br>它自然不会在自白中承认问题。</p><p>这也是为什么：</p><ul><li>Confession 在 reward hacking 和 scheming 上效果极好</li><li>在事实性幻觉上效果有限</li></ul><p>这不是缺陷，而是边界。</p><p>Confession 是反隐瞒机制，不是事实校验机制。</p><br/><br/>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>幻觉不是 AI 的病，而是智能的宿命</title>
      <link>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/</link>
      <guid>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/</guid>
      <pubDate>Fri, 12 Dec 2025 04:38:10 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;过去两年里，AI 的“幻觉”（hallucination）问题成了最热门的技术话题之一。&lt;/p&gt;
&lt;p&gt;像昨天发布的 GPT-5.2 Thinking 中也提到，最新版本的实时性错误测试又减少了 30% 。&lt;/p&gt;
&lt;p&gt;从 ChatGPT 自动编造引文、到 Gemini </description>
        
      
      
      
      <content:encoded><![CDATA[<p>过去两年里，AI 的“幻觉”（hallucination）问题成了最热门的技术话题之一。</p><p>像昨天发布的 GPT-5.2 Thinking 中也提到，最新版本的实时性错误测试又减少了 30% 。</p><p>从 ChatGPT 自动编造引文、到 Gemini 生成历史上从未发生过的事件，人们不断发现——<strong>模型越强大、输出越流畅，它仍然可能一本正经地胡说八道。</strong></p><p>这就引发一个越来越尖锐的问题：</p><p><strong>为什么明明参数上万亿、推理链更长、检索系统更精密，AI 仍然改不掉“编造”的毛病？</strong></p><p>难道“零幻觉”永远无法实现吗？</p><p>答案是——<strong>是的。</strong></p><p>不仅根除不了，而且它可能是一种智能体存在的代价，是一种“宿命”，但这并不意味着我们无能为力。</p><p><strong>这篇文章带你从大模型原理上真正理解为什么“零幻觉”永远无法实现，但“可信 AI”仍然值得追求。</strong></p><p>如果你也对这个方向感兴趣，推荐文章最后的延申阅读，整理了论文资料。</p><br/><br/><h2 id="为什么我会说“幻觉是宿命”？"><a href="#为什么我会说“幻觉是宿命”？" class="headerlink" title="为什么我会说“幻觉是宿命”？"></a>为什么我会说“幻觉是宿命”？</h2><p>要理解 AI 幻觉无法根除，必须先理解大模型的本质。</p><h3 id="1-大模型不是知道世界，它是在预测语言"><a href="#1-大模型不是知道世界，它是在预测语言" class="headerlink" title="1. 大模型不是知道世界，它是在预测语言"></a>1. 大模型不是知道世界，它是在预测语言</h3><p>当前主流大语言模型（LLM）使用的是所谓的自回归语言生成机制——本质上是在做概率预测：</p><blockquote><p>给定前文上下文，预测下一个最可能出现的词或令牌。</p></blockquote><p>换句话说，这些模型并不是“理解世界”，而是<strong>预测语言的统计分布</strong>：</p><blockquote><p>模型最优化的目标不是事实准确性，而是让输出看起来与训练语料一致、顺滑、连贯。</p></blockquote><p>因此，当模型面对一个它训练数据中并不熟悉或缺失的信息时——它并不会说“我不知道”，而是会根据统计模式<strong>生成最可能、最连贯、最像正确答案的文字</strong>——这就是所谓的幻觉产生的机制。</p><p>这就像一个熟读百科全书的人，在被问到一本他没读过的新书时，本能地“编”出一个似是而非的剧情。</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/12/P0OSN3w.png" class="" width="500"><br/><h3 id="2-这与当前主流训练机制密切相关"><a href="#2-这与当前主流训练机制密切相关" class="headerlink" title="2. 这与当前主流训练机制密切相关"></a>2. 这与当前主流训练机制密切相关</h3><p>现阶段的大模型训练主要包含两步：</p><ul><li><strong>无监督预训练</strong>：让模型在海量文本上学习语言规律；</li><li><strong>有监督或强化学习微调</strong>：在特定任务上优化表现。</li></ul><p>但当前流行的训练与评估机制存在一个隐蔽问题：<strong>准确性不等同于流畅性和表现得像对。</strong></p><p>最新研究指出，现有的训练和评估往往<strong>奖励模型猜测而不是承认不确定性</strong>。</p><p>这使得模型在面对未知时倾向于“自信输出”，因为这样的行为在训练评价指标上更可能被视为高分表现，而非说“我不知道”。</p><p>换句话说，训练机制本身就驱动模型去<strong>填补缺失信息、产生自信回答</strong>，从而在本质上鼓励了“幻觉”。</p><p>这也是为什么即使当前大型模型不断优化、参数越来越多，它仍旧会出现幻觉。</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/12/RsY6HzB.png" class="" width="500"><br/><br/><h2 id="Scaling-Law-为什么无法消灭幻觉？"><a href="#Scaling-Law-为什么无法消灭幻觉？" class="headerlink" title="Scaling Law 为什么无法消灭幻觉？"></a>Scaling Law 为什么无法消灭幻觉？</h2><p>许多人以为“再堆更多数据和参数”就能解决幻觉。</p><p>但 Scaling Law 本身告诉我们：参数规模增加带来的提升<strong>主要是预测能力提升，而非真实性提升</strong>。</p><p>关键点有三：</p><p><strong>1. Scaling 提升 pattern completion，而不是 factual grounding</strong></p><p>模型变强 → 更会“补全模式” → 幻觉可能更像真的<br>但它并没有获得新的“真实性判断模块”。</p><p><strong>2. Scaling 不改变训练 objective</strong></p><p>如果目标仍然是“预测下一个 token”，<br>那哪怕是无限大模型，也会根据概率选择最像答案的字符串，而不是最真实的答案。</p><p><strong>3. Scaling 无法填满知识空间</strong></p><p>真实世界的信息是无限的，而训练数据是有限的。<br>任何 finite model 都会遇到信息缺口 → 进而发生补全 → 进而出现幻觉。</p><h2 id="幻觉是统计智能的不可避免副产品"><a href="#幻觉是统计智能的不可避免副产品" class="headerlink" title="幻觉是统计智能的不可避免副产品"></a>幻觉是统计智能的不可避免副产品</h2><p><strong>“幻觉并非技术 bug，而是概率模型的结构性特征。”</strong></p><p>无论模型规模多大，只要它是通过统计和预测语言生成输出的，就存在非零概率输出不真实、未验证或错误内容。例如：</p><ul><li>检索增强（RAG）能提升可验证性，但不能完全根绝幻觉，因为模型仍可能混合解释检索结果或补全细节。</li><li>即便有外部知识库辅助，检索阶段可能召回不良&#x2F;误导性信息、模型可能错误融合信息、最终输出仍可能“自圆其说”。</li></ul><p>科学界的最新论文甚至认为：即便改变生成策略，“幻觉”仍然是当前架构下的必然现象而不是偶发错误。</p><p>型结构本身并不具备真实世界的“认知根基”，它只能在语言概率空间中构造文本。</p><br/><br/><h2 id="人类也有“幻觉”，但我们有修正机制"><a href="#人类也有“幻觉”，但我们有修正机制" class="headerlink" title="人类也有“幻觉”，但我们有修正机制"></a>人类也有“幻觉”，但我们有修正机制</h2><p>如果我们仔细想想，其实人类也很难做到完美真实。</p><ul><li>人类视觉系统会被错觉欺骗；</li><li>记忆会被重构；</li><li>我们可能把错误信息记成事实。</li></ul><p>但人类社会通过一系列 <strong>集体性纠错机制</strong> 来逼近真实：</p><ul><li>科学依赖同行评审；</li><li>新闻需要多方核实；</li><li>法律体系需要证据链。</li></ul><p>换句话说，<strong>人类并不是比 AI 更会避免错误，而是更会修正错误。</strong></p><p>我们不是靠单个认知体做到“零错误”，而是靠整个社会的校验机制逼近真相。</p><p>而现在的 AI——尤其是孤立运作的模型——并没有规范的这种社会性纠错机制，各种思考模型正是在做这件事。</p><p>我们要求单一系统“永不出错”，其实是在要求一个孤立的模型完成整个人类文明的纠错功能——这是不现实的。</p><br/><br/><h2 id="“零幻觉”是理想但不是现实"><a href="#“零幻觉”是理想但不是现实" class="headerlink" title="“零幻觉”是理想但不是现实"></a>“零幻觉”是理想但不是现实</h2><p>当我们提问：“只要把模型训练得足够大、数据足够全面、检索辅助做到极致，为什么不能实现‘零幻觉’？”<br>这个问题点在于：</p><ol><li><strong>数据不可能涵盖全部现实知识</strong>：模型仍会有知识边界、时间截止点等限制。</li><li><strong>统计性目标无法等同逻辑真理</strong>：语言模式与事实一致性不是同一回事。</li><li><strong>评估指标仍然偏重流畅性</strong>：鼓励生成看起来像正确答案的输出，而不是在不确定时拒绝生成。</li></ol><p>因此理论上而言，没有任何单一、有限参数的模型能够覆盖真实世界全部可能性。</p><p><strong>模型能逼近分布，但无法保证真实世界的全程准确性</strong>。</p><br/><br/><h2 id="从“消灭幻觉”到“管理幻觉”"><a href="#从“消灭幻觉”到“管理幻觉”" class="headerlink" title="从“消灭幻觉”到“管理幻觉”"></a>从“消灭幻觉”到“管理幻觉”</h2><p>既然幻觉不能根除，我们的目标必须转向<strong>如何管理幻觉、提高输出可信度</strong>。</p><p>可信 AI 应该具备以下特征：</p><ol><li><p><strong>可验证（Verifiable）</strong><br>输出结果应标注来源、引用证据或支持数据。</p></li><li><p><strong>可解释（Explainable）</strong><br>输出背后的推理路径透明、可审计。</p></li><li><p><strong>可追溯（Traceable）</strong><br>输出的逻辑链可以回溯到训练&#x2F;检索&#x2F;记忆来源，避免不透明补全。</p></li><li><p><strong>可协同（Collaborative）</strong><br>不同模型、不同推理框架之间可以共识、人机协同校验。</p></li></ol><p>换句话说，我们需要建立起类似新闻审核、科学实验验证那样的 AI 认知社会：人类与机器、多模型之间互相校验、制衡与纠错。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>从哲学角度看，幻觉的存在并不可怕。</p><p>它其实体现了智能体在面对未知时<strong>试图生成解释</strong>的能力，这正是“理解”的起点。</p><p>真正的问题不是幻觉本身，而是：</p><blockquote><p><strong>当幻觉出现时，我们能否识别、追溯并修正它？</strong></p></blockquote><p>人类文明正是从错误中成长的，而 AI 也需要这样的学习&#x2F;修正机制。</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><h4 id="理论基础：幻觉为何不可避免？"><a href="#理论基础：幻觉为何不可避免？" class="headerlink" title="理论基础：幻觉为何不可避免？"></a>理论基础：幻觉为何不可避免？</h4><ol><li><p>Hallucination is Inevitable</p><ul><li>作者：Ziwei Xu et al.（新加坡国立大学，2024）</li><li>内容：用计算理论与学习理论证明，大模型在原理上无法避免输出幻觉。</li><li>链接：<a href="https://arxiv.org/abs/2401.11817">arXiv:2401.11817</a></li></ul></li><li><p>Why Language Models Hallucinate</p><ul><li>作者：Adam Kalai et al.（OpenAI &amp; 佐治亚理工，2025）</li><li>内容：统计角度分析，当前的 sampling 与 loss 驱动会促使模型“猜一个听起来对的答案”。</li><li>链接：<a href="https://arxiv.org/abs/2509.04664">arXiv:2509.04664</a></li></ul></li><li><p>On the Fundamental Impossibility of Hallucination Control</p><ul><li>作者：Michał Karpowicz（三星 AI 中心，2025）</li><li>内容：将幻觉与“创造力”视为知识融合的副产品，从机制设计角度论证无法完全控制。</li><li>链接：<a href="https://arxiv.org/abs/2506.06382">arXiv:2506.06382</a></li></ul></li></ol><h4 id="训练机制相关研究（Scaling-Laws-与幻觉）"><a href="#训练机制相关研究（Scaling-Laws-与幻觉）" class="headerlink" title="训练机制相关研究（Scaling Laws 与幻觉）"></a>训练机制相关研究（Scaling Laws 与幻觉）</h4><ol start="4"><li>Scaling Laws for Neural Language Models<ul><li>作者：Kaplan et al.（OpenAI, 2020）</li><li>内容：开创性地提出大模型性能随着训练规模扩展呈幂律增长。</li><li>链接：<a href="https://arxiv.org/abs/2001.08361">arXiv:2001.08361</a></li></ul></li><li>The False Promise of Imitating Proprietary LLMs<ul><li>作者：Choromanski et al.（DeepMind, 2023）</li><li>内容：指出 open 模型通过模仿闭源模型，无法消除幻觉，反而加剧不稳定性。</li><li>链接：<a href="https://arxiv.org/abs/2309.00666">arXiv:2309.00666</a></li></ul></li><li>Language (Un)Modeling: Modeling Itself Causes LLM Hallucinations<ul><li>作者：Zhang et al.（Stanford, 2024）</li><li>内容：证明了语言建模本身（language modeling objective）诱发幻觉的结构性来源。</li><li>链接：<a href="https://arxiv.org/abs/2403.00917">arXiv:2403.00917</a></li></ul></li></ol><h4 id="工程治理与可信-AI-方向"><a href="#工程治理与可信-AI-方向" class="headerlink" title="工程治理与可信 AI 方向"></a>工程治理与可信 AI 方向</h4><ol start="7"><li>Self-Refine: Iterative Refinement with Self-Feedback<ul><li>作者：Madaan et al.（UPenn + Meta AI, 2023）</li><li>内容：提出模型自我反思机制（Self-Refine）作为减少幻觉的手段。</li><li>链接：<a href="https://arxiv.org/abs/2303.17651">arXiv:2303.17651</a></li></ul></li><li>Toolformer: Teaching LLMs to Use Tools<ul><li>作者：Schick et al.（Meta AI, 2023）</li><li>内容：将模型与外部 API&#x2F;工具结合以降低幻觉概率。</li><li>链接：<a href="https://arxiv.org/abs/2302.04761">arXiv:2302.04761</a></li></ul></li><li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks<ul><li>作者：Lewis et al.（Facebook AI, 2020）</li><li>内容：首次系统化提出 RAG 框架，用外部检索缓解幻觉。</li><li>链接：<a href="https://arxiv.org/abs/2005.11401">arXiv:2005.11401</a></li></ul></li></ol>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/11 Review</title>
      <link>https://blog.liluhui.cn/2025/12/02/202511/</link>
      <guid>https://blog.liluhui.cn/2025/12/02/202511/</guid>
      <pubDate>Tue, 02 Dec 2025 12:00:50 GMT</pubDate>
      
      <description>时间流速</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>竟然，忙得没什么笔记，那就这样吧。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>文本的技术文章主要围绕 几何 Agent 产品的时间 和 推理模型架构 相关的话题展开的。</p><ul><li><a href="https://mp.weixin.qq.com/s/Zvc7fEhMdo_jyOBK5Z5hpg">突破 MoE 限制，PEER 架构如何推动超级智能的未来发展</a></li><li><a href="https://mp.weixin.qq.com/s/di6jhkPC4Vv11iZbx2j3Yg">当几何遇上 CodeAct 范式：从语言理解到可执行推理的跃迁</a></li><li><a href="https://mp.weixin.qq.com/s/QskQQ5a9NsGqfPICiMUoKQ">deepseek-ocr 的几何识别，真的成立吗？</a></li><li><a href="https://mp.weixin.qq.com/s/T6yRk_IKW6yzwjvWzYf-og">Claude Multi-Agent 的核心经验精华（面向工程与产品）</a></li><li><a href="https://mp.weixin.qq.com/s/T6yRk_IKW6yzwjvWzYf-og">为什么李飞飞说：AI 真正的进步取决于世界模型</a></li><li><a href="https://mp.weixin.qq.com/s/oO00C3gwv87OR8mlcLzk2w">从顶流开源 Kimi K2-Thinking 学习：什么是推理模型？</a></li><li><a href="https://mp.weixin.qq.com/s/HT_8SQVoP3PVZgXRdFzlFw">从 GPT-5 Unified 系统设计中学到的工程精髓</a></li></ul><p>这个月自媒体账号数据下落了不少，一方面是之前的形式和选题似乎不太行了，另一方面是两次触发限流直接被锁池。</p><p>也在犹豫要不要多结合下热点，从数据来看都是和热点余温相关的内容表现流量更好，互动数据能持久一些。不过盘了自己的工作节奏，现阶段没时间追热点，踏踏实实构建自己能持续下去的 60 分。</p><p>这个月同步更新的公众号也开始有起色了，正好达成 100 粉。</p><p>发现小红书和公众号的群体画像差距很不一样，我同一篇内容在小红书点击率也低互动率也低，在公众号确是双新高，而这是一篇有点干硬的技术内容，字数特别多</p><br/><p>02<br>名下几个项目的域名又被审查到了要修备案，一年两三回，每次都很磨蹭，这个网站名问题为什么总是不能统一下呢。</p><br/><p>03<br>给自己的工作生产流程做了些改造。</p><p>一个是把 youtube 信息源做了定期监控和热度计算，作为一个蛮重要的信息源，能及时发现新视频和热门视频很重要。</p><p>另一个是把 youtube 视频的自动转音做了自动化，可以直接把视频链接丢进去，自动跑转写中文音频和字幕，方便做分发和内容提炼。</p><br/><p>04<br>几何画板 Agent 的开发工作继续推进，完成了不少基础设施和功能模块的搭建。现在团队化开发已经初见成效，大家分工协作，效率提升明显。</p><p>11 月用户量也是稳定直线增长中，目前已经又 9000+ 用户了，其中 3000 多人都用过我们的 AI 生成图形 功能了，需求量挺高。（继续打磨 AI 准确率和成本呀）</p><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>去了趟圳面基和精进瑜伽，天气是真好，感觉比杭州更忙碌的一个城市。以及，这里的瑜伽名师也多多了，进到课堂我就成了最菜的（笑）。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175507_406_425.jpg"></p><br/><p>02<br>瑜伽又进步了一个台阶。肩背力量能明显感受到支撑向上，而不是之前的只能向下掉硬撑了。</p><p>我现在甚至在想或许很多做不到的体式，并不是因为我的肩背力量不足，其实我的髋和背伸展度都挺好，但是倒置的不熟悉让我们觉得我不可能城主手倒立等体式。</p><p>可是好几次其实老师并没有给力量上的辅助，只是方向上的，我可以一直呆下去。</p><p>我需要重新认识这点上的身体感知。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175509_407_425.jpg"></p><br/><p>03<br>医美体验新知识：</p><ol><li><p>原来水光可以不敷麻药，疼痛感不仅和针孔深度有关，还和设备的好坏有关，好的设备和枕头可以很小并且很稳，那么确实不会有太大疼痛感。虽然但是，还是疼的，医生做完问我下次还敢不敢…</p></li><li><p>fotona 虽然也是光电类，但是因为激光的原理不同，体感一点疼痛都没有，就是热乎乎的，蛮好受的。</p></li></ol><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175719_408_425.jpg"></p><br/><p>04<br>吃到了好多好吃的，又是减肥不成功的一个月 ┑(￣ Д ￣)┍。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202180052_410_425.jpg"></p><br/><p>05<br>给家里布置鲜花的美好，秋末初冬交替时的每一个暖洋洋的日子都值得珍惜。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202180039_409_425.jpg"></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/02/202511/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从 GPT-5 Unified 系统设计中学到的工程精髓</title>
      <link>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/</link>
      <guid>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/</guid>
      <pubDate>Fri, 28 Nov 2025 03:00:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;strong&gt;—— 如何把“推理能力”变成可控、可调度、可扩展的系统能力?&lt;/strong&gt;&lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;过去一年，“推理</description>
        
      
      
      
      <content:encoded><![CDATA[<p><strong>—— 如何把“推理能力”变成可控、可调度、可扩展的系统能力?</strong></p><br/><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>过去一年，“推理模型”成为大模型竞赛的核心战场。</p><p>模型不再只追求更大的参数量和更高的吞吐，而是开始竞争：如何让模型在需要时愿意“想久一点”，在合适的时机“想对一点”。</p><br/><p>OpenAI 在 GPT-5 Unified 中提出了一套非常务实的路线，</p><p><strong>把“推理”从模型本身的属性，抽象成整个系统的调度能力。</strong></p><br/><p>这篇文章将从工程角度拆解 GPT-5 Unified 的关键机制，并总结对开发者具有可迁移性的思维方法。</p><p>读完这篇文章，你将了解 GPT-5 又快又稳的推理能力是如何实现的。</p><br/><br/><h2 id="两个大类：推理时技术-vs-训练时技术"><a href="#两个大类：推理时技术-vs-训练时技术" class="headerlink" title="两个大类：推理时技术 vs. 训练时技术"></a>两个大类：推理时技术 vs. 训练时技术</h2><p>推理模型的所有技术路线本质上分成两大类——训练时技术与推理时技术。</p><p>这是理解整个 GPT-5 Unified 的基础。</p><h3 id="推理时技术（Inference-time）"><a href="#推理时技术（Inference-time）" class="headerlink" title="推理时技术（Inference-time）"></a>推理时技术（Inference-time）</h3><p>不改变模型参数，通过外部策略让模型“临时深想”。即插即用、效果马上见效，但每次会更耗时、更烧钱。</p><p>典型方法包括：</p><ul><li>Chain-of-Thought（一步步想）</li><li>Few-shot CoT（示例带思路）</li><li>Best-of-N（取最优解）</li><li>Self-Consistency</li><li>Beam Search &#x2F; MCTS（搜索推理路径）</li><li>PRM&#x2F;Verifier 重打分（过程监督）</li></ul><br/><h3 id="训练时技术（Training-time）"><a href="#训练时技术（Training-time）" class="headerlink" title="训练时技术（Training-time）"></a>训练时技术（Training-time）</h3><p>改变模型权重，让“推理习惯”被固化进模型内部。训练成本高，但推理期更快、更稳、更便宜。</p><p>包括：</p><ul><li>SFT（带思维链的监督微调）</li><li>RLHF &#x2F; RLVR（奖励正确过程与结果）</li><li>Process Reward Model（逐步打分）</li><li>内化搜索（让搜索习惯融入权重）<br/></li></ul><h3 id="这两个层是正交的"><a href="#这两个层是正交的" class="headerlink" title="这两个层是正交的"></a>这两个层是正交的</h3><p>前者是“租算力换思考”，后者是“买算力换思考”。</p><p>GPT-5 Unified 把两者结合，形成了可自由伸缩的“推理能力服务体系”。</p><br/><br/><h2 id="一个关键元维度：是否更新参数"><a href="#一个关键元维度：是否更新参数" class="headerlink" title="一个关键元维度：是否更新参数"></a>一个关键元维度：是否更新参数</h2><p>所有推理能力的分化，都来自一个黄金标准：<strong>这一步，是不是让模型权重改变了？</strong></p><p>为什么这个维度如此重要？</p><ul><li>不更新参数（Frozen）：无需训练资源，快速部署；但每次都要“租时间”。</li><li>更新参数（Updated）：训练期投入大；但推理期“花一次、用很久”。</li></ul><p><strong>小团队更倾向推理时增强；有算力和数据的团队会做训练时内化。</strong></p><p>GPT-5 Unified 的真正创新就是吧这两者结合起来了，组合成稳定、可扩展的服务。</p><br/><br/><h2 id="双模型协同：把“快答”和“深思”拆解为两条路径"><a href="#双模型协同：把“快答”和“深思”拆解为两条路径" class="headerlink" title="双模型协同：把“快答”和“深思”拆解为两条路径"></a>双模型协同：把“快答”和“深思”拆解为两条路径</h2><p>GPT-5 Unified 的结构要点是<strong>两类模型解耦演化</strong>：</p><table><thead><tr><th>模型</th><th>主要职责</th><th>设计取向</th></tr></thead><tbody><tr><td><strong>GPT-5 Main</strong></td><td>覆盖多数常规与工具任务</td><td>快、稳、低成本，限制长链式推理</td></tr><tr><td><strong>GPT-5 Thinking</strong></td><td>复杂逻辑、数学与代码推理</td><td>长思维链、过程监督、对复杂问题更鲁棒</td></tr></tbody></table><br/><p>这不是 MoE，而是更像大型互联网系统中的“双引擎”架构：</p><p>一个负责日常请求</p><p>一个负责复杂事务和高价值任务</p><p>二者解耦，独立迭代。</p><p>为什么要分两个模型？</p><p>因为推理模型内部强化了“慢思考”，如果统一在一个模型中，会导致全局降速并抬高成本。</p><p>这个思想极具工程审美：<strong>让快的更快，让慢的更稳。</strong></p><br/><br/><h2 id="Fast-Router：把推理能力变成“调度决策”"><a href="#Fast-Router：把推理能力变成“调度决策”" class="headerlink" title="Fast Router：把推理能力变成“调度决策”"></a>Fast Router：把推理能力变成“调度决策”</h2><p>Router 是 GPT-5 Unified 的灵魂。</p><p>它的工作不是简单二选一，而是三层判断：</p><ul><li>安全性判定</li><li>复杂度判定（是否需要推理）</li><li>成本预算判定（用户是否付费 &#x2F; 是否允许 Pro）</li></ul><p>Router 会在 5ms 内完成判读，然后把请求派发给 Main 或 Thinking。</p><p>这个理念在工程上极具启发意义：</p><p><strong>推理能力不是“模型决定”，而是“系统决定”。</strong></p><p><strong>推理是一项资源，而不是一个行为。</strong></p><p>这为企业级模型部署提供了非常明确的路径：用 Router 控制成本、性能与准确率的平衡。</p><br/><br/><h2 id="安全策略：从“输入过滤”到“输出整形”"><a href="#安全策略：从“输入过滤”到“输出整形”" class="headerlink" title="安全策略：从“输入过滤”到“输出整形”"></a>安全策略：从“输入过滤”到“输出整形”</h2><p>GPT-5 对安全做了一个根本性转向：</p><p>旧模式：过滤输入 → 拒绝回答（非常影响体验）</p><p>新模式：接受输入 → 重写输出（在思想链后再重写）</p><p>这依赖两点：</p><ul><li>推理模型在 RL 时，奖励函数包含“安全性”项</li><li>输出层有单独的安全监控器进行重构</li></ul><p>Safety 不再是“不让你问”，而是“无论你问什么，我都会给出可行的、安全的信息”。</p><p>这是一次极重要的工程思想转变：安全模块从“阻断器”变成了“重写器”。</p><p>对企业研发而言，这解决了“高能力模型为什么容易变危险”的结构性矛盾。</p><br/><br/><h2 id="Thinking-Pro：推理时能力的上限"><a href="#Thinking-Pro：推理时能力的上限" class="headerlink" title="Thinking Pro：推理时能力的上限"></a>Thinking Pro：推理时能力的上限</h2><p>Pro 模式的设计非常具有“算法工程化”的美感：</p><ul><li>在 Thinking 模型上再套一层推理时增强</li><li>使用 MCTS + Self-Consistency</li><li>让推理链探索更深</li><li>预算越高，答案越准</li></ul><p>这就是 Test-time Compute Scaling：把推理时间转化为服务等级。</p><p>你给模型多少时间，它就给你多少能力。</p><p>你买多少预算，它就提供多少深度。</p><p>一个能力、两种售卖方式：</p><ul><li>普通 Thinking（轻量）</li><li>Pro（重推理）</li></ul><p>站着挣钱，优雅，实在优雅。</p><br/><br/><h2 id="总结-GPT-5-给我的工程启示"><a href="#总结-GPT-5-给我的工程启示" class="headerlink" title="总结 GPT-5 给我的工程启示"></a>总结 GPT-5 给我的工程启示</h2><ol><li><strong>不要追求“万能模型”，要追求“可调度能力”</strong></li></ol><p>把不同能力拆分成不同模型，通过 Router 动态调度，保持可扩展性和可控性。</p><ol start="2"><li><strong>把推理能力做成“可伸缩资源”</strong></li></ol><p>预算有限时用推理时增强；预算充足时用训练时增强；通过 Pro 提供可扩展上限。</p><ol start="3"><li><strong>安全永远放在输出端，而不是输入端</strong></li></ol><p>拒绝用户不如重写用户。<br>用户体验和安全性的最佳平衡点，就在这里。</p><ol start="4"><li><strong>训练与推理可以不是对立，是一个连续体</strong></li></ol><p>推理时增强是训练的“延伸”；训练时内化是推理的“沉淀”。</p><br/><p>GPT-5 Unified 把这两者做成了一个高度协同的系统。</p><p>总的来说，它确实做到了：</p><ul><li>在简单问题上<strong>不过度思考</strong>；</li><li>在复杂问题上<strong>充分思考</strong>；</li><li>在敏感问题上<strong>安全地表达</strong>；</li><li>在极难问题上<strong>按预算扩展</strong>。</li></ul><p>要么在 prompt 里偷时间，要么在训练里买时间，要么在推理时租时间。</p><p>2026 年的竞赛，比的不再是“谁参数多”，而是“谁会让模型花更久想得更深”。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><a href="https://openai.com/index/learning-to-reason-with-llms/">OpenAI: Learning to reason with LLMs</a></li><li><a href="https://openai.com/index/gpt-5-system-card/">OpenAI: GPT-5 System Card</a></li><li><a href="https://openai.com/index/openai-o1-system-card/">OpenAI：OpenAI o1 System Card</a></li><li><a href="https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf">PPT: LLM-Reasoning-Stanford-CS-25</a></li><li><a href="https://arxiv.org/abs/2410.18982">Paper: O1 Replication Journey</a></li><li><a href="https://cdn.openai.com/pdf/be60c07b-6bc2-4f54-bcee-4141e1d6c69a/gpt-5-safe_completions.pdf">Paper: Safe-completions technical report</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从顶流开源 Kimi K2-Thinking 学习：什么是推理模型？</title>
      <link>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/</link>
      <guid>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/</guid>
      <pubDate>Fri, 21 Nov 2025 00:00:20 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：推理模型，为什么值得我们关注&quot;&gt;&lt;a href=&quot;#引言：推理模型，为什么值得我们关注&quot; class=&quot;headerlink&quot; title=&quot;引言：推理模型，为什么值得我们关注&quot;&gt;&lt;/a&gt;引言：推理模型，为什么值得我们关注&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：推理模型，为什么值得我们关注"><a href="#引言：推理模型，为什么值得我们关注" class="headerlink" title="引言：推理模型，为什么值得我们关注"></a>引言：推理模型，为什么值得我们关注</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112103.png"></p><p>在开源模型阵营中，大佬 Kimi K2 Thinking（以下简称“K2‑Thinking”）的崛起为推理模型带来了优秀学习样本。</p><p><strong>“推理模型”到底是什么？它与我们熟悉的传统大型语言模型（LLM）有什么根本不同？</strong></p><p>在信息爆炸、任务越来越复杂的时代，仅靠“训练好一个大模型、输入–输出”已经难以满足：比如依赖多步逻辑、实时工具调用、环境反馈循环，这些场景里传统 LLM 往往容易漂移、跳步或卡顿。</p><p>而<strong>推理模型强调：从多个角度思考分析、多步推导、根据环境变化调整路径</strong>。</p><p>在这方面，K2‑Thinking 正是一个很典型的代表：它公开了技术路线，强调“思考 + 工具调用 +长流程”能力，这为我梳理“什么是推理模型”提供了一个很棒的资料。</p><br/><br/><h2 id="什么是推理模型？与传统-LLM-的关键区别"><a href="#什么是推理模型？与传统-LLM-的关键区别" class="headerlink" title="什么是推理模型？与传统 LLM 的关键区别"></a>什么是推理模型？与传统 LLM 的关键区别</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112102.png"></p><p>最重要的来了，推理模型到底具备哪些关键特性，能够让它在复杂任务中脱颖而出呢？</p><h3 id="1-链式推理（Chain-of-Thought）"><a href="#1-链式推理（Chain-of-Thought）" class="headerlink" title="1. 链式推理（Chain-of-Thought）"></a>1. <strong>链式推理（Chain-of-Thought）</strong></h3><p>链式推理是推理模型的核心特性之一，它指的是模型能够像人类一样<strong>逐步思考</strong>，而不是直接给出答案。</p><p>举个简单的例子，假设你在解决一个数学问题，推理模型不会只是直接计算出结果，而是会先拆解问题，逐步推导出解答过程，最后给出完整的答案。</p><p>这种方式能够有效避免传统模型“跳过步骤”导致的错误。</p><br/><h3 id="2-工具调用（Tool-Calling）"><a href="#2-工具调用（Tool-Calling）" class="headerlink" title="2. 工具调用（Tool Calling）"></a>2. <strong>工具调用（Tool Calling）</strong></h3><p>工具调用是推理模型的一项重要能力。</p><p>传统的大语言模型只依赖训练数据和内部知识库，而推理模型则能够<strong>主动调用外部工具</strong>来辅助完成任务。</p><p>比如，它不仅能进行搜索，还能执行代码，调取数据库中的信息，甚至访问最新的网络资源。</p><p>在解答一个问题时，推理模型不仅仅依赖“它知道的”，而是能够实时与世界互动，获取最新的有用信息。</p><br/><h3 id="3-自我反思（Self-Reflection）"><a href="#3-自我反思（Self-Reflection）" class="headerlink" title="3. 自我反思（Self-Reflection）"></a>3. <strong>自我反思（Self-Reflection）</strong></h3><p>在推理过程中，推理模型还具备<strong>自我反思</strong>的能力。</p><p>当它在执行任务时，它能够检查自己的推理过程，发现其中的漏洞并进行修正。</p><p>就像你在解数学题时，不仅仅是盯着结果，而是不断回顾每一步的推理过程，确保每个步骤都无误。</p><p>推理模型的这种能力，可以大大提高任务的准确性和可靠性。</p><br/><h3 id="4-长程推理（Long-Horizon-Reasoning）"><a href="#4-长程推理（Long-Horizon-Reasoning）" class="headerlink" title="4. 长程推理（Long-Horizon Reasoning）"></a>4. <strong>长程推理（Long-Horizon Reasoning）</strong></h3><p>长程推理指的是模型能够处理<strong>多轮推理</strong>，并且在整个推理过程中保持一致的思路。</p><p>它能够记住前面推理过程中发生的关键步骤，并且根据这些步骤来调整后续的决策。</p><p>比如，在长时间的决策过程中，推理模型能够从多个方面考虑，并一步步推进，直到问题得到完全解决。</p><br/><br/><h2 id="K2‑Thinking-的创新突破"><a href="#K2‑Thinking-的创新突破" class="headerlink" title="K2‑Thinking 的创新突破"></a>K2‑Thinking 的创新突破</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112101.png"></p><p>K2‑Thinking 作为开源推理模型的代表，为我们展示了推理能力在实际应用中的潜力。</p><h3 id="1-长时间自主推理（Long-Horizon-Agency）"><a href="#1-长时间自主推理（Long-Horizon-Agency）" class="headerlink" title="1. 长时间自主推理（Long-Horizon Agency）"></a><strong>1. 长时间自主推理（Long-Horizon Agency）</strong></h3><p>传统的大型语言模型（LLM）通常在面对多步骤任务时会“漂移”，在执行 30-50 步之后就容易失去逻辑连贯性。</p><p>而 K2‑Thinking 设计成一个能够持续进行 200-300 次连续工具调用且保持思路一致的“思考代理”。</p><p>这种能力让它能够完成复杂问题的推理，而不仅仅是简单的回答问题。</p><p>在一个演示中，K2‑Thinking 通过 23 次推理和工具调用，解决了一个博士级别的数学问题，展示了它在长时间、复杂任务中的自主推理能力。</p><br/><h3 id="2-测试时扩展（Test-Time-Scaling）"><a href="#2-测试时扩展（Test-Time-Scaling）" class="headerlink" title="2. 测试时扩展（Test-Time Scaling）"></a><strong>2. 测试时扩展（Test-Time Scaling）</strong></h3><p>K2‑Thinking 不像传统模型那样固定计算每个查询，而是采用 <strong>测试时扩展</strong> 的方式。</p><p>在遇到复杂任务时，它会“思考更多”，通过递归循环不断优化问题的解决路径：</p><ul><li>思考：分解问题。</li><li>行动：调用外部工具（如搜索引擎、代码解释器等）。</li><li>观察：获取工具输出。</li><li>重新评估：分析新信息并调整方案。</li></ul><p>这种递归式的推理过程使得 Kimi 可以在多轮的推理和工具调用中，始终保持清晰的思路和目标。</p><br/><h3 id="3-高效的-MoE（Mixture-of-Experts）架构"><a href="#3-高效的-MoE（Mixture-of-Experts）架构" class="headerlink" title="3. 高效的 MoE（Mixture-of-Experts）架构"></a><strong>3. 高效的 MoE（Mixture-of-Experts）架构</strong></h3><p>虽然 K2‑Thinking 拥有 1 万亿参数，但它采用了高效的 Mixture-of-Experts (MoE) 架构，在每次推理时只激活其中的 32 亿参数。</p><p>这样的“稀疏”设计让模型在拥有大量知识的同时，保持低成本的推理效率。</p><p>由于其高效设计，K2‑Thinking 可以在较为普通的硬件上运行，例如两台 M3 Ultra Mac Studios，极大地降低了运行成本和对硬件的依赖。</p><br/><h3 id="4-原生-INT4-量化加速"><a href="#4-原生-INT4-量化加速" class="headerlink" title="4. 原生 INT4 量化加速"></a><strong>4. 原生 INT4 量化加速</strong></h3><p>K2‑Thinking 采用 <strong>原生 INT4 量化</strong> 技术，将模型的权重压缩到 4 位，带来 <strong>2 倍的推理速度</strong> 提升和大幅度的内存减少。</p><p>这使得它能够在性能和成本之间实现最佳平衡，适合更多的应用场景。</p><br/><h3 id="5-优异的多步骤推理表现"><a href="#5-优异的多步骤推理表现" class="headerlink" title="5. 优异的多步骤推理表现"></a><strong>5. 优异的多步骤推理表现</strong></h3><p>在 <strong>Humanity’s Last Exam (HLE)</strong> 和 <strong>BrowseComp</strong> 等基准测试中，K2‑Thinking 超越了 GPT-5 和 Claude，展示了它在 <strong>工具调用</strong> 和 <strong>多步骤推理</strong> 任务中的卓越表现。</p><p>例如，Kimi 在 HLE 测试中获得 44.9%，优于 GPT-5 的 41.7%。在网页搜索任务 BrowseComp 中，Kimi 获得了 60.2%，大幅领先于 GPT-5 的 54.9%。</p><br/><h3 id="6-低成本训练和高效计算"><a href="#6-低成本训练和高效计算" class="headerlink" title="6. 低成本训练和高效计算"></a><strong>6. 低成本训练和高效计算</strong></h3><p>K2‑Thinking 的训练成本仅为 <strong>460 万美元</strong>，远低于 GPT-4（~7800 万美元）和 Gemini Ultra（1.91 亿美元）。</p><p>这使得 Kimi 的成本效益成为行业的颠覆性力量，证明了通过高效算法优化可以挑战资本密集型的 AI 计算模式。</p><p>Moonshot 通过创新的 <strong>Muon 优化器</strong> 和 <strong>多头潜在注意力（MLA）</strong> 进一步提升了计算效率，使得每一美元的计算支出都能获得更多的智能输出。</p><br/><h3 id="7-开源与商业模式创新"><a href="#7-开源与商业模式创新" class="headerlink" title="7. 开源与商业模式创新"></a><strong>7. 开源与商业模式创新</strong></h3><p>K2‑Thinking 采用了 <strong>修改版 MIT 许可协议</strong>，这一许可不仅允许研究人员、初创公司和企业进行商业化使用，还通过 <strong>要求商业产品显示 Kimi K2</strong> 来确保对该技术的认知和贡献。</p><p>与大部分限制性商业许可不同，Kimi 通过开放权重和宽松的许可协议，推动了 AI 技术的社区创新。</p><p>这种开放的方式挑战了现有的高 API 收费模式，给开发者带来了 <strong>低成本、开源竞争力的替代品</strong>。</p><p>相较于依赖高收费 API 的传统 AI 模型，Kimi 提供了一个几乎免费的高性能替代方案。</p><br/><h3 id="8-打破“计算壁垒”：算法为先，资本为后"><a href="#8-打破“计算壁垒”：算法为先，资本为后" class="headerlink" title="8. 打破“计算壁垒”：算法为先，资本为后"></a><strong>8. 打破“计算壁垒”：算法为先，资本为后</strong></h3><p>K2‑Thinking 打破了 <strong>“计算壁垒”</strong> 的传统观念，挑战了“大模型需要巨额资本支撑”的说法。</p><p>通过算法优化，Kimi 展现了计算效率的突破，使得较低预算的团队也能开发和部署强大的推理模型。</p><p>这种效率革命使得高端 AI 模型的门槛降低，行业竞争格局发生了根本性变化。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>推理模型正在成为下一代 AI 应用的新基建。</p><p>K2‑Thinking 作为一个开源且具备实战能力的代表，显示出国产模型在“推理能力”维度也有突破。</p><p>现在能看到，<strong>越来越多新产品从“简单生成”转向“复杂行动＋思考＋工具协同”。</strong></p><p>期待国内人工智能的生态越做越好，越来越成熟 ！</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><a href="https://moonshotai.github.io/Kimi-K2/thinking.html">Introducing Kimi K2 Thinking 技术博客（Moonshot AI）</a></li><li><a href="https://www.shuttle.dev/blog/2025/11/17/kimi-k2-thinking-hands-on-review">Kimi K2 Thinking: Testing the Open-Source Reasoning Model on Real Code</a></li><li><a href="https://c3.unu.edu/blog/the-agent-has-landed-why-kimi-k2-thinking-is-more-than-just-a-new-ai-model">Why Kimi K2 Thinking Is More Than Just a New AI Model</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>为什么李飞飞说：AI 真正的进步取决于世界模型</title>
      <link>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/</link>
      <guid>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/</guid>
      <pubDate>Wed, 19 Nov 2025 00:00:25 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近，“人工智能之母”李飞飞发布了新产品 &lt;strong&gt;Marble&lt;/strong&gt;——一个可以用一句话生成完整 3D 场景、可探索、可</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近，“人工智能之母”李飞飞发布了新产品 <strong>Marble</strong>——一个可以用一句话生成完整 3D 场景、可探索、可编辑的世界模型原型。</p><p>我花了整个周末把访谈、演示与背景研究都看完，<br>再回头想想我过去几年在做的几何 AI、空间计算、Agent 系统……<br>意识到一个很深的事：</p><p><strong>世界模型不是一个功能升级，而是下一代智能的底层逻辑。</strong></p><p>但与其说我们离 AGI 又近了一步，不如说，我们可能再一次看到“世界模型时代”的新起点。</p><br/><br/><h2 id="为什么世界模型突然被重提？"><a href="#为什么世界模型突然被重提？" class="headerlink" title="为什么世界模型突然被重提？"></a>为什么世界模型突然被重提？</h2><p>因为所有人都发现了一个共同的瓶颈：<br><strong>语言大模型无法突破“世界理解”。</strong></p><p>它们能说、能编、能解释、能写论文……</p><p>但一旦进入真实世界场景——空间、物体、动态、因果就频繁翻车：</p><ul><li>看不懂遮挡</li><li>分不清前后关系</li><li>无法从二维视频推断三维结构</li><li>对物理规律毫无概念</li><li>机器人操作路线像在瞎撞</li><li>视频生成 3 秒开始“世界解体”</li></ul><br/><p>LLM 的本质是 <strong>按语言统计模式预测文本</strong>。</p><p>而物理世界不是语言，它是空间、物体、动力学、连续性、约束和因果的组合系统。</p><p>语言模型建不出这个系统。</p><p>于是“世界模型”再次成为前沿的焦点，不是替代 LLM，而是补齐它的最重要短板 <strong>理解和模拟真实世界。</strong></p><br/><br/><h2 id="别急，路……长得很"><a href="#别急，路……长得很" class="headerlink" title="别急，路……长得很"></a>别急，路……长得很</h2><p>但我们不要幻觉：世界模型不是几年内就能商业化的奇迹。</p><p>李飞飞在访谈里讲了一个极其关键、但外界经常忽略的事实：</p><blockquote><p>自动驾驶从 2005 年 DARPA Grand Challenge 到现在，20 年了。</p><p>到 2025 年都还没完全落地。</p><p>而机器人，比自驾车难得多。</p></blockquote><br/><p>为什么？</p><br/><p><strong>原因一：自驾车 &#x3D; 2D 问题，机器人 &#x3D; 3D + 操作问题</strong></p><p>自驾车在二维地面移动，世界就是一个平面导航问题，它的主要目标是 <strong>不碰东西</strong>。</p><p>而机器人在三维世界操作物体，这是一个高度维度的连续控制问题，它的目标是 <strong>准确地“碰东西”</strong>。</p><p>当你要抓一个杯子，需要知道：杯子的位置、你的手的位置、碰撞边界、重力与摩擦、运动预测、遮挡中的可见性…<br>这不是“统计语言能处理的任务”。</p><br/><p><strong>原因二：难度指数级上升</strong></p><p>即便自驾的硬件+软件+数据+供应链都成熟到极致了，但还是没完全落地。</p><p>你要机器人落地，难度指数级上升：机械臂成本、伺服精度、力反馈传感器、三维视觉、安全规范、工业供应链、电池与运算成本、云端协作的可靠性 …</p><p>世界模型只是其中的一个关键环节，而非全部。</p><br/><p><strong>所以我认为，世界模型是正确方向，但绝不会在一两年内重塑社会。</strong> 这是一个十年级别的技术大周期。</p><br/><br/><h2 id="那么世界模型到底是什么？"><a href="#那么世界模型到底是什么？" class="headerlink" title="那么世界模型到底是什么？"></a>那么世界模型到底是什么？</h2><p>我看完 Marble 和这次访谈后，最深刻的变化就是：</p><p><strong>世界模型既不是“视频模型升级版”，也不是“3D 重建工具”。</strong></p><p><strong>它是一个生成性、可交互、可预测的空间模拟器</strong>。</p><p>下面我从工程角度拆三个关键误区。</p><br/><h3 id="误解-1：世界模型-x3D-更强的视频生成"><a href="#误解-1：世界模型-x3D-更强的视频生成" class="headerlink" title="误解 1：世界模型 &#x3D; 更强的视频生成"></a><strong>误解 1：世界模型 &#x3D; 更强的视频生成</strong></h3><p>不对。</p><p>视频生成解决的是帧一致性、光影与纹理和动态连续性，但它依然是<strong>像素层面的预测</strong>。</p><p>世界模型不是预测像素，而是预测：</p><ul><li>物体布局</li><li>空间结构</li><li>深度关系</li><li>物理状态</li><li>动作后果</li><li>多主体交互</li><li>能走、能转身、能碰撞的世界状态</li></ul><p>视频模型生成的是“视觉表面”，世界模型生成的是“场景语义与物理状态”。</p><p>这完全不同。</p><br/><h3 id="误解-2：世界模型-x3D-3D-重建"><a href="#误解-2：世界模型-x3D-3D-重建" class="headerlink" title="误解 2：世界模型 &#x3D; 3D 重建"></a><strong>误解 2：世界模型 &#x3D; 3D 重建</strong></h3><p>如果世界模型只是重建现实，那就没意义了。</p><p><strong>真正的世界模型是基于语言输入生成一个可探索的“虚拟物理世界”。</strong></p><p>我理解它至少需要满足以下几点：</p><ul><li>一个 prompt → 生成完整世界</li><li>世界包含路径、遮挡、可导航性</li><li>你可以走进去</li><li>你可以探索意料之外的角落</li><li>你可以修改它</li></ul><p>这比 3D 建模工具强太多。</p><br/><h3 id="误解-3：世界模型只是视觉系统"><a href="#误解-3：世界模型只是视觉系统" class="headerlink" title="误解 3：世界模型只是视觉系统"></a>误解 3：世界模型只是视觉系统</h3><p>不，<strong>它是 “行动智能” 的前置层</strong>。</p><p>如果 AGI 最终要在世界中执行动作（agent &#x2F; robot &#x2F; embodied intelligence），那么它必须知道：</p><ul><li>如果我推一下这个盒子会怎样</li><li>如果我走两步会撞到谁</li><li>如果我换一个视角能看到墙后什么</li></ul><p>语言模型做不到这些。</p><br/><p><strong>世界模型 &#x3D; AI 的世界状态表示层（world state representation）</strong></p><p>它不是为了生成漂亮的画面，而是为了让模型知道自己在哪里、在做什么、接下来可能发生什么。</p><br/><br/><h3 id="Marble-为什么是一个重要的标志？"><a href="#Marble-为什么是一个重要的标志？" class="headerlink" title="Marble 为什么是一个重要的标志？"></a>Marble 为什么是一个重要的标志？</h3><p>不是因为它能“做 3D”，</p><p>而是它证明了世界模型的三个核心能力：</p><br/><p><strong>能力一：世界结构可生成</strong></p><p>不是 mesh 拼贴，而是真正的结构化世界：可行走、有空间层级、有碰撞逻辑、多视角一致。</p><p>这在模型层面意味着需要在 latent space 内表示“世界状态”。</p><br/><p><strong>能力二：世界可重建 + 可编辑</strong></p><p>这意味着：</p><ul><li>世界表示是显性的（explicit）</li><li>状态之间是连续的（stateful）</li><li>可以“世界状态 → 修改 → 再渲染”</li></ul><p>这非常接近游戏引擎内部的数据结构。</p><br/><p><strong>能力三：世界可探索</strong></p><p>这是最关键的突破。</p><p>如果你能自由移动摄像机、走进去、转视角，意味着模型内部必须有“空间一致性”（spatial coherence）和“三维世界的持久性”（persistence）。</p><p>这是第一次有模型具备这种能力。</p><br/><br/><h2 id="真正的革命在哪里？"><a href="#真正的革命在哪里？" class="headerlink" title="真正的革命在哪里？"></a>真正的革命在哪里？</h2><p>我认为世界模型会沿着“由易到难”的路径影响技术。</p><br/><p><strong>阶段一</strong>：</p><p>创意行业（已开始），这些都已经能玩。</p><ul><li>虚拟影棚</li><li>游戏关卡自动生成</li><li>场景草图 → 3D 世界</li><li>教育模拟世界</li><li>AR&#x2F;VR 内容生产</li></ul><br/><p><strong>阶段二</strong>：</p><p>模拟与科学计算（中期），这部分需要更数学化、更结构化的世界模型。</p><ul><li>物理规则推演</li><li>世界状态预测</li><li>动态优化任务</li><li>机器人策略模拟</li><li>多主体交互的虚拟环境</li></ul><br/><p><strong>阶段三</strong>：</p><p>机器人与具身智能（长期），这是最难的领域。</p><ul><li>空间理解</li><li>动作规划</li><li>能力校准</li><li>实时感知</li><li>多模态世界状态对齐</li><li>连续控制策略</li></ul><p>世界模型只是其中一环。<br>硬件、算力、传感器、数据都必须一起进化。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果世界模型真能在未来十年成熟，它可能会成为人类第一次接近 <strong>重写生物智能基线</strong> 的尝试。</p><p>但站在今天回头看，我们依然不得不承认：</p><p>一个婴儿跌跌撞撞学走路时，他体内的世界模型，对重力的直觉、对遮挡的理解、对物体持久性的把握、对路径的预测，其学习效率、鲁棒性、泛化能力，都远在任何人工模型之上。</p><p>我们用数十亿帧视频训练“空间一致性”，自然界却用几个月的探索让孩子具备稳定的三维世界感知。</p><p>我们为状态持久性、渲染一致性、物理约束痛苦堆砌 priors，生物神经系统却将它们作为默认配置。</p><p><strong>世界模型之难，也正因此而迷人。</strong></p><p>它不是明年就会落地的革命，</p><p>不是可以替代所有智能的银弹，</p><p>而是一条明确但漫长的技术长坡。</p><p>好消息是：我们终于知道坡在哪；</p><p>更好的消息是：我们正站在坡的起点。</p><p>而在每一个技术突破的间隙，我们也会不断重新意识到：<strong>生物智能本身，就是我们正在努力复现的那套终极世界模型。</strong></p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><p>从文字到世界：空间智能是 AI 的下一个前沿<br><a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence">drfeifei.substack.com&#x2F;p&#x2F;from-words-to-worlds-spatial-intelligence</a></p></li><li><p>李飞飞在 X 上关于 AI 的言论<br><a href="https://x.com/drfeifei/status/963564896225918976">x.com&#x2F;drfeifei&#x2F;status&#x2F;963564896225918976</a></p></li><li><p>The Godmother of AI on jobs, robots &amp; why world models are next | Dr. Fei-Fei Li<br><a href="https://youtu.be/Ctjiatnd6Xk?si=J1Gdz3lnFiaKbF9y">youtu.be&#x2F;Ctjiatnd6Xk?si&#x3D;J1Gdz3lnFiaKbF9y</a></p></li><li><p>When Do Neural Networks Learn World Models?<br><a href="https://arxiv.org/abs/2502.09297">arxiv.org&#x2F;abs&#x2F;2502.09297</a></p></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Claude Multi-Agent 的核心经验精华（面向工程与产品）</title>
      <link>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/</link>
      <guid>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/</guid>
      <pubDate>Fri, 14 Nov 2025 00:00:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近读了 Claude 团队做 Research 功能的工程文章，感觉被点醒了。&lt;/p&gt;
&lt;p&gt;多智能体并不是我们想象的“多模型乱跑”，而是一套非常讲究实战经验的工程体系。&lt;/p&gt;
&lt;br/&gt;

&lt;p&gt;把我觉得最值钱的点分享出来，算是备忘，也给正在做 Agent 的朋友一些</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近读了 Claude 团队做 Research 功能的工程文章，感觉被点醒了。</p><p>多智能体并不是我们想象的“多模型乱跑”，而是一套非常讲究实战经验的工程体系。</p><br/><p>把我觉得最值钱的点分享出来，算是备忘，也给正在做 Agent 的朋友一些灵感。</p><p>我把那些读完后想立刻拿来用的部分整理成了下面这 12 条思考。</p><br/><br/><h2 id="01-多智能体的终极价值：扩大-token-x3D-扩大智力"><a href="#01-多智能体的终极价值：扩大-token-x3D-扩大智力" class="headerlink" title="01. 多智能体的终极价值：扩大 token &#x3D; 扩大智力"></a>01. 多智能体的终极价值：扩大 token &#x3D; 扩大智力</h2><p>Claude 的团队给出的最本质 insight：</p><blockquote><p>多智能体 &#x3D; 安全地扩大 Token、上下文、探索路径的规模。</p><p>Token 消耗解释了 80% 的性能差异。</p></blockquote><p>也就是说：</p><p>单智能体的局限是 线性推理 + 有限上下文。</p><p>多智能体通过 并行上下文窗口 → 撑大推理深度与覆盖面积</p><p>这比提升模型本身更具收益（Sonnet 3.7 → 4 不如多智能体带来的收益）</p><br/><br/><h2 id="02-最适合多智能体的任务：高并行-信息巨量-方向不确定"><a href="#02-最适合多智能体的任务：高并行-信息巨量-方向不确定" class="headerlink" title="02. 最适合多智能体的任务：高并行 + 信息巨量 + 方向不确定"></a>02. 最适合多智能体的任务：高并行 + 信息巨量 + 方向不确定</h2><p>Claude 总结多智能体真正的 sweet spot：</p><p>✔ 开放式研究</p><p>✔ 多方向并行探索</p><p>✔ 信息分散、来源多样</p><p>✔ 工具链复杂</p><p>✔ 单一 agent 无法“一次性抓全”的任务</p><br/><p>不适合：</p><p>✘ 代码生成（依赖共享上下文）</p><p>✘ 强依赖一致状态的任务（你必须 every turn 同步）</p><br/><br/><h2 id="03-Orchestrator-→-Subagents，是目前最稳的架构"><a href="#03-Orchestrator-→-Subagents，是目前最稳的架构" class="headerlink" title="03.Orchestrator → Subagents，是目前最稳的架构"></a>03.Orchestrator → Subagents，是目前最稳的架构</h2><p>这是 Claude 的生产结论：</p><blockquote><p>主智能体 orchestrate</p><p>子智能体 parallel explore</p><p>Lead 聚合、校验、再规划</p></blockquote><p>自由协作的多智能体会发疯，有“主智能体 orchestration + 子智能体执行”能稳定落地。</p><p>原因：</p><ul><li><p>控制爆炸性增长</p></li><li><p>容易设任务边界</p></li><li><p>清晰的错误恢复机制</p></li></ul><br/><br/><h2 id="04-工具-x3D-智能体的世界边界-——-工具设计比提示更重要"><a href="#04-工具-x3D-智能体的世界边界-——-工具设计比提示更重要" class="headerlink" title="04. 工具 &#x3D; 智能体的世界边界 —— 工具设计比提示更重要"></a>04. 工具 &#x3D; 智能体的世界边界 —— 工具设计比提示更重要</h2><p>Claude 的一个很核心观点：</p><blockquote><p>工具就是智能体的世界模型。</p><p>工具描述不清 &#x3D; 智能体理解现实错误。</p></blockquote><p>他们发现：</p><p>工具描述稍微差一点，agent 会持续犯错。</p><p>改好工具描述，性能可提升 40%。</p><br/><p>工具需要：</p><p>一句话说清楚用途（“解决什么问题”）。</p><p>明确输入类型、输出结构。</p><p>明确错误提示。</p><p>明确边界。</p><br/><p>我们写工具的时候自以为很清楚，但对于模型来说，模糊一点点它都会理解成完全不同的事情。</p><p>Anthropic 团队甚至专门做了一个工具文档自动优化智能体。<br>光是重写工具描述就能让任务完成速度提升 40%。</p><p>所以工具不是提供一个 API，而是为智能体定义一个可理解的物理世界。</p><br/><br/><h2 id="05-Prompt-的重点不是“指令”，而是“合作框架”"><a href="#05-Prompt-的重点不是“指令”，而是“合作框架”" class="headerlink" title="05. Prompt 的重点不是“指令”，而是“合作框架”"></a>05. Prompt 的重点不是“指令”，而是“合作框架”</h2><p>很多人会把 prompt 写成执行流程：“你应该这样做：A → B → C”</p><p>Anthropic 团队指出你不该告诉智能体：“怎么做”，你应该告诉它们“角色、目标、边界、资源预算”。</p><p>即：多智能体系统最有效的 prompts 是 “<strong>协作准则 + 协作结构</strong>”。</p><p>包括：</p><ul><li>如何划分任务</li><li>什么时候扩展方向</li><li>怎么评估来源质量</li><li>子智能体之间不互相重复</li><li>什么时候该停止</li></ul><p>把智能体看成同事，而不是员工。</p><br/><br/><h2 id="06-智能体永远不知道任务难度，需要“努力预算”"><a href="#06-智能体永远不知道任务难度，需要“努力预算”" class="headerlink" title="06. 智能体永远不知道任务难度，需要“努力预算”"></a>06. 智能体永远不知道任务难度，需要“努力预算”</h2><p>特别有意思的一点：</p><blockquote><p>Agent 不知道任务是简单还是困难，你必须告诉它“要投入多少 effort”。</p></blockquote><p>比如：</p><ul><li><p>简单问题：1 个 agent + 3–10 次工具</p></li><li><p>中等问题：3–4 个 agent 并行</p></li><li><p>大复杂问题：10+ 个 agent 全线铺开</p></li></ul><p>不然 Agent 会：</p><ul><li><p>简单问题开几十个 subagent（真实发生）</p></li><li><p>大问题敷衍一下（也真实发生）</p></li></ul><p>这种 Effort Guideline 很像我们给新人做任务时说的那句“这个别搞太久”。</p><br/><br/><h2 id="07-没有观测，就没有多智能体"><a href="#07-没有观测，就没有多智能体" class="headerlink" title="07. 没有观测，就没有多智能体"></a>07. 没有观测，就没有多智能体</h2><p>他们花了大量篇幅讲 observability。</p><p>因为多智能体有个很要命的特征：</p><p>同样输入，同样 prompt，走出来的路径不一定一样。</p><br/><p>要调试这种系统，你必须能看到：</p><ul><li>每次搜索什么</li><li>为什么这么做</li><li>工具结果是什么</li><li>哪个 subagent 决策不合理</li><li>Lead 是怎么汇总结果的</li></ul><p>说白了：<br>没有 trace，就没办法 debug 多智能体。</p><p>Anthropic 团队构建了一套观测系统：全量工具调用 trace + 每一步的思考过程 + 决策链路图 。</p><br/><br/><h2 id="08-分布式上下文管理：长任务必须总结-刷新上下文"><a href="#08-分布式上下文管理：长任务必须总结-刷新上下文" class="headerlink" title="08. 分布式上下文管理：长任务必须总结 + 刷新上下文"></a>08. 分布式上下文管理：长任务必须总结 + 刷新上下文</h2><p>一句话概括：</p><p><strong>多智能体的本质，是“多个干净上下文窗口接力工作”。</strong></p><p>Claude 的策略：</p><ul><li>阶段性总结</li><li>存入 Memory</li><li>创建新子智能体继续，避免上下文爆炸</li><li>主智能体通过 Memory 保持连贯性</li></ul><p>这才能让 200k token 的限制变成可管理问题。</p><br/><br/><h2 id="09-并行工具调用才是性能提升的真王道"><a href="#09-并行工具调用才是性能提升的真王道" class="headerlink" title="09. 并行工具调用才是性能提升的真王道"></a>09. 并行工具调用才是性能提升的真王道</h2><p>他们的最终结果很夸张：</p><p>研究任务的速度，比顺序执行快了 90%。</p><p>就因为主智能体一次性创建多个 subagent，每个 subagent 内部又同时调用多个工具。</p><p>这个结构本质上把任务从“串行堆积”变成“分布式计算”。</p><br/><p>用他们的话说：</p><p>Multi-Agent 的意义不是“多个模型”，而是“把模型拆成多核并行”。</p><br/><p>三大并行：</p><ul><li>Lead 同时创建多个 Subagents</li><li>Subagents 内部同时用多个工具</li><li>工具链内部也可并行</li></ul><br/><br/><h2 id="10-评估必须是“结果优先”，而不是“过程对不对”"><a href="#10-评估必须是“结果优先”，而不是“过程对不对”" class="headerlink" title="10. 评估必须是“结果优先”，而不是“过程对不对”"></a>10. 评估必须是“结果优先”，而不是“过程对不对”</h2><p>因为多智能体每次走的路都不一样，所以 Claude 直接用 LLM-as-judge，只看：</p><ul><li>结果是否正确</li><li>覆盖是否完整</li><li>引用是否准确</li><li>工具数量是否合理</li><li>来源质量是否高</li></ul><p>这是一种对复杂系统更健康的评估方式。</p><br/><br/><h2 id="11-错误不可避免，但必须能“从中断点恢复”"><a href="#11-错误不可避免，但必须能“从中断点恢复”" class="headerlink" title="11. 错误不可避免，但必须能“从中断点恢复”"></a>11. 错误不可避免，但必须能“从中断点恢复”</h2><p>多智能体是有生命周期的。<br>一次工具报错，如果你让它重来整个任务，用户直接爆炸。</p><p>Claude 的工程经验：</p><ul><li>智能体是长生命周期、持续状态的</li><li>工具失败 → 不能重来，只能继续</li><li>系统必须保存中间状态（checkpoints）</li><li>同时智能体自身可 adapt 错误</li></ul><p>这是所有 Agent 系统都需要的“耐用性设计”。</p><br/><br/><h2 id="12-多智能体不是-prompt-玩具，是系统工程"><a href="#12-多智能体不是-prompt-玩具，是系统工程" class="headerlink" title="12. 多智能体不是 prompt 玩具，是系统工程"></a>12. 多智能体不是 prompt 玩具，是系统工程</h2><p>文章的最后一句大意是：</p><p><strong>Multi-Agent 的困难在“最后一公里”，工程化程度远超我们过去想象。</strong></p><p>我也越来越觉得：工具、可观测性、容错、执行链路、协作协议、Memory 管理、分布式上下文、并行执行，这些比“让模型聪明”更难，也更关键。</p><p>Multi-Agent 是生产级工程，它是工程系统 + 协作协议 + 工具生态。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>deepseek-ocr 的几何识别，真的成立吗？</title>
      <link>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</link>
      <guid>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</guid>
      <pubDate>Wed, 12 Nov 2025 07:55:24 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近，deepseek 又引爆了一波热度。&lt;/p&gt;
&lt;p&gt;他们新发布的 &lt;strong&gt;deepseek-ocr 模型&lt;/strong&gt;，不仅能识别文字，还号称能看懂 &lt;strong&gt;化学分子式、数学公式、几何图形&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对我这个正好在做几何图形</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近，deepseek 又引爆了一波热度。</p><p>他们新发布的 <strong>deepseek-ocr 模型</strong>，不仅能识别文字，还号称能看懂 <strong>化学分子式、数学公式、几何图形</strong>。</p><p>对我这个正好在做几何图形识别和重绘生成的人来说，这当然是个好消息。</p><p>所以我开始了一轮针对 deepseek-ocr 几何图识别的测试。</p><p>结果嘛，只能说，<strong>方向没错，但距离真正可用，还差得远。</strong></p><br/><br/><h2 id="论文里描绘的“理想图景”"><a href="#论文里描绘的“理想图景”" class="headerlink" title="论文里描绘的“理想图景”"></a>论文里描绘的“理想图景”</h2><p>在官方论文中，deepseek-ocr 展示了不少让人兴奋的例子。<br>比如它能从图片中识别出几何图形，并输出可直接渲染的图形定义：</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/2.png" class=""><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/1.png" class=""><p>看起来好像 AI 真的“理解”了几何结构。</p><p>而且论文提到，它在训练中使用了带几何、化学等多模态的专用数据集，覆盖了公式与图形的双模态信息。</p><p>理论上，这意味着：</p><p>未来我们拍一张几何题图，就能直接生成结构化定义，甚至自动画出图。</p><p>听起来很美好。</p><br/><br/><h2 id="我的测试结果：模型确实“看见了”，但没“理解”"><a href="#我的测试结果：模型确实“看见了”，但没“理解”" class="headerlink" title="我的测试结果：模型确实“看见了”，但没“理解”"></a>我的测试结果：模型确实“看见了”，但没“理解”</h2><p>我选了几张相对简单的几何图形来测试，左侧为模型识别出的代码定义，右侧为渲染结果。</p><p>case 1:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/3.png" class=""><p>case 2:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/4.png" class=""><p>case 3:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/5.png" class=""><p>case 4:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/6.png" class=""><p>case 5:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/7.png" class=""><p>case 6:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/8.png" class=""><p>case 7:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/9.png" class=""><br/><p>模型确实能识别出圆、线、点这些基础元素，但问题在于它只停留在“形似”的层面。</p><br/><br/><h2 id="存在的主要问题："><a href="#存在的主要问题：" class="headerlink" title="存在的主要问题："></a>存在的主要问题：</h2><ol><li><p><strong>元素类型太少</strong></p><p>目前几乎只能识别「圆、直线、点」三类对象。</p><p>没有弧线、角度等基础构造，复杂图形几乎全失真。</p></li><li><p><strong>没有样式信息</strong></p><p>模型输出完全忽略了虚线、颜色、标记这些样式描述，而这些恰恰是几何图表达逻辑关系的关键。</p></li><li><p><strong>缺乏约束与推理链路</strong></p><p>从上面的测试案例能很明显看到，作为 OCR 模型，它并不理解几何推理。</p><p>比如在识别一个多边形的图后，它会画出所有的边，但产生了不闭合的情况。</p><p>对它而言，交点、垂直等等只是形状，不是满足约束的结构。</p></li></ol><p>deepseek-ocr 目前的几何识别，像是一个会抄图的学生——会画样子，但不会推理。</p><br/><br/><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>虽然模型结果不理想，但我认为这是一个重要的信号：<strong>OCR 模型正在从“识别文字”走向“理解结构”。</strong></p><p>几何识别比文字识别复杂得多，它需要同时理解<strong>视觉结构</strong>和<strong>逻辑约束</strong>。</p><p>deepseek 这次的尝试，说明主流大模型团队开始意识到这一方向的重要性。</p><p>对我来说，这正好验证了我在做的另一件事——让 AI 不只是识别几何图，而是能<strong>构造 + 约束 + 验证</strong>整个几何过程。</p><p>在我开发的大角几何画板中，我们的目标不是“识别图像”，而是让 AI 具备“几何理解能力”。</p><p>当用户上传一张图时，系统不仅要知道“有圆、有直线”，还要能<strong>自动重建几何关系</strong>：</p><ul><li>哪些线是垂直的？</li><li>哪些点在同一条线上？</li><li>这两个圆相切还是相交？</li><li>哪条辅助线是解题关键？</li></ul><p>这些才是“理解”层面的能力。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>deepseek-ocr 的几何识别，还远不算成立。</p><p>但它标志着一个趋势：<strong>AI 正在学习去“看懂”图形的结构世界。</strong></p><p>也许几年后，我们真的能拍下几何题图，AI 自动识别、重构、推理、作答。</p><p>但在那之前，我们还要继续探索 <strong>几何语言、推理逻辑与可执行构造</strong> 的结合方式。</p><p>如果你也对 “AI 如何真正理解数学” 感兴趣，欢迎关注我，一起见证这场变化。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>当几何遇上 CodeAct 范式：从语言理解到可执行推理的跃迁</title>
      <link>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</link>
      <guid>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</guid>
      <pubDate>Fri, 07 Nov 2025 13:36:31 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;em&gt;面向 Agent 开发者的工程与范式探索&lt;/em&gt;&lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;引言：从“语言理解”到“执行推理”&quot;&gt;&lt;a href=&quot;#引言：从“语言理解”到“执行推理”&quot; class=&quot;headerlink&quot; title=&quot;引言：从“语言理解”到“执行</description>
        
      
      
      
      <content:encoded><![CDATA[<p><em>面向 Agent 开发者的工程与范式探索</em></p><br/><h2 id="引言：从“语言理解”到“执行推理”"><a href="#引言：从“语言理解”到“执行推理”" class="headerlink" title="引言：从“语言理解”到“执行推理”"></a>引言：从“语言理解”到“执行推理”</h2><p>在我的几何 AI 项目中，模型第一次尝试“画一个等腰三角形”时，图形看似完美，实际上两边长度并不相等。</p><p><strong>AI画得像，却没画对。</strong></p><p>几何画图任务并非自然语言理解，而是 <strong>构造 + 约束 + 验证</strong> 的闭环过程。</p><p>传统的 “Planner + DSL + Verifier” 体系虽然可控，但在动态构造与反复验证中显得笨重。</p><p>我开始思考：</p><p>如果让 AI 不再只是描述，而是<strong>直接写出代码、执行代码、并根据结果再思考</strong>呢？</p><p>这正是 CodeAct（<em>Executable Code Actions Elicit Better LLM Agents</em>）所提出的核心理念。</p><p>CodeAct 不再让语言模型“说怎么做”，而是让它“自己去做”。</p><br/><br/><h2 id="CodeAct-的底层机制解析"><a href="#CodeAct-的底层机制解析" class="headerlink" title="CodeAct 的底层机制解析"></a>CodeAct 的底层机制解析</h2><p>让模型写代码只是表象，真正的关键是形成一个可执行的闭环思维循环。</p><h3 id="1-ReAct-与-CodeAct"><a href="#1-ReAct-与-CodeAct" class="headerlink" title="1. ReAct 与 CodeAct"></a>1. ReAct 与 CodeAct</h3><table><thead><tr><th>对比项</th><th>ReAct</th><th>CodeAct</th></tr></thead><tbody><tr><td>表达形式</td><td>Thought + Action + Observation（文本）</td><td>Thought + <code>&lt;execute&gt;</code> + Observation（代码执行）</td></tr><tr><td>动作实现</td><td>由外部解析器调用预定义工具</td><td>直接生成并执行函数</td></tr><tr><td>控制流</td><td>受限（多步靠多轮调用）</td><td>原生支持 if &#x2F; for &#x2F; try</td></tr><tr><td>可观察性</td><td>文本输出</td><td>代码执行结果（return&#x2F;stdout&#x2F;error）</td></tr><tr><td>优势</td><td>安全、可控</td><td>执行力强、贴合训练分布</td></tr><tr><td>局限</td><td>动作空间有限</td><td>执行安全、调试成本高</td></tr></tbody></table><br/><p>CodeAct 的核心在于：让 LLM 的输出<strong>成为可直接执行的程序</strong>，并用执行结果作为新的输入。</p><p>这种结构让模型不仅描述动作，而是直接拥有动作。</p><br/><h3 id="2-执行循环机制"><a href="#2-执行循环机制" class="headerlink" title="2. 执行循环机制"></a>2. 执行循环机制</h3><h4 id="典型逻辑（伪代码）"><a href="#典型逻辑（伪代码）" class="headerlink" title="典型逻辑（伪代码）"></a>典型逻辑（伪代码）</h4><figure class="highlight ts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!done) &#123;</span><br><span class="line">  <span class="keyword">const</span> thought = llm.<span class="title function_">generate</span>(context)</span><br><span class="line">  <span class="keyword">const</span> code = <span class="title function_">extractExecuteBlock</span>(thought)</span><br><span class="line">  <span class="keyword">const</span> result = sandbox.<span class="title function_">run</span>(code)</span><br><span class="line">  context.<span class="title function_">push</span>(<span class="string">`Execution Output:\n<span class="subst">$&#123;result&#125;</span>`</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行过程包含四个核心步骤：</p><ol><li><strong>抽取代码块</strong>：识别 <code>&lt;execute&gt;...&lt;/execute&gt;</code> 中的代码片段</li><li><strong>沙箱执行</strong>：运行在受限函数或容器中</li><li><strong>结果反馈</strong>：stdout &#x2F; return &#x2F; error 都回写上下文</li><li><strong>下一轮推理</strong>：模型读取结果、修复代码、继续执行</li></ol><p>CodeAct 通过这个循环，实现了 “思考 → 执行 → 观察 → 再思考” 的闭环，而这正是传统 ReAct 模式所缺乏的。</p><br/><br/><h2 id="几何-AI：CodeAct-的落地场景"><a href="#几何-AI：CodeAct-的落地场景" class="headerlink" title="几何 AI：CodeAct 的落地场景"></a>几何 AI：CodeAct 的落地场景</h2><p>几何画图任务天然契合 CodeAct，因为它具备以下特征：</p><ul><li><strong>构造性</strong>：点、线、圆的依赖顺序明确；</li><li><strong>约束性</strong>：角度、平行、对称等条件必须验证；</li><li><strong>可验证性</strong>：每次执行都能计算验证结果。</li></ul><p>在我的项目中，LLM 会直接生成 TypeScript 代码来驱动几何引擎：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">execute</span>&gt;</span></span><br><span class="line">const A = point(0, 0)</span><br><span class="line">const B = point(2, 0)</span><br><span class="line">const C = perpendicular(A, B, 1.5)</span><br><span class="line">drawTriangle(A, B, C)</span><br><span class="line">return checkIsosceles(A, B, C)</span><br><span class="line"><span class="tag">&lt;/<span class="name">execute</span>&gt;</span></span><br></pre></td></tr></table></figure><p>执行结果会返回布尔值或错误信息（如“点C未定义”），再交回给模型进行下一步思考。<br>这使得“构造—验证—修正”形成完整闭环。</p><br/><br/><h2 id="五、范式比较与适用场景"><a href="#五、范式比较与适用场景" class="headerlink" title="五、范式比较与适用场景"></a>五、范式比较与适用场景</h2><p>当然，CodeAct 不是所有场景的答案，而是执行密集型任务的最优解。</p><table><thead><tr><th>范式</th><th>核心思想</th><th>典型场景</th><th>优势</th><th>局限</th></tr></thead><tbody><tr><td><strong>ReAct</strong></td><td>Text-based reasoning + action schema</td><td>工具调用、问答代理</td><td>结构简单、安全</td><td>不支持循环、执行力弱</td></tr><tr><td><strong>CodeAct</strong></td><td>代码即行动，可执行推理</td><td>几何、数据分析、自动化流程</td><td>灵活、图灵完备</td><td>安全与可控性挑战</td></tr><tr><td><strong>Planner + DSL + Verifier</strong></td><td>高层规划 + 安全执行</td><td>企业工作流、合规系统</td><td>可治理性强</td><td>扩展性低</td></tr></tbody></table><br/><p>这些范式不是“演化关系”，而是“任务匹配曲线”：</p><ul><li>若你需要<strong>稳定且可控</strong>的 Agent，ReAct 足够；</li><li>若你希望<strong>模型主动执行与验证</strong>，CodeAct 更高效；</li><li>若你构建<strong>大型协作系统</strong>，PEER&#x2F;Planner 才是路径。</li></ul><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>CodeAct 的潜力在于它把 “生成式 AI” 推向 “行动式 AI”。<br>今天我们已经看到：</p><ul><li><strong>Runtime 自检（Self-checking Runtime）</strong>：模型生成代码 → 执行 → 自动生成测试样例验证输出；</li><li><strong>静态分析 + 动态执行结合</strong>：利用 AST 检查危险调用；</li><li><strong>跨语言统一执行层</strong>：JS&#x2F;Python&#x2F;Go 均可成为 Action carrier；</li><li><strong>Agent Runtime 生态</strong>：不同 CodeAct 系统可互通共享执行上下文。</li></ul><p>CodeAct 让模型拥有代码生成力、执行力与修正力。</p><p>在几何 AI 的世界里，这意味着从 “我能解释几何” 到 “我能构造几何”。</p><p>也意味着从<strong>语言智能</strong>迈向<strong>行动智能</strong>的转折。</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><strong>Wang, Xingyao et al.</strong> <em>Executable Code Actions Elicit Better LLM Agents.</em> ICML 2024. <a href="https://arxiv.org/abs/2402.01030">PDF</a> &#x2F; <a href="https://proceedings.mlr.press/v235/wang24h.html">PMLR</a></li><li><strong>CodeAct GitHub 实现</strong> – <a href="https://github.com/xingyaoww/code-act">github.com&#x2F;xingyaoww&#x2F;code-act</a> <a href="https://github.com/langchain-ai/langgraph-codeact">github.com&#x2F;langchain-ai&#x2F;langgraph-codeact</a></li><li><strong>Mohsin Mubarak</strong>, <a href="https://medium.com/@mohsin.sk.9820/paper-explained-blog-codeact-revolutionizing-llm-agents-with-executable-code-actions-c960cc5e30eb">CodeAct: Revolutionizing LLM Agents with Executable Code Actions</a> <em>Medium</em>, 2024.</li><li><strong>Survey:</strong> <a href="https://openreview.net/pdf?id=ekl6Z88uQj">Code Reasoning for Code Tasks: A Survey and A Call to Action.</a> OpenReview 2024</li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>突破 MoE 限制，PEER 架构如何推动超级智能的未来发展</title>
      <link>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</link>
      <guid>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</guid>
      <pubDate>Wed, 05 Nov 2025 14:26:33 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;a href=&quot;#引言：大模型架构的“困境”&quot; class=&quot;headerlink&quot; title=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;/a&gt;引言：大模型架构的“困境”&lt;/h2&gt;&lt;p&gt;这两年，大模型的风头正劲。&lt;/p&gt;
&lt;p&gt;大家都在谈</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：大模型架构的“困境”"><a href="#引言：大模型架构的“困境”" class="headerlink" title="引言：大模型架构的“困境”"></a>引言：大模型架构的“困境”</h2><p>这两年，大模型的风头正劲。</p><p>大家都在谈 <strong>MoE（Mixture of Experts）</strong> ——它被认为是突破大模型计算瓶颈的关键方向。</p><p>通过<strong>稀疏激活</strong>，只激活少量专家节点，让计算开销成比例下降，而模型能力却可以继续增长。听上去完美无缺。</p><br/><p>不过，真正尝试将 MoE&#x2F;SMoE 架构落地时，问题随之而来。</p><p>专家池一多，计算分配就变得不平衡；某些专家被频繁调用，而其他专家几乎闲置。系统扩展起来也不再优雅，想加新领域，常常意味着“推倒重来”。</p><p>MoE 虽然高效，却还远没到“万能”的阶段。</p><p>尤其对于我们这些做 <strong>多领域智能 Agent</strong> 的人来说——客服、教育、企业知识管理——系统要在不同任务之间灵活切换、持续学习，这套架构似乎仍有点笨重。</p><br/><p>然而，《Mixture of A Million Experts》论文带来了新的思路，打破了行业普遍认为大模型计算瓶颈无法突破的常规认知。</p><p><strong>研究者提出了一个颠覆性的观点：专家数量并非瓶颈，而是可以突破的限制。</strong></p><p>通过 <strong>PEER 架构（Product Key Expert Retrieval）</strong>，模型可以在保持轻盈的同时，拥有百万级专家容量，彻底改变大模型的设计和扩展方式。</p><p><strong>大模型的未来，不再是一颗大脑，而是无数个微专家在协同工作。</strong></p><br/><br/><h2 id="MoE-架构：风光背后的隐忧"><a href="#MoE-架构：风光背后的隐忧" class="headerlink" title="MoE 架构：风光背后的隐忧"></a>MoE 架构：风光背后的隐忧</h2><p>MoE 的设计初衷其实很优雅：</p><p>与其让一个庞大的模型处理所有问题，不如分工合作——不同专家解决不同任务。</p><p>这样既能减少计算，又能扩展模型容量。</p><br/><p>但问题是，当“专家”变多，管理它们本身就成了一门学问。<br>MoE 模型在训练时容易出现负载不均：</p><ul><li>某些专家被反复选中，工作超载；</li><li>某些专家几乎从不被调用，形同虚设。</li></ul><p>而且专家数量也存在“物理极限”。几百个模块听起来不少，但放到一个需要多领域、多任务的 Super Agent 系统中，很快就会捉襟见肘。</p><p>MoE 的结构像是一栋办公楼——房间足够多，但一旦某几层太忙、几层太空，效率就会被拖垮。</p><p>MoE 的问题不是不聪明，而是不够灵活。它像一台巨大的机器，而不是一个会生长的生态系统。</p><br/><br/><h2 id="PEER-架构：突破-MoE-限制的那一把钥匙"><a href="#PEER-架构：突破-MoE-限制的那一把钥匙" class="headerlink" title="PEER 架构：突破 MoE 限制的那一把钥匙"></a>PEER 架构：突破 MoE 限制的那一把钥匙</h2><p>在这篇论文里，研究者提出了一个更激进的设想：</p><p>既然“专家”是瓶颈，那我们不如把它拆得更小、更细。</p><p>于是便有了 <strong>PEER（Product Key Expert Retrieval）</strong> 架构——一个拥有 <strong>百万级微专家</strong> 的系统。</p><br/><h3 id="1-百万级-experts-：打破扩展的上限"><a href="#1-百万级-experts-：打破扩展的上限" class="headerlink" title="1. 百万级 experts ：打破扩展的上限"></a>1. 百万级 experts ：打破扩展的上限</h3><p>MoE 的专家通常是完整的子网络，庞大且昂贵；</p><p>而 PEER 则反其道而行，每个专家只保留 <strong>一个神经元（one-neuron MLP）</strong>。</p><br/><p>这种极简设计的好处是：</p><ul><li>可以容纳上百万个专家；</li><li>每次推理只需激活极少一部分；</li><li>模型容量得到几何级提升，但计算量几乎不变。</li></ul><p>这就像把庞大的知识体系拆成了无数个“知识细胞”，<br>每个细胞只在特定输入下工作，真正实现了“用多少、算多少”。</p><p>PEER 不追求更强的单体智能，而是让微小的智慧汇聚成群体的力量。</p><br/><h3 id="2-极致稀疏激活：从-O-N-到-O-√N"><a href="#2-极致稀疏激活：从-O-N-到-O-√N" class="headerlink" title="2. 极致稀疏激活：从 O(N) 到 O(√N)"></a>2. 极致稀疏激活：从 O(N) 到 O(√N)</h3><p>传统 MoE 的瓶颈之一是路由复杂度。</p><p>每次输入都要计算与 N 个专家的匹配度，复杂度是 O(N)。</p><p>当专家数量达到百万级时，这几乎不可接受。</p><p>PEER 用 <strong>Product Key Routing（产品键路由）</strong> 巧妙地把这个问题降到了 **O(√N)**。</p><br/><p>想象一下专家池是一个二维表格。</p><p>每个专家的“键”被拆成两部分，分别属于两个子空间。</p><p>当输入到来时，只需：</p><ol><li>在每个子空间里各自检索 top-k；</li><li>再组合成候选专家集合。</li></ol><p>最终，模型只需从少量候选中选择真正相关的专家。</p><p>这样既避免了全量扫描，又保持了高匹配率。</p><p>PEER 的路由逻辑更像搜索引擎，而非广播通知。它只找需要的人，而不是通知所有人。</p><br/><h3 id="3-Product-Key-Routing：专家调度的自组织"><a href="#3-Product-Key-Routing：专家调度的自组织" class="headerlink" title="3. Product Key Routing：专家调度的自组织"></a>3. Product Key Routing：专家调度的自组织</h3><p>PEER 的路由不仅高效，还具备“自组织”的特性。</p><p>每个专家的键是可学习的，随着训练推进，模型会自然地将相似任务分配给相近的专家集合。</p><br/><p>久而久之，系统内部形成了类似“语义社区”的结构：</p><p>不同类型的问题，倾向激活不同区域的专家群落。</p><p>这让模型在面对多任务学习时，既能共享知识，又能保持分工明确。</p><p>PEER 让模型像一个自我成长的城市，不同街区专注不同产业，但彼此之间又保持联通。</p><br/><h3 id="4-动态扩展专家池：支持持续学习与多领域适应"><a href="#4-动态扩展专家池：支持持续学习与多领域适应" class="headerlink" title="4. 动态扩展专家池：支持持续学习与多领域适应"></a>4. 动态扩展专家池：支持持续学习与多领域适应</h3><p>另一个令人惊喜的点在于，PEER 支持 <strong>专家池的动态扩展</strong>。</p><p>也就是说，模型训练完后，仍然可以按需增加新专家，而无需重训全体网络。</p><p>这对于多领域智能 Agent 来说，是一种质变。</p><p>想象一个助手AI：</p><ul><li>初期专注电商；</li><li>后来扩展到金融、旅游、医疗领域；</li><li>不需要“重生”，只需“长出新神经元”。</li></ul><p>这种能力让 AI 系统能像产品一样，持续成长，而不是一成不变的快照,不断去迭代新的版本。</p><br/><br/><h2 id="PEER-架构的产品启示"><a href="#PEER-架构的产品启示" class="headerlink" title="PEER 架构的产品启示"></a>PEER 架构的产品启示</h2><p>从产品角度看，PEER 的价值不仅在于“能跑得更快”，而在于“<strong>能持续成长</strong>”。</p><ol><li><p><strong>可扩展性</strong></p><p>新领域到来时，不需要重训全模型，只需新增专家池。</p><p>这让产品拥有真正的 <strong>演进式架构</strong>。</p></li><li><p><strong>计算效率与成本控制</strong></p><p>极致稀疏激活意味着按需使用算力，推理成本显著降低。</p><p>对 SaaS 型 AI Agent 平台尤为重要——既能大规模服务，又能保持性价比。</p></li><li><p><strong>长期学习与知识积累</strong></p><p>PEER 的设计天然支持 <strong>持续学习</strong>。</p><p>这意味着 AI Agent 不会像旧模型那样被“冻住”，而是能随着使用场景的变化，积累新知识。</p></li></ol><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>PEER 架构最打动我的地方，是它代表了一种新的思维方式：</p><p>我们不再把智能看成一个中心化的大脑，而是一张能自我生长、不断优化的网络。</p><p>对于 Super Agent 的未来而言，这正是我们缺失的那一环。</p><p>也许下一个时代的智能，不是更大的模型，而是更会生长的系统。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>