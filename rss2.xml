<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Luhui&#39;s Personal Website</title>
    <link>https://blog.liluhui.cn/</link>
    
    <image>
      <url>https://blog.liluhui.cn/asset/img/logo-green.ico</url>
      <title>Luhui&#39;s Personal Website</title>
      <link>https://blog.liluhui.cn/</link>
    </image>
    
    <atom:link href="https://blog.liluhui.cn/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>关于生活、学习、工作 feedId:66855595489698816+userId:55886336755964928</description>
    <pubDate>Wed, 12 Nov 2025 08:16:39 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>deepseek-ocr 的几何识别，真的成立吗？</title>
      <link>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</link>
      <guid>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</guid>
      <pubDate>Wed, 12 Nov 2025 07:55:24 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近，deepseek 又引爆了一波热度。&lt;/p&gt;
&lt;p&gt;他们新发布的 &lt;strong&gt;deepseek-ocr 模型&lt;/strong&gt;，不仅能识别文字，还号称能看懂 &lt;strong&gt;化学分子式、数学公式、几何图形&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对我这个正好在做几何图形</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近，deepseek 又引爆了一波热度。</p><p>他们新发布的 <strong>deepseek-ocr 模型</strong>，不仅能识别文字，还号称能看懂 <strong>化学分子式、数学公式、几何图形</strong>。</p><p>对我这个正好在做几何图形识别和重绘生成的人来说，这当然是个好消息。</p><p>所以我开始了一轮针对 deepseek-ocr 几何图识别的测试。</p><p>结果嘛，只能说，<strong>方向没错，但距离真正可用，还差得远。</strong></p><br/><br/><h2 id="论文里描绘的“理想图景”"><a href="#论文里描绘的“理想图景”" class="headerlink" title="论文里描绘的“理想图景”"></a>论文里描绘的“理想图景”</h2><p>在官方论文中，deepseek-ocr 展示了不少让人兴奋的例子。<br>比如它能从图片中识别出几何图形，并输出可直接渲染的图形定义：</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/2.png" class=""><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/1.png" class=""><p>看起来好像 AI 真的“理解”了几何结构。</p><p>而且论文提到，它在训练中使用了带几何、化学等多模态的专用数据集，覆盖了公式与图形的双模态信息。</p><p>理论上，这意味着：</p><p>未来我们拍一张几何题图，就能直接生成结构化定义，甚至自动画出图。</p><p>听起来很美好。</p><br/><br/><h2 id="我的测试结果：模型确实“看见了”，但没“理解”"><a href="#我的测试结果：模型确实“看见了”，但没“理解”" class="headerlink" title="我的测试结果：模型确实“看见了”，但没“理解”"></a>我的测试结果：模型确实“看见了”，但没“理解”</h2><p>我选了几张相对简单的几何图形来测试，左侧为模型识别出的代码定义，右侧为渲染结果。</p><p>case 1:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/3.png" class=""><p>case 2:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/4.png" class=""><p>case 3:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/5.png" class=""><p>case 4:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/6.png" class=""><p>case 5:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/7.png" class=""><p>case 6:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/8.png" class=""><p>case 7:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/9.png" class=""><br/><p>模型确实能识别出圆、线、点这些基础元素，但问题在于它只停留在“形似”的层面。</p><br/><br/><h2 id="存在的主要问题："><a href="#存在的主要问题：" class="headerlink" title="存在的主要问题："></a>存在的主要问题：</h2><ol><li><p><strong>元素类型太少</strong></p><p>目前几乎只能识别「圆、直线、点」三类对象。</p><p>没有弧线、角度等基础构造，复杂图形几乎全失真。</p></li><li><p><strong>没有样式信息</strong></p><p>模型输出完全忽略了虚线、颜色、标记这些样式描述，而这些恰恰是几何图表达逻辑关系的关键。</p></li><li><p><strong>缺乏约束与推理链路</strong></p><p>从上面的测试案例能很明显看到，作为 OCR 模型，它并不理解几何推理。</p><p>比如在识别一个多边形的图后，它会画出所有的边，但产生了不闭合的情况。</p><p>对它而言，交点、垂直等等只是形状，不是满足约束的结构。</p></li></ol><p>deepseek-ocr 目前的几何识别，像是一个会抄图的学生——会画样子，但不会推理。</p><br/><br/><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>虽然模型结果不理想，但我认为这是一个重要的信号：<strong>OCR 模型正在从“识别文字”走向“理解结构”。</strong></p><p>几何识别比文字识别复杂得多，它需要同时理解<strong>视觉结构</strong>和<strong>逻辑约束</strong>。</p><p>deepseek 这次的尝试，说明主流大模型团队开始意识到这一方向的重要性。</p><p>对我来说，这正好验证了我在做的另一件事——让 AI 不只是识别几何图，而是能<strong>构造 + 约束 + 验证</strong>整个几何过程。</p><p>在我开发的大角几何画板中，我们的目标不是“识别图像”，而是让 AI 具备“几何理解能力”。</p><p>当用户上传一张图时，系统不仅要知道“有圆、有直线”，还要能<strong>自动重建几何关系</strong>：</p><ul><li>哪些线是垂直的？</li><li>哪些点在同一条线上？</li><li>这两个圆相切还是相交？</li><li>哪条辅助线是解题关键？</li></ul><p>这些才是“理解”层面的能力。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>deepseek-ocr 的几何识别，还远不算成立。</p><p>但它标志着一个趋势：<strong>AI 正在学习去“看懂”图形的结构世界。</strong></p><p>也许几年后，我们真的能拍下几何题图，AI 自动识别、重构、推理、作答。</p><p>但在那之前，我们还要继续探索 <strong>几何语言、推理逻辑与可执行构造</strong> 的结合方式。</p><p>如果你也对 “AI 如何真正理解数学” 感兴趣，欢迎关注我，一起见证这场变化。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>当几何遇上 CodeAct 范式：从语言理解到可执行推理的跃迁</title>
      <link>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</link>
      <guid>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</guid>
      <pubDate>Fri, 07 Nov 2025 13:36:31 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;em&gt;面向 Agent 开发者的工程与范式探索&lt;/em&gt;&lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;引言：从“语言理解”到“执行推理”&quot;&gt;&lt;a href=&quot;#引言：从“语言理解”到“执行推理”&quot; class=&quot;headerlink&quot; title=&quot;引言：从“语言理解”到“执行</description>
        
      
      
      
      <content:encoded><![CDATA[<p><em>面向 Agent 开发者的工程与范式探索</em></p><br/><h2 id="引言：从“语言理解”到“执行推理”"><a href="#引言：从“语言理解”到“执行推理”" class="headerlink" title="引言：从“语言理解”到“执行推理”"></a>引言：从“语言理解”到“执行推理”</h2><p>在我的几何 AI 项目中，模型第一次尝试“画一个等腰三角形”时，图形看似完美，实际上两边长度并不相等。</p><p><strong>AI画得像，却没画对。</strong></p><p>几何画图任务并非自然语言理解，而是 <strong>构造 + 约束 + 验证</strong> 的闭环过程。</p><p>传统的 “Planner + DSL + Verifier” 体系虽然可控，但在动态构造与反复验证中显得笨重。</p><p>我开始思考：</p><p>如果让 AI 不再只是描述，而是<strong>直接写出代码、执行代码、并根据结果再思考</strong>呢？</p><p>这正是 CodeAct（<em>Executable Code Actions Elicit Better LLM Agents</em>）所提出的核心理念。</p><p>CodeAct 不再让语言模型“说怎么做”，而是让它“自己去做”。</p><br/><br/><h2 id="CodeAct-的底层机制解析"><a href="#CodeAct-的底层机制解析" class="headerlink" title="CodeAct 的底层机制解析"></a>CodeAct 的底层机制解析</h2><p>让模型写代码只是表象，真正的关键是形成一个可执行的闭环思维循环。</p><h3 id="1-ReAct-与-CodeAct"><a href="#1-ReAct-与-CodeAct" class="headerlink" title="1. ReAct 与 CodeAct"></a>1. ReAct 与 CodeAct</h3><table><thead><tr><th>对比项</th><th>ReAct</th><th>CodeAct</th></tr></thead><tbody><tr><td>表达形式</td><td>Thought + Action + Observation（文本）</td><td>Thought + <code>&lt;execute&gt;</code> + Observation（代码执行）</td></tr><tr><td>动作实现</td><td>由外部解析器调用预定义工具</td><td>直接生成并执行函数</td></tr><tr><td>控制流</td><td>受限（多步靠多轮调用）</td><td>原生支持 if &#x2F; for &#x2F; try</td></tr><tr><td>可观察性</td><td>文本输出</td><td>代码执行结果（return&#x2F;stdout&#x2F;error）</td></tr><tr><td>优势</td><td>安全、可控</td><td>执行力强、贴合训练分布</td></tr><tr><td>局限</td><td>动作空间有限</td><td>执行安全、调试成本高</td></tr></tbody></table><br/><p>CodeAct 的核心在于：让 LLM 的输出<strong>成为可直接执行的程序</strong>，并用执行结果作为新的输入。</p><p>这种结构让模型不仅描述动作，而是直接拥有动作。</p><br/><h3 id="2-执行循环机制"><a href="#2-执行循环机制" class="headerlink" title="2. 执行循环机制"></a>2. 执行循环机制</h3><h4 id="典型逻辑（伪代码）"><a href="#典型逻辑（伪代码）" class="headerlink" title="典型逻辑（伪代码）"></a>典型逻辑（伪代码）</h4><figure class="highlight ts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!done) &#123;</span><br><span class="line">  <span class="keyword">const</span> thought = llm.<span class="title function_">generate</span>(context)</span><br><span class="line">  <span class="keyword">const</span> code = <span class="title function_">extractExecuteBlock</span>(thought)</span><br><span class="line">  <span class="keyword">const</span> result = sandbox.<span class="title function_">run</span>(code)</span><br><span class="line">  context.<span class="title function_">push</span>(<span class="string">`Execution Output:\n<span class="subst">$&#123;result&#125;</span>`</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行过程包含四个核心步骤：</p><ol><li><strong>抽取代码块</strong>：识别 <code>&lt;execute&gt;...&lt;/execute&gt;</code> 中的代码片段</li><li><strong>沙箱执行</strong>：运行在受限函数或容器中</li><li><strong>结果反馈</strong>：stdout &#x2F; return &#x2F; error 都回写上下文</li><li><strong>下一轮推理</strong>：模型读取结果、修复代码、继续执行</li></ol><p>CodeAct 通过这个循环，实现了 “思考 → 执行 → 观察 → 再思考” 的闭环，而这正是传统 ReAct 模式所缺乏的。</p><br/><br/><h2 id="几何-AI：CodeAct-的落地场景"><a href="#几何-AI：CodeAct-的落地场景" class="headerlink" title="几何 AI：CodeAct 的落地场景"></a>几何 AI：CodeAct 的落地场景</h2><p>几何画图任务天然契合 CodeAct，因为它具备以下特征：</p><ul><li><strong>构造性</strong>：点、线、圆的依赖顺序明确；</li><li><strong>约束性</strong>：角度、平行、对称等条件必须验证；</li><li><strong>可验证性</strong>：每次执行都能计算验证结果。</li></ul><p>在我的项目中，LLM 会直接生成 TypeScript 代码来驱动几何引擎：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">execute</span>&gt;</span></span><br><span class="line">const A = point(0, 0)</span><br><span class="line">const B = point(2, 0)</span><br><span class="line">const C = perpendicular(A, B, 1.5)</span><br><span class="line">drawTriangle(A, B, C)</span><br><span class="line">return checkIsosceles(A, B, C)</span><br><span class="line"><span class="tag">&lt;/<span class="name">execute</span>&gt;</span></span><br></pre></td></tr></table></figure><p>执行结果会返回布尔值或错误信息（如“点C未定义”），再交回给模型进行下一步思考。<br>这使得“构造—验证—修正”形成完整闭环。</p><br/><br/><h2 id="五、范式比较与适用场景"><a href="#五、范式比较与适用场景" class="headerlink" title="五、范式比较与适用场景"></a>五、范式比较与适用场景</h2><p>当然，CodeAct 不是所有场景的答案，而是执行密集型任务的最优解。</p><table><thead><tr><th>范式</th><th>核心思想</th><th>典型场景</th><th>优势</th><th>局限</th></tr></thead><tbody><tr><td><strong>ReAct</strong></td><td>Text-based reasoning + action schema</td><td>工具调用、问答代理</td><td>结构简单、安全</td><td>不支持循环、执行力弱</td></tr><tr><td><strong>CodeAct</strong></td><td>代码即行动，可执行推理</td><td>几何、数据分析、自动化流程</td><td>灵活、图灵完备</td><td>安全与可控性挑战</td></tr><tr><td><strong>Planner + DSL + Verifier</strong></td><td>高层规划 + 安全执行</td><td>企业工作流、合规系统</td><td>可治理性强</td><td>扩展性低</td></tr></tbody></table><br/><p>这些范式不是“演化关系”，而是“任务匹配曲线”：</p><ul><li>若你需要<strong>稳定且可控</strong>的 Agent，ReAct 足够；</li><li>若你希望<strong>模型主动执行与验证</strong>，CodeAct 更高效；</li><li>若你构建<strong>大型协作系统</strong>，PEER&#x2F;Planner 才是路径。</li></ul><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>CodeAct 的潜力在于它把 “生成式 AI” 推向 “行动式 AI”。<br>今天我们已经看到：</p><ul><li><strong>Runtime 自检（Self-checking Runtime）</strong>：模型生成代码 → 执行 → 自动生成测试样例验证输出；</li><li><strong>静态分析 + 动态执行结合</strong>：利用 AST 检查危险调用；</li><li><strong>跨语言统一执行层</strong>：JS&#x2F;Python&#x2F;Go 均可成为 Action carrier；</li><li><strong>Agent Runtime 生态</strong>：不同 CodeAct 系统可互通共享执行上下文。</li></ul><p>CodeAct 让模型拥有代码生成力、执行力与修正力。</p><p>在几何 AI 的世界里，这意味着从 “我能解释几何” 到 “我能构造几何”。</p><p>也意味着从<strong>语言智能</strong>迈向<strong>行动智能</strong>的转折。</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><strong>Wang, Xingyao et al.</strong> <em>Executable Code Actions Elicit Better LLM Agents.</em> ICML 2024. <a href="https://arxiv.org/abs/2402.01030">PDF</a> &#x2F; <a href="https://proceedings.mlr.press/v235/wang24h.html">PMLR</a></li><li><strong>CodeAct GitHub 实现</strong> – <a href="https://github.com/xingyaoww/code-act">github.com&#x2F;xingyaoww&#x2F;code-act</a> <a href="https://github.com/langchain-ai/langgraph-codeact">github.com&#x2F;langchain-ai&#x2F;langgraph-codeact</a></li><li><strong>Mohsin Mubarak</strong>, <a href="https://medium.com/@mohsin.sk.9820/paper-explained-blog-codeact-revolutionizing-llm-agents-with-executable-code-actions-c960cc5e30eb">CodeAct: Revolutionizing LLM Agents with Executable Code Actions</a> <em>Medium</em>, 2024.</li><li><strong>Survey:</strong> <a href="https://openreview.net/pdf?id=ekl6Z88uQj">Code Reasoning for Code Tasks: A Survey and A Call to Action.</a> OpenReview 2024</li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>突破 MoE 限制，PEER 架构如何推动超级智能的未来发展</title>
      <link>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</link>
      <guid>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</guid>
      <pubDate>Wed, 05 Nov 2025 14:26:33 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;a href=&quot;#引言：大模型架构的“困境”&quot; class=&quot;headerlink&quot; title=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;/a&gt;引言：大模型架构的“困境”&lt;/h2&gt;&lt;p&gt;这两年，大模型的风头正劲。&lt;/p&gt;
&lt;p&gt;大家都在谈</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：大模型架构的“困境”"><a href="#引言：大模型架构的“困境”" class="headerlink" title="引言：大模型架构的“困境”"></a>引言：大模型架构的“困境”</h2><p>这两年，大模型的风头正劲。</p><p>大家都在谈 <strong>MoE（Mixture of Experts）</strong> ——它被认为是突破大模型计算瓶颈的关键方向。</p><p>通过<strong>稀疏激活</strong>，只激活少量专家节点，让计算开销成比例下降，而模型能力却可以继续增长。听上去完美无缺。</p><br/><p>不过，真正尝试将 MoE&#x2F;SMoE 架构落地时，问题随之而来。</p><p>专家池一多，计算分配就变得不平衡；某些专家被频繁调用，而其他专家几乎闲置。系统扩展起来也不再优雅，想加新领域，常常意味着“推倒重来”。</p><p>MoE 虽然高效，却还远没到“万能”的阶段。</p><p>尤其对于我们这些做 <strong>多领域智能 Agent</strong> 的人来说——客服、教育、企业知识管理——系统要在不同任务之间灵活切换、持续学习，这套架构似乎仍有点笨重。</p><br/><p>然而，《Mixture of A Million Experts》论文带来了新的思路，打破了行业普遍认为大模型计算瓶颈无法突破的常规认知。</p><p><strong>研究者提出了一个颠覆性的观点：专家数量并非瓶颈，而是可以突破的限制。</strong></p><p>通过 <strong>PEER 架构（Product Key Expert Retrieval）</strong>，模型可以在保持轻盈的同时，拥有百万级专家容量，彻底改变大模型的设计和扩展方式。</p><p><strong>大模型的未来，不再是一颗大脑，而是无数个微专家在协同工作。</strong></p><br/><br/><h2 id="MoE-架构：风光背后的隐忧"><a href="#MoE-架构：风光背后的隐忧" class="headerlink" title="MoE 架构：风光背后的隐忧"></a>MoE 架构：风光背后的隐忧</h2><p>MoE 的设计初衷其实很优雅：</p><p>与其让一个庞大的模型处理所有问题，不如分工合作——不同专家解决不同任务。</p><p>这样既能减少计算，又能扩展模型容量。</p><br/><p>但问题是，当“专家”变多，管理它们本身就成了一门学问。<br>MoE 模型在训练时容易出现负载不均：</p><ul><li>某些专家被反复选中，工作超载；</li><li>某些专家几乎从不被调用，形同虚设。</li></ul><p>而且专家数量也存在“物理极限”。几百个模块听起来不少，但放到一个需要多领域、多任务的 Super Agent 系统中，很快就会捉襟见肘。</p><p>MoE 的结构像是一栋办公楼——房间足够多，但一旦某几层太忙、几层太空，效率就会被拖垮。</p><p>MoE 的问题不是不聪明，而是不够灵活。它像一台巨大的机器，而不是一个会生长的生态系统。</p><br/><br/><h2 id="PEER-架构：突破-MoE-限制的那一把钥匙"><a href="#PEER-架构：突破-MoE-限制的那一把钥匙" class="headerlink" title="PEER 架构：突破 MoE 限制的那一把钥匙"></a>PEER 架构：突破 MoE 限制的那一把钥匙</h2><p>在这篇论文里，研究者提出了一个更激进的设想：</p><p>既然“专家”是瓶颈，那我们不如把它拆得更小、更细。</p><p>于是便有了 <strong>PEER（Product Key Expert Retrieval）</strong> 架构——一个拥有 <strong>百万级微专家</strong> 的系统。</p><br/><h3 id="1-百万级-experts-：打破扩展的上限"><a href="#1-百万级-experts-：打破扩展的上限" class="headerlink" title="1. 百万级 experts ：打破扩展的上限"></a>1. 百万级 experts ：打破扩展的上限</h3><p>MoE 的专家通常是完整的子网络，庞大且昂贵；</p><p>而 PEER 则反其道而行，每个专家只保留 <strong>一个神经元（one-neuron MLP）</strong>。</p><br/><p>这种极简设计的好处是：</p><ul><li>可以容纳上百万个专家；</li><li>每次推理只需激活极少一部分；</li><li>模型容量得到几何级提升，但计算量几乎不变。</li></ul><p>这就像把庞大的知识体系拆成了无数个“知识细胞”，<br>每个细胞只在特定输入下工作，真正实现了“用多少、算多少”。</p><p>PEER 不追求更强的单体智能，而是让微小的智慧汇聚成群体的力量。</p><br/><h3 id="2-极致稀疏激活：从-O-N-到-O-√N"><a href="#2-极致稀疏激活：从-O-N-到-O-√N" class="headerlink" title="2. 极致稀疏激活：从 O(N) 到 O(√N)"></a>2. 极致稀疏激活：从 O(N) 到 O(√N)</h3><p>传统 MoE 的瓶颈之一是路由复杂度。</p><p>每次输入都要计算与 N 个专家的匹配度，复杂度是 O(N)。</p><p>当专家数量达到百万级时，这几乎不可接受。</p><p>PEER 用 <strong>Product Key Routing（产品键路由）</strong> 巧妙地把这个问题降到了 **O(√N)**。</p><br/><p>想象一下专家池是一个二维表格。</p><p>每个专家的“键”被拆成两部分，分别属于两个子空间。</p><p>当输入到来时，只需：</p><ol><li>在每个子空间里各自检索 top-k；</li><li>再组合成候选专家集合。</li></ol><p>最终，模型只需从少量候选中选择真正相关的专家。</p><p>这样既避免了全量扫描，又保持了高匹配率。</p><p>PEER 的路由逻辑更像搜索引擎，而非广播通知。它只找需要的人，而不是通知所有人。</p><br/><h3 id="3-Product-Key-Routing：专家调度的自组织"><a href="#3-Product-Key-Routing：专家调度的自组织" class="headerlink" title="3. Product Key Routing：专家调度的自组织"></a>3. Product Key Routing：专家调度的自组织</h3><p>PEER 的路由不仅高效，还具备“自组织”的特性。</p><p>每个专家的键是可学习的，随着训练推进，模型会自然地将相似任务分配给相近的专家集合。</p><br/><p>久而久之，系统内部形成了类似“语义社区”的结构：</p><p>不同类型的问题，倾向激活不同区域的专家群落。</p><p>这让模型在面对多任务学习时，既能共享知识，又能保持分工明确。</p><p>PEER 让模型像一个自我成长的城市，不同街区专注不同产业，但彼此之间又保持联通。</p><br/><h3 id="4-动态扩展专家池：支持持续学习与多领域适应"><a href="#4-动态扩展专家池：支持持续学习与多领域适应" class="headerlink" title="4. 动态扩展专家池：支持持续学习与多领域适应"></a>4. 动态扩展专家池：支持持续学习与多领域适应</h3><p>另一个令人惊喜的点在于，PEER 支持 <strong>专家池的动态扩展</strong>。</p><p>也就是说，模型训练完后，仍然可以按需增加新专家，而无需重训全体网络。</p><p>这对于多领域智能 Agent 来说，是一种质变。</p><p>想象一个助手AI：</p><ul><li>初期专注电商；</li><li>后来扩展到金融、旅游、医疗领域；</li><li>不需要“重生”，只需“长出新神经元”。</li></ul><p>这种能力让 AI 系统能像产品一样，持续成长，而不是一成不变的快照,不断去迭代新的版本。</p><br/><br/><h2 id="PEER-架构的产品启示"><a href="#PEER-架构的产品启示" class="headerlink" title="PEER 架构的产品启示"></a>PEER 架构的产品启示</h2><p>从产品角度看，PEER 的价值不仅在于“能跑得更快”，而在于“<strong>能持续成长</strong>”。</p><ol><li><p><strong>可扩展性</strong></p><p>新领域到来时，不需要重训全模型，只需新增专家池。</p><p>这让产品拥有真正的 <strong>演进式架构</strong>。</p></li><li><p><strong>计算效率与成本控制</strong></p><p>极致稀疏激活意味着按需使用算力，推理成本显著降低。</p><p>对 SaaS 型 AI Agent 平台尤为重要——既能大规模服务，又能保持性价比。</p></li><li><p><strong>长期学习与知识积累</strong></p><p>PEER 的设计天然支持 <strong>持续学习</strong>。</p><p>这意味着 AI Agent 不会像旧模型那样被“冻住”，而是能随着使用场景的变化，积累新知识。</p></li></ol><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>PEER 架构最打动我的地方，是它代表了一种新的思维方式：</p><p>我们不再把智能看成一个中心化的大脑，而是一张能自我生长、不断优化的网络。</p><p>对于 Super Agent 的未来而言，这正是我们缺失的那一环。</p><p>也许下一个时代的智能，不是更大的模型，而是更会生长的系统。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/10 Review</title>
      <link>https://blog.liluhui.cn/2025/11/01/202510/</link>
      <guid>https://blog.liluhui.cn/2025/11/01/202510/</guid>
      <pubDate>Sat, 01 Nov 2025 13:34:22 GMT</pubDate>
      
      <description>炉要小，火要旺</description>
      
      
      
      <content:encoded><![CDATA[<p><em>炉要小，火要旺</em></p><br/><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01</p><p>人常说，<strong>期望会影响现实，创造出自我实现的预言。</strong><br>这些年，这种说法有许多相近的面孔：<br>“向宇宙许愿，它就会来。”<br>“你发出的能量，会以另一种形式回到你身上。”<br>“信则有。”<br>可在我看来，那些形式并不重要。<br>真正起作用的，是<strong>人的愿力</strong>。</p><p>因为足够想要，于是卯足了劲。<br>因为足够想要，于是显得勇敢。<br>因为足够想要，于是能感染他人。<br>因为足够想要，于是连认知都会被扭曲。</p><p>我并不贬义地说“扭曲”。<br>专注于一件事，本就是一种偏折，一种放大。<br>深度往往带来变形，这是客观的。</p><p>人要正视自己心中的“想要”。<br>不是靠仪式去塑造愿力，<br>而是靠体验去发现它、沉淀它、深化它。<br>否则，就容易在模糊的欲望里耗尽数十年，<br>搭建出一个精致却虚幻的黄粱梦。</p><p>从来不是那几支香实现了愿望，<br>只是有人<strong>真的去做了</strong>。<br>那些没能成的事，往往就差在这一步。</p><p>人总爱标榜“我是怎样的人”，<br>却不愿脏手，不想受苦。<br>有些选择可以优中择优，<br>但更多时候，人生只能一一排除，<br>留下的，就是方向。</p><p><strong>先活下来，先让心境平稳，<br>才有余裕去优选。</strong></p><br/><p>02<br>痛苦情绪在人的生活中是无法避免的。<br>它不是意外的杂质，而是人性的一部分。<br>当我们急着否定它、逃避它、压抑它的时候，<br>其实也在否认那部分真实的自己。</p><p>错误地认为成熟意味着时刻保持平静，<br>意味着能“控制情绪”，<br>但有时候，真正的成熟恰恰是——<strong>允许自己痛。</strong><br>允许悲伤，允许失落，允许迷茫地站在原地。<br>因为那些情绪并不是“坏的”，<br>它们只是告诉我们，<strong>哪里被触碰了，哪里还在乎。</strong></p><p>痛苦是对生活的敏感反应。<br>正因为你仍有渴望、有期待、有爱，<br>痛苦才会出现。<br>若没有这些，世界对你也就再无波澜。</p><p>有时候，痛苦只是提醒你——<br>你还在活着。<br>而<strong>活着</strong>，本身就意味着不可能只拥有快乐的一面。</p><br/><p>03</p><p>在《金钱的艺术中》给出<strong>财务独立</strong>的等级：</p><p>第0级:完全依赖陌生人的善意，毫无掌控力。<br>第1级:依赖家人、朋友或慈善的救助才能生活，<br>第2级:依赖政府或社会保障体系维持生计。<br>第3级:靠工作养活自己，但一旦失业就陷入困境。<br>第4级:有少量储蓄，能短期应对意外开支。<br>第5级:积蓄能支撑几个月，即使暂时失业也不至于崩溃。<br>第6级:技能或职业有一定不可替代性，不易被取代。<br>第7级:能拒绝糟糕的老板或工作，真正拥有选择权。<br>第8级:即使换城市、换行业，也能凭储蓄和能力顺利过渡。<br>第9级:拥有稳定的资产或副业收入，减少对单一工作的依赖。<br>第10级:储蓄和投资足以覆盖数年的生活成本<br>第11级:即便遭遇金融危机或行业衰退，也能从容应对。<br>第12级:被动收入足以覆盖基本生活开支，不再依赖工作生存。<br>第13级:储蓄和资产支撑的不仅是温饱，而是理想的生活方式。<br>第14级:财富充裕到足以支持你做任何想做的事。<br>第15级:完全自由，能随心所欲支配时间，按照自己的方式生活。</p><p>感觉自己的理财账算下来也快稳稳到12级了，却还受着7级的苦。<br>背后是因为想要做成一些事，总是要接受一些现实的不舒服，因为想要实现的自我，也想要抵达13级，还是需要沉下心做好事。<br>人不能总是追求那个享乐的结果，只要在做事，想要把上下的链路跑通，想要把相关的人顺清楚，总是有些苦恼的，会失落、会气愤，也会经历欣喜、满足，这些过程中的体验都算人生的数。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>这个月一边在大角项目和只记项目里实践 Agent，一边再输出内容，也算是给自己严格安排了写作任务，一个月10篇，有不满意的，也有写完很舒畅的，但不论完美与否完成了还是开心的，值得鼓励。（八百年都没这么勤快过）<br>从数据上来看，标题还是太重要了，大众一点也不喜欢抽象和自以为是的高高在上，有些名字起的挺失败的，但我还是想试试市场，可惜点击率太低连流量都进不了下一个池子。<br>现在有两个 Agent 系列并行，<strong>Agent Memory 深入系列</strong>累计到现在一共 8篇了，<strong>Agent 大众科普系列</strong> 就比较随意想到就安排写了，中间还尝试想开 <strong>Agent Design Pattern 系列</strong>，第一篇不太理想，不过还是想试试这个方向。<br>总之越写想法越多，慢慢来吧。</p><br/><p>本月更新<br><a href="https://mp.weixin.qq.com/s/y03S4X_GqGorua10PROmBw">AI 可以像人类一样选择性记忆了吗？现在真有人在做这事了</a> | 技术<br><a href="https://mp.weixin.qq.com/s/K4boDF2ZSIDjVosY3EyOZA">你以为你在和 AI 聊天，其实你在调度一个系统</a> | 科普<br><a href="https://mp.weixin.qq.com/s/5wGsoUshx6mhCbra_7xSZw">Agent 又又失忆了！我来做一次记忆体检</a> | 技术<br><a href="https://mp.weixin.qq.com/s/13Pkwku9rJLQrl4Bs0sMWw">什么让一个 AI 系统成为“智能体”（Agent）？</a> | 技术<br><a href="https://mp.weixin.qq.com/s/v9VcuiBE23eb54JmmDxckA">一文看懂，Agent 避免记忆漂移三大策略</a> | 技术<br><a href="https://mp.weixin.qq.com/s/ZCPeEu5noEoy6RRx55hm9w">一句话讲明白，什么是多 Agent 系统？</a> | 科普<br><a href="https://mp.weixin.qq.com/s/uAhbuENNSiIszO4iVAfIVw">一句话讲明白：Agent Memory 是什么？</a>  | 科普<br><a href="https://mp.weixin.qq.com/s/Aa01jvz3q1nZp2f5F4loKQ">Agent 记忆检索策略：怎样学会想起？</a> | 技术<br><a href="https://mp.weixin.qq.com/s/2f9ngZGTd0f-5LO91qMOuw">把 MCP 放回它该在的位置</a>  | 科普<br><a href="https://mp.weixin.qq.com/s/1OLbkeaWQaIAEgOQ-30LTw">Agent 记忆写入三大策略，决定“记什么”的工程学</a> | 技术</p><br/><p>02<br>大角几何画板上线以来的数据增长还是非常明显，上个月的活跃用户有2k，这个月单月达到了3k，随着用户反馈需要的绘图功能需求越来越多，目标用户群体想解决的场景是编排出卷、是比赛判定、是教学演示… 每一个场景想覆盖完整一个最小闭环还有很多的细节，远不是只优化AI能搞定的。<br>我现在犹豫的是整件事情的初心和现状其实有不少偏离，这本是一个重AI的研究型做影响力的项目，现在在深入用户群体后要解决的是场景问题，需要传统工程上的很多工作量，而这个模式养不活团队。<br>做影响力的故事和做场景的商业价值，两套打法本就是不一样的，我有全新的更轻的想法，但不是一个好的时机。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/Pasted%20image%2020251101211731.png" alt="image"></p><br/><p>03<br>自媒体方向也感受到了欣喜的数据增长和流量退坡的焦虑，前面三个月小红书从0干到1000，十月单月又增加了500，开始同步注册其他平台，全平台粉丝数也达到了 2k。</p><p>总结下这个月从运营账号中学会的：</p><ol><li>安排好数据复盘的规律，安排好回复消息的时间段，数据下降就下次调整不要纠结着当期的内容，重要的永远是持续的节奏。 ——  比如被风控，去折腾能浪费掉一整天，最终的结果是发出来了也是最小流量池，没必要。</li><li>我本来以为要过个半年一年才能体会到平台数据规律，现在没必要管，但现在运营起来已经能很明确地在内容发布6小时后知道这篇的潜力了，只要持续做，目标人群的数据规律很快就会浮现出来。—— 其实跑不出来的内容1小时候就能下判断了，最小流量池，但凡吸引力达标就能过。</li><li>标题党避免不了，建议收藏、一文看懂 这些词就是有用，不然大家根本不想看。比如“一句话说明白”就是比“一文看懂”数据更好。</li><li>要看好，大家都是视觉动物，有那么东西可以看的情况下，大家也是本能想看更美好的更舒服的。</li><li>要么有用，要么有情绪。想要提升互动量，还是需要内容本身有价值，我现在就很不擅长提供情绪价值，做不到卡兹克老师建议的唤起共鸣，另一方面也是现在的定位偏向更有用，也在开始尝试加入一些带有情绪点的表达。</li></ol><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/Pasted%20image%2020251101212051.png" alt="image"></p><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>年度拼图 +1<br>虽然但是，春天买的拼图，拖到了秋天才拼完。<br>这是一幅自然风景——有猫、有狗、有风、有茶，一种向往的悠闲生活。<br>坐下来拼图的时候，时间走得飞快。像数独一样，一坐就是三四个小时，全身心沉浸在那一片片形状与颜色的线索中。</p><p>也许这就是专注的魔力——当意识完全投入某个小小的世界，时间便失去了意义。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/167782d55a9ba2e34b155e6463d049db.jpg" alt="image"></p><br/><p>02<br>杭州的晚霞，随手拍下的，感受到一种宏伟感。<br>金色的光从云层的缝隙间涌出，一道又一道，像被世界温柔包裹。<br>或许这就是晚霞的力量吧：在疲惫的时刻，给人一个喘息的窗口，提醒我们，天还很高，路还很长。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/857e1fd90c30ead8b606e18175037e74.jpg" alt="image"></p><br/><p>03<br>定制了一个好玩的印章，盖上去是自己的头像。<br>印在手账页上，像是在给自己签名，又像是给生活做一个温柔的注脚。<br>那种“按一按”的小仪式，让平凡的记录多了一点体温。</p><p>我想，人需要这些无用的小物件。<br>它们没有目的，只是让生活有点趣味、有点痕迹 XD。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/46177c9dd0d1028f0857bb8caaff7b80.jpg" alt="image"></p><br/><p>04<br>自从加入霖子工作室的瑜伽练习，进步得让自己都有些惊讶。<br>这两个月尝试了许多从未体验过的动作，尤其是倒立。<br>练习的时候，我发现自己身体里藏着许多“怯懦”——  该放松的地方僵硬，该用力的地方又松散。<br>像极了生活中那些犹豫和不信任自己的时刻。</p><p>倒立最难的，不是力量，而是信任。<br>你得先相信自己能稳住，身体才会真的稳住。</p><p>我时常羡慕那些没有沉默成本的人，<br>能轻易推翻从前，重新开始，不怕浪费，不怕错。<br><strong>他们像孩子一样去学习、去构建，<br>不带惯性地感受每一个动作、每一段路。</strong></p><p>也许成长就是这样一件事：<br><strong>不断拆除旧的自己，在一次次倒立与坠落中，<br>重新找到身体与心的平衡。</strong></p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/cdc079bacf9eb5e4da2481c5ef7653b8.jpg" alt="image"></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/01/202510/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI 如何自主管理记忆？三种前沿架构详解 A-MEM / Mem-α / Mem0</title>
      <link>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/</link>
      <guid>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/</guid>
      <pubDate>Fri, 31 Oct 2025 09:23:13 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：AI-的记忆问题&quot;&gt;&lt;a href=&quot;#引言：AI-的记忆问题&quot; class=&quot;headerlink&quot; title=&quot;引言：AI 的记忆问题&quot;&gt;&lt;/a&gt;引言：AI 的记忆问题&lt;/h2&gt;&lt;p&gt;我们常常说 AI 越来越像人类，但在“记忆”这一环节，现有的系统仍然</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：AI-的记忆问题"><a href="#引言：AI-的记忆问题" class="headerlink" title="引言：AI 的记忆问题"></a>引言：AI 的记忆问题</h2><p>我们常常说 AI 越来越像人类，但在“记忆”这一环节，现有的系统仍然远未达到真正的类人能力。</p><p>在之前的文章《Agent 如何避免记忆漂移：三大策略与工程实践》中，我们讨论了如何设计一个稳定且高效的记忆系统，并分享了三大策略帮助解决记忆一致性和长期记忆问题。</p><p>然而，这些策略多侧重于工程实践，强调了如何避免记忆的失真、漂移和过度遗忘。</p><p>今天，我们要深入探讨一个更前沿的课题：<strong>AI 如何自主决定自己的记忆？</strong> </p><p>这一问题不仅挑战了传统的记忆存储方式，也为智能系统提供了更高的灵活性和自适应能力。</p><br/><p>通过 AI 系统根据任务需求和经验，自主更新和优化记忆，AI 可以更智能地应对复杂的任务。</p><p>在这篇文章中，我们将介绍三种突破性的记忆管理方案——<strong>A-MEM、Mem-α 和 Mem0</strong>，并讨论它们如何通过创新的记忆架构，推动 AI 系统在长期任务中的表现。</p><br/><br/><h2 id="记忆管理的基本概念"><a href="#记忆管理的基本概念" class="headerlink" title="记忆管理的基本概念"></a>记忆管理的基本概念</h2><p>在深入这三种方案之前，我们先简单回顾一下 AI 记忆管理的基本概念。</p><p>通常，AI 系统的记忆可以分为两大类：</p><ol><li><p><strong>工作记忆</strong></p><p> 类似于人类的短期记忆，用于存储和处理当前任务的信息。它是即时性的，帮助 AI 快速做出决策。</p></li><li><p><strong>长期记忆</strong></p><p> 涉及 AI 系统在长时间内积累的知识和经验，帮助其从过去的经验中学习并做出更好的决策。</p></li></ol><p>然而，AI 的“记忆”常常是静态的，即记忆的内容一旦储存，就不会再变化或更新。</p><p>这种设计使得 AI 在面对长期任务或多轮交互时，常常面临记忆的局限性，容易忘记先前的重要信息，甚至陷入“记忆漂移”的问题——记忆内容的过时或错误。</p><br/><p>为了解决这一问题，<strong>Agentic Memory</strong> 提出了一个新思路：<strong>让 AI 系统自主决定记忆内容、何时更新以及如何检索</strong>。</p><p>换句话说，AI 具备了像人类一样选择性记忆的能力，能根据任务需求动态调整记忆内容。</p><br/><br/><h2 id="A-MEM：基于-Zettelkasten-方法的动态记忆"><a href="#A-MEM：基于-Zettelkasten-方法的动态记忆" class="headerlink" title="A-MEM：基于 Zettelkasten 方法的动态记忆"></a>A-MEM：基于 Zettelkasten 方法的动态记忆</h2><p>A-MEM 是一种创新的记忆架构，借鉴了 Zettelkasten 方法——一种常用于知识管理的笔记方法。</p><p>在 Zettelkasten 中，信息被分解为多个“笔记块”，每个笔记块都与其他笔记块建立联系，从而形成一个有机的知识网络。</p><p>A-MEM 将这一概念引入 AI 记忆管理，创建了一个动态的记忆系统，让 AI 根据当前的任务需求，灵活地选择和组织记忆。</p><br/><p>核心设计：</p><ul><li><p><strong>结构化记忆笔记</strong></p><p>每条记忆以结构化笔记形式存储，包含上下文摘要、关键词、标签和嵌入向量。</p></li><li><p><strong>动态链接与演化</strong></p><p>新记忆加入后，系统基于共享属性和语义相似度，自动与历史记忆建立连接，并触发对旧记忆的内容和属性更新。</p></li><li><p><strong>自组织记忆网络</strong></p><p>系统通过记忆之间的链接持续优化知识结构，无需手动提示即可逐步演化出连贯的认知网络。</p></li></ul><br/><p>在实际应用中，A-MEM 被用来提高多任务学习和推理系统的表现。</p><p>比如，假设一个 AI 需要完成一个多轮对话任务，它不仅能记住每轮对话的内容，还能根据对话的上下文，动态调整记忆结构，避免遗漏重要信息。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/agiresearch/A-mem">https://github.com/agiresearch/A-mem</a></p><br/><br/><h2 id="Mem-α：强化学习驱动的记忆管理"><a href="#Mem-α：强化学习驱动的记忆管理" class="headerlink" title="Mem-α：强化学习驱动的记忆管理"></a>Mem-α：强化学习驱动的记忆管理</h2><p>Mem-α 采用了强化学习（Reinforcement Learning, RL）来优化记忆管理。</p><p>与传统的静态记忆方式不同，Mem-α 通过奖励机制，帮助 AI 代理选择和存储最有价值的记忆。</p><p>AI 代理会根据任务的反馈信号，自动决定哪些记忆应该被强化，哪些应该被忽视或删除。</p><br/><p>核心设计：</p><ul><li><p><strong>多类型记忆系统</strong></p><p>模型支持三类记忆：核心记忆（如身份、偏好）、情节记忆（带时间标签的事件）和语义记忆（抽象知识），分别支持更新、插入、删除等操作。</p></li><li><p><strong>序列决策训练机制</strong></p><p>记忆构建被建模为一个序列决策过程，模型使用强化学习逐步学习如何构建和使用记忆。</p></li><li><p><strong>多重奖励优化目标</strong></p><p>包括回答准确性、操作规范性、记忆压缩效率和信息质量等指标，确保记忆系统在效率与表现之间取得平衡。</p></li></ul><br/><p>Mem-α 在长时间的任务执行中表现得尤为出色。</p><p>例如，在一个需要多轮决策的任务中，AI 代理能够通过强化学习逐步选择性记忆，将更多的注意力集中在当前决策需要的关键经验上。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/wangyu-ustc/Mem-alpha">https://github.com/wangyu-ustc/Mem-alpha</a></p><br/><br/><h2 id="Mem0：图数据库驱动的记忆系统"><a href="#Mem0：图数据库驱动的记忆系统" class="headerlink" title="Mem0：图数据库驱动的记忆系统"></a>Mem0：图数据库驱动的记忆系统</h2><p>Mem0 是一个基于图数据库的记忆系统。通过图数据库，Mem0 将记忆内容模块化，允许不同记忆块之间通过关系连接，形成一个可以灵活查询的记忆网络。这个设计使得 AI 能够在复杂任务中有效检索和管理记忆内容。</p><p>核心设计：</p><ul><li><p><strong>两阶段记忆机制</strong></p><p>首先由 LLM 从用户交互中提取“候选记忆”（结构化事实或摘要），然后与历史记忆比对后，智能决定添加、更新、删除或跳过。</p></li><li><p><strong>多层次记忆结构</strong></p><p>支持用户级、会话级和代理级的记忆管理，确保跨场景、跨时间段的记忆一致性。</p></li><li><p><strong>成本控制与响应效率</strong></p><p>与传统将全部历史放入上下文的方式相比，Mem0 显著降低 token 消耗与响应延迟，并提高答案准确率。</p></li></ul><br/><p>Mem0 在多轮对话和长期任务中表现得尤为出色。</p><p>在实际应用中，Mem0 能够根据对话历史或任务进展，快速检索到相关的记忆内容，并作出更加智能的反应。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/mem0ai/mem0">https://github.com/mem0ai/mem0</a></p><br/><br/><h2 id="其他自主记忆方案"><a href="#其他自主记忆方案" class="headerlink" title="其他自主记忆方案"></a>其他自主记忆方案</h2><p>除了 A-MEM、Mem-α 和 Mem0 之外，还有一些其他方案也在探索 AI 自主决定记忆的方向。</p><p>例如，<strong>Episodic Memory</strong> 关注的是通过情节记忆来支持 AI 的长时记忆能力，<strong>Vision-Language Episodic Memory（VLEM）</strong> 则通过结合视觉和语言模态，增强了记忆系统的多模态能力。而 <strong>Memory-as-Action</strong> 则通过将记忆管理视为可学习的行为，提升了系统的自适应性。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>随着 AI 技术的进步，记忆管理 将不再是简单的存储和检索任务，而是演变 <strong>AI 自主决策</strong>的动态过程。</p><p>展望未来，记忆的自我更新机制、跨模态记忆整合和长期记忆优化等技术将进一步推动 AI 系统更智能地管理和利用自己的记忆，不断提升其处理长期任务的能力。</p><p>然而，这种记忆自主权的提高也引发了一个耐人寻味的思考：<strong>我们真的要把“选择记忆”的权力交给算法吗？</strong></p><p>当 AI 可以自主决定保留什么、遗忘什么，它在某种意义上也开始塑造自己的“身份认同”。</p><p>而人类最终或许只是那个在幕后制定基本规则的系统设计者。</p><p>这不禁让人想起科幻电影《银翼杀手》中复制人罗伊的临终独白：</p><blockquote><p>所有这些时刻终将流失在时光中，<br/>一如眼泪消失在雨中。</p></blockquote><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li><strong>A-MEM 论文</strong>：<a href="https://arxiv.org/abs/2502.12110">A-MEM: Agentic Memory for LLM Agents</a></li><li><strong>Mem-α 论文</strong>：<a href="https://arxiv.org/abs/2509.25911">Mem-α: Learning Memory Construction via Reinforcement Learning</a></li><li><strong>Mem0 论文</strong>：<a href="https://arxiv.org/abs/2504.19413">Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</a></li></ol>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>了解和 AI 对话时真正发生了什么（你可能一直理解错了）</title>
      <link>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/</link>
      <guid>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/</guid>
      <pubDate>Wed, 29 Oct 2025 07:34:03 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;你可能也有过这个体验：&lt;/p&gt;
&lt;p&gt;打开豆包、腾讯元宝 或者 ChatGPT，输入一句话，它“立刻开始打字”，像一个反应敏捷的朋友，甚至比人类还懂得如何接话、如何幽默。&lt;/p&gt;
&lt;p&gt;在绝大多数人心里，这是“像一个人一样在聊”。&lt;/p&gt;
&lt;p&gt;但你知道吗？&lt;/p&gt;
&lt;p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<p>你可能也有过这个体验：</p><p>打开豆包、腾讯元宝 或者 ChatGPT，输入一句话，它“立刻开始打字”，像一个反应敏捷的朋友，甚至比人类还懂得如何接话、如何幽默。</p><p>在绝大多数人心里，这是“像一个人一样在聊”。</p><p>但你知道吗？</p><p><strong>技术视角看到的完全不是“对话”，而是一套被精准触发的工业级装配流程。</strong></p><br/><br/><h2 id="你看到的是“对话感”，系统看到的是“请求处理管线”"><a href="#你看到的是“对话感”，系统看到的是“请求处理管线”" class="headerlink" title="你看到的是“对话感”，系统看到的是“请求处理管线”"></a>你看到的是“对话感”，系统看到的是“请求处理管线”</h2><p>我们以最常见的一句自然问话为例：</p><p>“可以帮我整理成一份可以发给团队的正式总结吗？”</p><p>你觉得它马上开始“思考并组织语言”，但实际上</p><p><strong>它做的是一串极其严格、完全程序化的流程：</strong></p><p>① 接收到请求 → ② 安全扫描 → ③ 构建上下文窗口 → ④ 文本切割成 Token → ⑤ 开始预测第一个 Token ⑥ 一边预测一边实时往回流（你看到的“正在输入…”）→ ⑦ （如需要）中途调用某个外部工具 → 再继续生成</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/29/steps.png"></p><p>这几乎和流水线制造一台 iPhone 没本质区别。</p><p>任何一个环节没有被触发，它都不会“自动领会你的真实意图”。</p><p>它不是在“理解你”，它是在“执行一个极快的数据处理流程”。</p><br/><br/><h2 id="为什么你以为“它在认真和你聊天”？"><a href="#为什么你以为“它在认真和你聊天”？" class="headerlink" title="为什么你以为“它在认真和你聊天”？"></a>为什么你以为“它在认真和你聊天”？</h2><p>因为它的表现被刻意设计得“像人”。</p><ul><li>输出是“打字式流式返回”，不是一次生成后才展示</li><li>回复语气刻意模拟“人类”而非“系统提示”</li><li>会主动说“我理解你的需求是…”来暗示“理解”</li><li>会“接上语境”，强化“它在听你说话”的错觉</li></ul><p><strong>而人类大脑极容易把“模式匹配 + 语言流畅”误认成“智能 + 意识 + 交流”。</strong></p><p>那种“我被理解了”的错觉，是设计出来的体验层目的，不是它的本质。</p><br/><br/><h2 id="真正的误解在这里"><a href="#真正的误解在这里" class="headerlink" title="真正的误解在这里"></a>真正的误解在这里</h2><p>我们误以为它像人在持续听你说话，会自然而然记得你之前提过什么。</p><p>但事实是：<strong>绝大多数 AI 并不是“忘记”，而是根本“没有被允许记”。除非你触发系统判定“值得写入的结构化信息”。</strong></p><br/><p>换句话说，它不是健忘，而是在严格执行一套记忆筛选机制：</p><ul><li>你以为它会像人类那样自动回顾上下文？<strong>不会</strong>。</li><li>你以为它“听懂了你的意图”？<strong>它只是暂存成 Token 流，随时会被窗口上限挤掉</strong>。</li><li>你以为你“前面铺垫过”的内容它应该记得？<strong>系统其实是在评估“这件事是否值得被存档”</strong>。</li></ul><p><strong>我们常常痛苦地喊“怎么又忘了前提？”，但错不在它——是我们误以为它会记。</strong></p><p>理解这一点的瞬间，你会突然解释通自己过去 90% 和 AI 沟通挫败的原因。</p><p>不是它“不行”，而是你在“用跟人聊天的方式，调用一个完全不一样的系统架构”。</p><br/><br/><h2 id="但知道真相之后，有什么用？"><a href="#但知道真相之后，有什么用？" class="headerlink" title="但知道真相之后，有什么用？"></a>但知道真相之后，有什么用？</h2><p>非常关键的一点是，</p><p>一旦你知道你不是在“对话”，而是在“调度装配线”，你就可以主动提高效率。</p><p>比如：</p><ul><li><p>想让它优先说结论？</p><p>→ 在开头加一句 “<strong>请先给结论，再解释原因</strong>”</p></li><li><p>想确保它不会“忘掉前提”？</p><p>→ 你不要“自然聊天”，而是<strong>每轮都把任务上下文重新锚定</strong></p></li><li><p>想让它知道“你正在执行一项任务而非闲聊”？</p><p>→ 明确说 “<strong>这是一个多轮协作任务，我们每轮专注一个子步骤。</strong>”</p></li></ul><p>你会突然发现，它不再“随机发挥”，而是开始“听你指挥”。</p><br/><br/><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>对话式 AI 当然正在快速进化。</p><p>你今天看到的是“即时推理的装配线”，但下一阶段的趋势已经非常明确。</p><p><strong>它会逐步具备可训练的、可被赋予“持久记忆结构”的能力。</strong></p><p>这不是“像人一样变聪明”，而是“<strong>像系统一样逐步成为你大脑外部的长期扩展层</strong>”。</p><p>所以，真正的临界点不是“等它变强”，而是 你何时开始以系统设计者的身份与它协作—— 你定义它记什么、它如何进化、它最终成为怎样的“第二工作大脑”。</p><p><strong>当你不再把它当聊天对象，而是当未来基础设施去驯养，你就已经站在下一代人机协作的入口处。</strong></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent Memory 评估测试方案：从指标体系到开源基准</title>
      <link>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/</link>
      <guid>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/</guid>
      <pubDate>Fri, 24 Oct 2025 08:38:29 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;em&gt;给那些正在构建智能体的开发者的一份记忆体检指南&lt;/em&gt;&lt;/p&gt;
&lt;br/&gt;
&lt;br/&gt;


&lt;p&gt;前段时间，我在调试一个几何 Agent。&lt;/p&gt;
&lt;p&gt;这个 Agent 能自动分析几何题、推理定理、调用绘图工具，看上去颇有点“自主学习”的影子。&lt;/p&gt;
&lt;br/</description>
        
      
      
      
      <content:encoded><![CDATA[<p><em>给那些正在构建智能体的开发者的一份记忆体检指南</em></p><br/><br/><p>前段时间，我在调试一个几何 Agent。</p><p>这个 Agent 能自动分析几何题、推理定理、调用绘图工具，看上去颇有点“自主学习”的影子。</p><br/><p>但问题也随之而来——它太健忘了。</p><p>有时候明明刚在上文证明过某个结论，下一步又开始怀疑它自己。</p><p>我给它接上了向量数据库、加了摘要器、甚至写了个小型索引器，但效果依旧不稳定。</p><br/><p>那时候我开始意识到：</p><p><strong>我们都在拼命强化 Agent 的“行动力”，却很少认真测量它的“记忆力”。</strong></p><br/><p>于是我决定系统地研究一下，怎么评估一个 Agent 的 Memory 模块。</p><p>今天这篇文章，写给所有已经或准备构建 Agent 工程的人。</p><p>希望帮你找到一套可落地、可复现的记忆评测方案。</p><br/><br/><h2 id="为什么要测记忆？"><a href="#为什么要测记忆？" class="headerlink" title="为什么要测记忆？"></a>为什么要测记忆？</h2><p>如果说大模型是 Agent 的大脑，那 Memory 就是它的长期神经系统。</p><p>没有记忆，再聪明的模型也只能“现想”而无法“积累”。</p><br/><p>在工程实践里，这会表现为：</p><ul><li>对话几轮后开始失忆</li><li>任务中重复提问</li><li>自相矛盾的人设</li><li>一旦上下文超过 10K，就变成另一位陌生 AI</li></ul><p>在我构建数学 Agent 的过程中，这种现象尤其明显。</p><p>模型在第一轮中记得“点 A 在圆上”，到了第五轮，它却开始假设“点 A 在圆外”。</p><p>一开始我以为问题在于 prompt 太短，后来才发现，真正的原因是<strong>缺乏系统的 Memory 测试</strong>。</p><p>没有量化，就无法优化。</p><p>于是，记忆评测成了我“修 Agent 心智”的重要一步。</p><br/><br/><h2 id="我们到底要测什么？"><a href="#我们到底要测什么？" class="headerlink" title="我们到底要测什么？"></a>我们到底要测什么？</h2><p>衡量 Agent 的记忆，其实可以借鉴人类心理学：</p><p>人类记忆分为短时、长时、情节、语义……</p><p>在工程里，我们可以把测试拆成几个核心问题：</p><ul><li><p><strong>能不能想起来？（Recall）</strong></p><p>例如：“用户之前提到的定理叫什么？”</p><p>对应指标 Recall@K。</p></li><li><p><strong>说得一致吗？（Consistency）</strong></p><p>“你昨天说角 A 等于 60°，今天怎么变 45° 了？”</p></li><li><p><strong>更新正确吗？（Update）</strong></p><p>“如果条件改变，旧记忆是否被覆盖？”</p></li><li><p><strong>能承认不知道吗？（Calibration）</strong></p><p>当没见过的信息出现，模型是否会拒答而不是胡编。</p></li><li><p><strong>记忆会不会膨胀？（Forgetting）</strong></p><p>随着交互增多，Agent 是否会被冗余记忆拖慢。</p></li><li><p><strong>多模态下是否还记得？（Multimodal Memory）</strong></p><p>看过的图、听过的指令，在下次提问时还能匹配回来吗？</p></li></ul><p>这六个维度几乎涵盖了所有常见的 Memory 问题。</p><p>我自己在调试几何 Agent 时，最常暴露的是前两个：</p><p>模型常常能“记得关键词”，却丢失了几何关系；</p><p>它能复述公式，但忘了变量的含义。</p><br/><p>这时候如果没有 Recall 或 Consistency 的量化指标，根本无法判断是哪一环在失灵。</p><br/><br/><h2 id="市面上能用的评测基准"><a href="#市面上能用的评测基准" class="headerlink" title="市面上能用的评测基准"></a>市面上能用的评测基准</h2><p>我查阅了近两年的研究，发现业界其实已经有了几套相当成熟的 Memory 评测基准。</p><p>只是大部分人没注意到它们可以直接上手。</p><br/><p>第一个是 <strong>LoCoMo</strong> —— Snap Research 的长对话评测。</p><p>它用上百轮对话测试模型的“长期记忆力”，</p><p>问题涵盖事实回忆、时间因果、甚至多模态对话。</p><p>在官方结果里，GPT-4 的 F1 分数只有 32，而人类平均是 88。</p><p>也就是说，我们距离真正的长期记忆，还差一整个物种的距离。</p><br/><p>第二个是 <strong>LongMemEval</strong> —— UCLA 团队的系统性基准。</p><p>它更像一个“记忆体检表”，从信息提取、跨会话推理、知识更新到拒答未知，一共五项指标。</p><p>如果你的 Agent 是任务型（比如客服、知识问答），</p><p>LongMemEval 是最值得尝试的那套测试。</p><br/><p>第三类是 <strong>多会话记忆集</strong>，如 Facebook 的 MSC 和百度的 Persona-Long。</p><p>它们主要考察“角色一致性”，模型是否能记住用户和自己的设定。</p><p>这对陪伴型、教育型 Agent 尤其重要。</p><p>这类基准就能帮你精确量化这种人格断层。</p><br/><br/><h2 id="评测指标背后的逻辑"><a href="#评测指标背后的逻辑" class="headerlink" title="评测指标背后的逻辑"></a>评测指标背后的逻辑</h2><p>这些基准的数据再多，也离不开几个核心指标：</p><ul><li><strong>Recall@K</strong> —— 检索召回率。衡量从记忆库中找到正确信息的能力。</li><li><strong>QA Accuracy &#x2F; F1</strong> —— 回答是否正确。最终看的是能否“说对话”。</li><li><strong>Consistency Score</strong> —— 是否前后矛盾。可以人工或自动打分。</li><li><strong>Rejection Rate</strong> —— 面对未知是否拒答，防止“假记忆”。</li><li><strong>ROUGE &#x2F; BLEU &#x2F; FactScore</strong> —— 用于生成式摘要或事件回忆任务。</li></ul><p>在工程里我发现，<strong>指标越细，越能暴露 Memory 模块的真实问题</strong>。</p><p>比如我的几何 Agent 在 Recall 很高时，QA 准确率却低；</p><p>这说明它能检索出正确片段，但推理时用错了。</p><p>有时候优化 Memory，不是改数据库，而是改检索策略。</p><br/><br/><h2 id="如何在自己的工程中测试记忆"><a href="#如何在自己的工程中测试记忆" class="headerlink" title="如何在自己的工程中测试记忆"></a>如何在自己的工程中测试记忆</h2><p>如果你已经有一个可运行的 Agent，不妨这样操作：</p><p><strong>第一步：准备测试集。</strong></p><p>可以直接使用 LoCoMo 或 LongMemEval 的公开数据，也可以用自己系统中的聊天日志，人工标几个“记忆点”。</p><p><strong>第二步：定义 Memory 模块接口。</strong></p><p>无论你用的是 LangChain 的 <code>ConversationBufferMemory</code>，还是自研的像 MemGPT 那样的分页机制，确保你能随时 dump 出它“记住了什么”。</p><p><strong>第三步：运行评测脚本。</strong></p><p>LoCoMo 官方仓库里有 <code>evaluate_qa.py</code>，只需改一下模型接口，就能测你的 Agent 的 QA F1。</p><p>LongMemEval 也有现成脚本，可以测 Recall、拒答率等。</p><p><strong>第四步：观察与反思。</strong></p><p>别只看准确率，更要看错误类型。</p><p>有些错误是检索不到，有些是检索到但没用。</p><p>如果可能，把指标接入 CI 流程，每次更新记忆逻辑时自动跑一轮。</p><p>当你第一次看到自己的 Agent 在 LoCoMo 上得分只有二十几，那种“原来它根本没记住我”的感觉，会比任何 bug 都真实。</p><br/><br/><h2 id="可直接上手的评测-Demo"><a href="#可直接上手的评测-Demo" class="headerlink" title="可直接上手的评测 Demo"></a>可直接上手的评测 Demo</h2><p>如果你不想从零开始，可以直接试这几个项目：</p><ul><li><p><strong>LoCoMo 官方评测工具</strong>：<br><a href="https://github.com/snap-research/locomo">https://github.com/snap-research/locomo</a></p><p>包含超长对话数据和 QA 评测脚本。</p></li><li><p><strong>LongMemEval 基准</strong>：<br><a href="https://github.com/xiaowu0162/LongMemEval">https://github.com/xiaowu0162/LongMemEval</a></p><p>五维记忆测试模板，支持 HuggingFace 模型。</p></li><li><p><strong>MemoryBank &#x2F; SiliconFriend</strong>：<br><a href="https://github.com/zhongwanjun/MemoryBank-SiliconFriend">https://github.com/zhongwanjun/MemoryBank-SiliconFriend</a></p><p>内置遗忘机制，可在本地启动一个有“长期记忆”的中文聊天机器人。</p></li><li><p><strong>Reflexion Agent</strong>：<br><a href="https://github.com/noahshinn/reflexion">https://github.com/noahshinn/reflexion</a></p><p>教你如何让 Agent 自己反思并记录经验。</p></li></ul><p>我个人推荐先跑 LongMemEval。</p><p>它轻量、指标明确、结果易解释。</p><p>如果你的 Agent 通过了它，再挑战 LoCoMo。</p><p>后者是真正的“长对话炼狱”，我第一次跑完时 GPU 都快烤化了。</p><br/><br/><h2 id="评测之外的坑与反思"><a href="#评测之外的坑与反思" class="headerlink" title="评测之外的坑与反思"></a>评测之外的坑与反思</h2><p>我最深的感触是：<strong>记忆问题不是硬件问题，是认知问题。</strong></p><br/><p>我曾花一周时间优化索引算法，却忽略了最根本的逻辑——Agent 根本不知道“什么时候该想起某件事”。</p><p>它拥有庞大的向量库，却缺乏触发机制。</p><p>那一刻我突然明白：真正需要评测的，不仅是“记得多少”，更是“何时想起”。</p><br/><p>另外一个坑是“假记忆”。</p><p>我曾让模型复盘教师一天的教学日志，它开始一本正经地编造“我今天教了学生三角函数”。</p><p>这时拒答率指标就很关键。</p><p>一个 Agent 不该无所不答，<strong>能承认不知道，本身就是记忆成熟的表现。</strong></p><br/><br/><h2 id="未来的方向"><a href="#未来的方向" class="headerlink" title="未来的方向"></a>未来的方向</h2><p>目前的基准都还停留在文本和问答层面。</p><p>未来的评测一定会走向更真实的多模态场景——让 Agent 看图、听音、记住空间位置，甚至在几天后的任务中回忆它曾经画过的图。</p><br/><p>我相信未来的开发工具里，会出现一种新的概念：<strong>MemoryOps</strong>。</p><p>就像今天的 DevOps 管理部署一样，MemoryOps 将监控和测试 Agent 的记忆健康。</p><p>到那时候，我们或许可以做到：“每次模型上线前，都跑一份记忆体检报告。”</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>写这篇文章时，我想到一个比喻：</p><p>训练模型像造大脑，但让它拥有记忆，才是赋予它灵魂。</p><p>所以，在你下一次调试 Agent 时，<br>别只盯着推理精度和响应速度，<br>也问问它一句：</p><blockquote><p>“你还记得我们第一次对话吗？”</p></blockquote><p><strong>没有被测过的记忆，终将变成幻觉。</strong></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent 如何避免记忆漂移：三大策略与工程实践</title>
      <link>https://blog.liluhui.cn/2025/10/17/Preventing-Memory-Drift-in-Agent-Systems-Three-Core-Techniques-and-Engineering-Implementations/</link>
      <guid>https://blog.liluhui.cn/2025/10/17/Preventing-Memory-Drift-in-Agent-Systems-Three-Core-Techniques-and-Engineering-Implementations/</guid>
      <pubDate>Fri, 17 Oct 2025 08:17:39 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h2&gt;&lt;p&gt;长期运行的 Agent 容易出现「记忆漂移」：随着时间推移，其记忆内容被反复重写、压缩、整合，逐渐偏离原始语义，进而</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>长期运行的 Agent 容易出现「记忆漂移」：随着时间推移，其记忆内容被反复重写、压缩、整合，逐渐偏离原始语义，进而导致自相矛盾、逻辑错乱或行为失常。</p><p>本篇文章总结了三种抵御记忆漂移的核心策略：</p><ul><li><p><strong>定期总结</strong>：定时摘要长对话内容，减少信息积压，保持上下文可控</p></li><li><p><strong>批处理蒸馏</strong>：从多轮对话中提炼出用户偏好与抽象知识，提升记忆质量</p></li><li><p><strong>冲突合并</strong>：发现重复或冲突内容，统一更新或标记失效，维持一致性</p></li></ul><p>结合实际系统如 MemGPT、AWS AgentCore、LangChain 等，下文提供了对应的工程实现与开源项目指引，适合构建具备长期稳定记忆的 Agent 架构。</p><br/><br/><h2 id="引言：智能体的「记忆漂移」现象"><a href="#引言：智能体的「记忆漂移」现象" class="headerlink" title="引言：智能体的「记忆漂移」现象"></a>引言：智能体的「记忆漂移」现象</h2><p>在构建具备长期行为一致性与任务连贯性的智能体（Agent）时，「记忆」机制成为不可或缺的一环。</p><p>与静态函数调用或短程上下文不同，Agent 的长期记忆的目标是支持跨轮次对话、持续学习与经验积累。</p><p>然而，随着记忆体量增长，「记忆漂移」问题逐渐显现：<strong>原始信息在多次重写、压缩、提取过程中偏离原意，导致 Agent 出现自相矛盾、事实错乱或行为不一致等现象</strong>。</p><p>这一问题本质上源自智能体在长期运行中所面临的信息冗余、表达歧义与语义演化挑战。</p><p>就像我们日常使用电脑，当桌面文件堆积如山、不及时分类整理，搜索效率降低、重复文件泛滥，最终反过来干扰工作。</p><br/><p>本文将深入探讨三种关键的抗记忆漂移策略：定期总结、批处理蒸馏、冲突合并。</p><p>结合最新的开源实践与研究进展，为构建稳定可靠的 Agent Memory 系统提供参考路径。</p><br/><br/><h2 id="策略一：定期总结-——-让记忆不过载"><a href="#策略一：定期总结-——-让记忆不过载" class="headerlink" title="策略一：定期总结 —— 让记忆不过载"></a>策略一：定期总结 —— 让记忆不过载</h2><p>如果你曾被对话窗口滚动到看不见开头、或者某个 Agent 总是健忘你提过的偏好，那大概率它缺了一套靠谱的定期总结机制。</p><p><strong>定期总结（Periodic Summarization）</strong> 是让智能体隔一段时间就回头看看：我刚刚都经历了什么？有哪些是值得留下的？</p><p>就像人写日记，不可能每句话都记，而是总结当日要点。</p><br/><p>工程实现示例：</p><ul><li><strong>LangChain SummaryBufferMemory</strong>：在上下文快要塞不下之前，自动将早期内容打包为摘要，留出空间给后续对话。</li><li><strong>MemGPT 滚动摘要</strong>：采用分层内存结构，短期记忆用完就「递归摘要」进中期记忆，就像桌面上的临时文件转存进资料夹。</li></ul><br/><p>一些提醒：</p><ul><li>连续多次摘要容易信息腐蚀，逐层失真。建议保留关键原句作为锚点。</li><li>可引入 pin memory 机制，确保核心事实始终原样保留。</li></ul><br/><br/><h2 id="策略二：批处理蒸馏-——-从日志中提炼稳定认知"><a href="#策略二：批处理蒸馏-——-从日志中提炼稳定认知" class="headerlink" title="策略二：批处理蒸馏 —— 从日志中提炼稳定认知"></a>策略二：批处理蒸馏 —— 从日志中提炼稳定认知</h2><p>有没有遇到过这种情况：你和 AI 聊了十几次后，TA还是不了解你？</p><p>这时，问题可能不在对话内容，而在于没有把那些碎片化记忆凝练为稳定认知。</p><p><strong>批处理蒸馏（Batched Distillation）</strong> 就像是回顾一整个项目后，总结出规律、套路、偏好，存档到知识库中。</p><br/><p>工程实现示例</p><ul><li><strong>Generative Agents（Stanford）</strong>：每个模拟人每天睡觉前会进行反思，总结一天所学所感。</li><li><strong>AWS AgentCore Memory</strong>：让新事件与历史记忆对话，LLM负责融合成一条高层认知。</li><li><strong>Letta MemGPT 的睡眠代理</strong>：利用主线程空闲时间悄悄整理旧资料，像我们周末清理相册一样。</li></ul><br/><p>提示模版示例</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请阅读以下 N 段日志，总结出关于用户的稳定特征、目标与情绪倾向。</span><br></pre></td></tr></table></figure><br/><br/><h2 id="策略三：冲突合并-——-去重与知识更新"><a href="#策略三：冲突合并-——-去重与知识更新" class="headerlink" title="策略三：冲突合并 —— 去重与知识更新"></a>策略三：冲突合并 —— 去重与知识更新</h2><p>Agent 说你喜欢喝美式，过一会儿又说你最讨厌咖啡？这不是它耍你，而是记忆没合并好的情况。</p><p>长期运行的智能体一定会遇到记忆冲突：用户今天说“我吃素”，下个月说“这家烧烤真香”。</p><br/><p><strong>不消解这些冲突，Agent 就会陷入人格分裂式输出。</strong></p><p>合并策略</p><ul><li><strong>语义合并</strong>：让 LLM判断两个说法是重复、矛盾还是递进。</li><li><strong>失效机制</strong>：标记旧记忆为「历史版本」，不参与默认推理。</li><li><strong>仲裁代理</strong>：在多智能体系统中，用专职 Agent 仲裁歧义事实。</li></ul><br/><p>工程落地示例</p><ul><li><strong>AWS AgentCore Memory</strong>：每条新记忆自动触发冲突检测与合并逻辑。</li><li><strong>Memori Memory Engine</strong>：具备版本控制与自动协调机制，适合长生命周期记忆管理。</li></ul><br/><br/><h2 id="拿来即用，策略对比与实践参考"><a href="#拿来即用，策略对比与实践参考" class="headerlink" title="拿来即用，策略对比与实践参考"></a>拿来即用，策略对比与实践参考</h2><table><thead><tr><th>策略名称</th><th>作用重点</th><th>典型触发时机</th><th>工程实现代表</th><th>风险点</th></tr></thead><tbody><tr><td>定期总结</td><td>控制体积，保存语境</td><td>消息超过阈值</td><td>LangChain、MemGPT</td><td>摘要失真</td></tr><tr><td>批量蒸馏</td><td>抽象提炼，积累知识</td><td>阶段归档或反思时刻</td><td>AWS AgentCore、Generative Agents</td><td>抽象过度</td></tr><tr><td>冲突合并</td><td>保持一致性，去重纠错</td><td>新旧冲突产生时</td><td>Memori、Letta、仲裁代理机制</td><td>决策不透明</td></tr></tbody></table><p><strong>组合建议</strong>：构建分层内存结构 + 批次反思 + 定期摘要 + 合并管道，能形成防漂移的“记忆治理闭环”。</p><br/><p>可用工具与开源实现一览：</p><table><thead><tr><th>名称</th><th>类型</th><th>地址</th><th>支持策略</th></tr></thead><tbody><tr><td>LangChain SummaryMemory</td><td>工具库</td><td><a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a></td><td>定期总结</td></tr><tr><td>MemGPT &#x2F; Letta</td><td>代理框架</td><td><a href="https://github.com/letta-ai/letta">https://github.com/letta-ai/letta</a></td><td>三者兼具</td></tr><tr><td>AgentCore Memory</td><td>云平台模块</td><td><a href="https://aws.amazon.com/bedrock/agents">https://aws.amazon.com/bedrock/agents</a></td><td>蒸馏 + 合并</td></tr><tr><td>Memori</td><td>记忆服务</td><td><a href="https://github.com/GibsonAI/memori">https://github.com/GibsonAI/memori</a></td><td>冲突合并</td></tr><tr><td>generative_agents</td><td>模拟研究框架</td><td><a href="https://github.com/joonspk-research/generative_agents">https://github.com/joonspk-research/generative_agents</a></td><td>批量蒸馏</td></tr></tbody></table><br/><br/><h2 id="写在最后：从记忆治理到认知形成"><a href="#写在最后：从记忆治理到认知形成" class="headerlink" title="写在最后：从记忆治理到认知形成"></a>写在最后：从记忆治理到认知形成</h2><p>智能体的记忆，终究不是日志数据库，而是它认知世界、理解人类、规划行动的基础。过去我们关注的是它记不记得；而现在，我们必须关心它记得对不对、准不准、合不合理。</p><p>三类策略分别作用于记忆系统的不同阶段：<strong>定期总结</strong> 帮它活在有限当下，<strong>批量蒸馏</strong> 帮它形成抽象理解，<strong>冲突合并</strong> 让它在真伪之间保持清醒。</p><p>三者结合，才可能走向真正可持续的 Agent 思维。</p><br/><p>我的观点是，未来 Agent 开发，不该再把 Memory 当附属功能，而应当作为核心设计范式来思考。</p><p>就像我们在产品中设计信息架构一样，我们也应该设计 Agent 的认知架构。</p><p>构建一个能学、能忘、能修的 Agent，不只是技术挑战，更是认知工程。</p><p>根基不稳，智能难远。</p><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/17/Preventing-Memory-Drift-in-Agent-Systems-Three-Core-Techniques-and-Engineering-Implementations/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>什么是多 Agent 系统？从单体模型到协作智能的进化</title>
      <link>https://blog.liluhui.cn/2025/10/15/What-Is-a-Multi-Agent-System-From-a-Single-LLM-to-Collaborative-Intelligence/</link>
      <guid>https://blog.liluhui.cn/2025/10/15/What-Is-a-Multi-Agent-System-From-a-Single-LLM-to-Collaborative-Intelligence/</guid>
      <pubDate>Wed, 15 Oct 2025 08:11:28 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;过去一年，我们见证了无数“大模型”带来的奇迹。&lt;br&gt;它们能写代码、能画图、能写策划书，甚至能帮你找 bug。  &lt;/p&gt;
&lt;p&gt;于是一个问题出现了：  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;既然大模型已经这么强了，&lt;strong&gt;为什么还要搞那么复杂的「多 Agent</description>
        
      
      
      
      <content:encoded><![CDATA[<p>过去一年，我们见证了无数“大模型”带来的奇迹。<br>它们能写代码、能画图、能写策划书，甚至能帮你找 bug。  </p><p>于是一个问题出现了：  </p><blockquote><p>既然大模型已经这么强了，<strong>为什么还要搞那么复杂的「多 Agent 系统」</strong>？ </p><p>是不是就是多开几个大模型？  </p></blockquote><p>其实，多 Agent 不是“堆模型”，而是一次<strong>智能结构的转变</strong>。  </p><p>它让我们开始用「团队」的方式去理解 AI，让语言模型从“单人作坊”变成“协作组织”。  </p><br/><br/><h2 id="一个-Agent-不够用吗？"><a href="#一个-Agent-不够用吗？" class="headerlink" title="一个 Agent 不够用吗？"></a>一个 Agent 不够用吗？</h2><p>想象你让 GPT 帮你开发一个网站：它能写前端，也能设计数据库，还能生成测试代码。 </p><p>但很快你会发现一个问题，它在写完后<strong>自测永远都通过</strong>，逻辑错误自己也检查不出来。  </p><br/><p>于是人们开始尝试：<br>让一个 GPT 写，另一个 GPT 审；<br>让一个负责规划，另一个负责执行；<br>甚至让两个 GPT 吵一架，看谁的结论更靠谱。  </p><p>这就是最早的 “<strong>multi-agent</strong>” 思想萌芽。<br>不是因为模型不行，而是因为<strong>智能需要结构</strong>。<br>单个 Agent 再强，也无法同时兼顾规划、执行、评估、反思这些不同心智功能。  </p><br/><br/><h2 id="为什么要多个-Agent：从「个体智能」到「集体智能」"><a href="#为什么要多个-Agent：从「个体智能」到「集体智能」" class="headerlink" title="为什么要多个 Agent：从「个体智能」到「集体智能」"></a>为什么要多个 Agent：从「个体智能」到「集体智能」</h2><h3 id="1️⃣-分工让思考更高效"><a href="#1️⃣-分工让思考更高效" class="headerlink" title="1️⃣ 分工让思考更高效"></a>1️⃣ 分工让思考更高效</h3><p>就像人类社会的分工一样，AI 也有“认知负荷”。<br>一个 Agent 的上下文再长，也无法同时规划和执行。  </p><br/><p>于是多 Agent 系统会把任务拆解：  </p><ul><li><strong>Planner</strong> 规划任务  </li><li><strong>Executor</strong> 调用工具或编写代码  </li><li><strong>Critic</strong> 检查结果、给出反馈</li></ul><br/><p>论文参考：</p><ul><li>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework (Hong et al., 2023)  </li><li>Reflexion: Verbal Reinforcement Learning with LLMs (Shinn et al., 2023)</li></ul><h3 id="2️⃣-不同视角，减少偏差"><a href="#2️⃣-不同视角，减少偏差" class="headerlink" title="2️⃣ 不同视角，减少偏差"></a>2️⃣ 不同视角，减少偏差</h3><p>当多个 Agent 相互讨论、辩论、复核时，错误就更难溜走。<br>这就是所谓的 “<strong>social critique</strong>” —— AI 用社会互动的方式，来弥补单模型思维的盲点。 </p><br/><p>例如 <em>CAMEL (2023)</em> 中，两个 LLM 扮演不同角色，通过角色扮演协作完成任务，结果准确率显著高于单模型执行。  </p><h3 id="3️⃣-群体记忆与长期任务"><a href="#3️⃣-群体记忆与长期任务" class="headerlink" title="3️⃣ 群体记忆与长期任务"></a>3️⃣ 群体记忆与长期任务</h3><p>一个模型记不住太久的上下文，而多 Agent 系统可以共享外部记忆。  </p><p><strong>LangGraph</strong> 就用图状结构记录各 Agent 的状态与结果，<br><strong>AutoGen</strong> 则通过消息路由实现多轮协作。  </p><p>这让智能系统第一次能像组织一样 <strong>有状态、有记忆、有分工</strong>。  </p><br/><br/><h2 id="「Agent」vs-「Multi-Agents」：什么时候该用哪种？"><a href="#「Agent」vs-「Multi-Agents」：什么时候该用哪种？" class="headerlink" title="「Agent」vs 「Multi Agents」：什么时候该用哪种？"></a>「Agent」vs 「Multi Agents」：什么时候该用哪种？</h2><p>不是所有任务都需要多 Agent，整理了下面这张表，帮你快速判断：  </p><table><thead><tr><th>场景类型</th><th>适合单 Agent</th><th>适合多 Agent</th></tr></thead><tbody><tr><td>简单问答、摘要、翻译</td><td>✅</td><td>❌</td></tr><tr><td>多步骤推理（数学、逻辑）</td><td>⚠️</td><td>✅</td></tr><tr><td>代码生成与测试</td><td>❌</td><td>✅</td></tr><tr><td>长期运营任务（游戏、规划）</td><td>❌</td><td>✅</td></tr><tr><td>决策 &#x2F; 研究讨论类任务</td><td>⚠️</td><td>✅</td></tr></tbody></table><p>👉 简单说：  </p><ul><li><strong>单 Agent</strong>：快、轻、直接。  </li><li><strong>多 Agent</strong>：稳、复杂、可解释。</li></ul><p>它更像是“团队思维”，适用于有阶段、有反馈、有评估的任务。  </p><br/><br/><h2 id="协作机制不是「堆模型」，而是「编排心智」"><a href="#协作机制不是「堆模型」，而是「编排心智」" class="headerlink" title="协作机制不是「堆模型」，而是「编排心智」"></a>协作机制不是「堆模型」，而是「编排心智」</h2><p>多 Agent 的价值在于协作机制，而不是数量。  </p><p>常见机制包括：  </p><ol><li><strong>角色定义</strong>：每个 Agent 有独立 Prompt 与目标（Planner &#x2F; Coder &#x2F; Tester）  </li><li><strong>通信协议</strong>：消息路由、对话规则（AutoGen 异步消息机制）  </li><li><strong>状态共享</strong>：外部记忆、数据库或上下文图（LangGraph）  </li><li><strong>自我审查</strong>：互评或辩论结构（CAMEL、Reflexion）  </li><li><strong>任务编排</strong>：从自然语言生成完整工作流（MetaGPT、CrewAI）</li></ol><p>工程参考：</p><ul><li>Microsoft AutoGen</li><li>LangChain LangGraph  </li><li>MetaGPT  </li><li>CAMEL-AI  </li><li>CrewAI</li></ul><br/><br/><h2 id="争议：多-Agent-是过渡，还是未来？"><a href="#争议：多-Agent-是过渡，还是未来？" class="headerlink" title="争议：多 Agent 是过渡，还是未来？"></a>争议：多 Agent 是过渡，还是未来？</h2><p>目前学术界有两种声音：  </p><p>1️⃣ <strong>单体强化派</strong><br>认为随着大模型上下文与记忆增强（如 GPT-o3、Gemini 2.5），<br>未来单模型内部就能模拟多角色思维，不再需要外部 Agent。  </p><p>2️⃣ <strong>协作网络派</strong><br>认为未来智能系统会像社会那样，通过标准协议（如 MCP、A2A）让不同 Agent 协作，<br>形成一个“分布式智能生态”。  </p><p>无论是哪种，multi-agent 都是一个重要的中间态：它让 AI 学会如何组织智能、引导协作，为真正的“社会化 AI”打下基础。  </p><br/><br/><h2 id="结语：AI，也需要同事"><a href="#结语：AI，也需要同事" class="headerlink" title="结语：AI，也需要同事"></a>结语：AI，也需要同事</h2><p>多 Agent 的出现，不是因为模型不够强，而是因为<strong>智能本身就不是单线程的</strong>。  </p><p>人类的思考依靠分工协作——感知、记忆、计划、执行、反思。  </p><p>AI 也在重走这条路，只不过是以 Agent 的形式。  </p><br/><p>当我们不再把智能看作一个「模型」，  </p><p>而是一群「协作的心智」时，  </p><p>也许我们就更接近真正的智能社会。  </p><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><p><strong>学术论文：</strong>  </p><ul><li><a href="https://arxiv.org/abs/2303.17760">Li et al. (2023) — <em>CAMEL: Communicative Agents for “Mind” Exploration of LLM Society</em></a></li><li><a href="https://arxiv.org/abs/2308.00352">Hong et al. (2023) — <em>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</em></a></li><li><a href="https://arxiv.org/html/2303.11366">Shinn et al. (2023) — <em>Reflexion: Verbal Reinforcement Learning for LLMs</em></a></li><li><a href="https://arxiv.org/abs/2304.03442">Park et al. (2023, Stanford) — <em>Generative Agents: Interactive Simulacra of Human Behavior</em></a></li><li><a href="https://arxiv.org/abs/2503.13657">Cemri et al. (2024) — <em>MAST: Multi-Agent System Taxonomy of Failures</em></a></li><li><a href="https://arxiv.org/abs/2503.05473">Mamie et al. (2025) - <em>The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms</em></a></li></ul><p><strong>开源工程：</strong>  </p><ul><li><a href="https://github.com/microsoft/autogen">Microsoft AutoGen</a>  </li><li><a href="https://github.com/langchain-ai/langgraph">LangChain LangGraph</a>  </li><li><a href="https://github.com/FoundationAgents/MetaGPT">MetaGPT</a>  </li><li><a href="https://github.com/camel-ai/camel">CAMEL-AI</a>  </li><li><a href="https://github.com/crewAIInc/crewAI">CrewAI</a>  </li><li><a href="https://github.com/openai/swarm">OpenAI Swarm</a> (实验性)</li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/15/What-Is-a-Multi-Agent-System-From-a-Single-LLM-to-Collaborative-Intelligence/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>什么是 Agent Memory？探索 AI 如何理解并记住你的话</title>
      <link>https://blog.liluhui.cn/2025/10/10/Understanding_Agent_Memory_The_Mechanisms_Behind_AI%E2%80%99s_Ability_to_Remember/</link>
      <guid>https://blog.liluhui.cn/2025/10/10/Understanding_Agent_Memory_The_Mechanisms_Behind_AI%E2%80%99s_Ability_to_Remember/</guid>
      <pubDate>Fri, 10 Oct 2025 08:18:47 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;记忆并非理所当然，理解-AI-时代的记忆&quot;&gt;&lt;a href=&quot;#记忆并非理所当然，理解-AI-时代的记忆&quot; class=&quot;headerlink&quot; title=&quot;记忆并非理所当然，理解 AI 时代的记忆&quot;&gt;&lt;/a&gt;记忆并非理所当然，理解 AI 时代的记忆&lt;/h2&gt;&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="记忆并非理所当然，理解-AI-时代的记忆"><a href="#记忆并非理所当然，理解-AI-时代的记忆" class="headerlink" title="记忆并非理所当然，理解 AI 时代的记忆"></a>记忆并非理所当然，理解 AI 时代的记忆</h2><p>记忆是我们日常生活中不可或缺的一部分。<br>从小到大，我们积累的经验和知识形成了对世界的认知框架。<br>人类的记忆帮助我们在面对未知时作出反应，记住重要的事情，避免重复犯错。<br>但是，<strong>如果我们只将 记忆 理解为人类自然存在的生理过程，那么在面对人工智能（AI）时代的 Agent Memory 时，可能会产生误解</strong>。</p><p>在 AI 的世界里，记忆并不像人类那样自然而然地存在。<br>这也是为什么我们需要从一个全新的角度，去理解和探索 Agent Memory —— 它不仅仅是记住过去的互动，而是让机器能够像人类一样，通过记忆优化每次决策的能力。</p><p>通过这篇文章，我将带你走进 Agent Memory 的世界，解答这个关键问题： <strong>Agent Memory 是什么？为什么它对智能代理至关重要？</strong></p><br/><br/><h2 id="什么是-Agent-Memory？"><a href="#什么是-Agent-Memory？" class="headerlink" title="什么是 Agent Memory？"></a>什么是 Agent Memory？</h2><p><strong>Agent Memory 是人工智能代理（AI Agent）中的“记忆系统”，它是人类设计的存储、更新&#x2F;遗忘和检索机制。</strong><br>它使得 AI 能够在与用户互动时，不仅记住和利用历史信息，还能根据任务的需要进行更新、遗忘和优化，以提供更加智能、个性化的服务。</p><p><strong>可以将 Agent Memory 理解为一套“存储-更新-检索”的策略。</strong><br>不同的实现路径和技术方案使得这一系统的具体应用形式多种多样。无论是短期记忆、长期记忆，还是对信息的筛选和检索方式，它都帮助 AI 在每次与用户的互动中，保持一致性、理解上下文，并从过去的经验中不断学习和调整。</p><p>举个例子，当你与智能语音助手对话时，它不仅记得你昨天询问过身体不舒服的症状，还会根据历史信息优化后续的建议。<strong>这不仅是记忆的简单存储，而是通过精心设计的策略，让 Agent Memory 具备了学习和适应的能力</strong>。</p><p>需要特别注意的是，在 Agent Memory 的设计中，<strong>上下文 和 记忆 是两个不同的概念</strong>。</p><ul><li><strong>上下文</strong>: 只存在于当前的交互中，它帮助 AI 理解当前对话或任务的背景。例如，用户正在询问今天的天气，AI 就会利用这个即时上下文来生成回答，但当对话结束时，这些上下文信息将不再保存。</li><li><strong>记忆</strong>：则是长期存储的信息，能够跨会话、跨任务持续保持，并帮助 AI 在未来的交互中做出更加个性化和高效的响应。</li></ul><p>这与人类的思维过程相似：我们在对话中可能暂时记住某些信息（上下文），但“记忆”则是指我们从过去的经历中提取经验，以便做出更明智的决策，而不仅仅局限于当前的对话。</p><br/><br/><h2 id="Agent-Memory-的类型"><a href="#Agent-Memory-的类型" class="headerlink" title="Agent Memory 的类型"></a>Agent Memory 的类型</h2><p>Agent Memory 并不是单一的记忆体，而是由多个层次和类型组成，旨在应对不同的应用场景。<br>这些记忆类型就像人类不同的记忆系统，分别负责即时的工作任务和长期的知识储备。<br>以下是 <strong>Agent Memory 的五种基本类型</strong>，详细区别可看同系列文章<a href="https://blog.liluhui.cn/2025/09/19/Agent-Memory-Five-Layer-Model-and-Engineering-Implementation/">《Agent 记忆五层模型与工程落地》</a>：</p><ul><li><strong>工作记忆（Working Memory）</strong><br> 工作记忆负责存储和处理当前任务或对话中的即时信息。它类似于人类的短期记忆，帮助 AI 在与用户的互动中保持上下文，处理即时的需求。工作记忆通常在会话结束后被清除，以便为新的任务腾出空间。</li><li><strong>情景记忆（Episodic Memory）</strong><br> 情景记忆负责记录和存储与特定事件相关的信息。这类记忆帮助 AI 记住特定的事件、用户的行为和相关的交互情境，从而提供更具上下文关联性的反馈。</li><li><strong>语义记忆（Semantic Memory）</strong><br> 语义记忆包含了广泛的通用知识和事实，例如世界常识、数学公式、日期等。这类记忆不依赖于个体的经历，而是基于普遍接受的事实和信息。</li><li><strong>程序性记忆（Procedural Memory）</strong><br> 程序性记忆用于存储如何执行特定任务或操作的知识。这些知识是 Agent 在反复执行某项技能时积累的，类似于人类如何记住特定技能的操作步骤。</li><li><strong>外部持久记忆（External Persistent Memory）</strong><br> 外部持久记忆负责记录并归档用户的长期历史和行为数据。这类记忆帮助 AI 在多个会话、任务和平台之间保持一致性，便于长期个性化和精准反馈。</li></ul><br/><br/><h2 id="Agent-Memory-如何工作？"><a href="#Agent-Memory-如何工作？" class="headerlink" title="Agent Memory 如何工作？"></a>Agent Memory 如何工作？</h2><p><strong>Agent Memory 的核心工作流是存储、更新和检索信息。</strong><br>AI 通过以下几个步骤有效管理记忆，使得每次与用户的互动都更加精准和高效：</p><ul><li><strong>信息存储</strong><br> AI 会根据一定的策略存储重要的交互信息，这些信息可能来自用户输入的对话内容、历史数据或任务需求。为了实现高效存储，信息通常通过数据库或嵌入式向量存储（如 FAISS）来实现。</li><li><strong>信息更新</strong><br> 信息并非一成不变，随着新的交互发生，AI 会更新其记忆。例如，用户的偏好、常见问题或新的任务需求都可以被记录下来，并反映在长期记忆中。信息的更新使得 AI 能够不断优化其响应。</li><li><strong>信息检索</strong><br> AI 必须能够根据当前需求，迅速从记忆中提取相关信息。这一过程类似于检索，通过技术手段（如向量数据库和嵌入检索），AI 可以从大量存储的数据中找到最匹配的内容，从而提供更加智能和精准的回应。</li></ul><br/><br/><h2 id="Agent-Memory-在实际应用中的作用"><a href="#Agent-Memory-在实际应用中的作用" class="headerlink" title="Agent Memory 在实际应用中的作用"></a>Agent Memory 在实际应用中的作用</h2><ol><li><strong>个性化体验</strong><br>Agent Memory 使得 AI 能够记住并理解用户的偏好和需求，从而提供更加个性化的服务。例如，智能推荐系统能根据用户的历史行为推荐商品或服务。</li><li><strong>多轮对话与一致性</strong><br> 在多轮对话中，Agent Memory 使得 AI 能够保持对话的上下文和连贯性，不会重复提问或对话断裂。AI 可以更好地理解用户的意图，从而提供更智能的回应。</li><li><strong>提高效率</strong><br> 通过有效的记忆管理，AI 能够减少用户重复输入信息的需求，提升交互效率。例如，智能助手记住用户的常见问题，能快速提供相关回答，而无需每次重新输入。</li></ol><br/><br/><h2 id="Agent-Memory-的技术实现"><a href="#Agent-Memory-的技术实现" class="headerlink" title="Agent Memory 的技术实现"></a>Agent Memory 的技术实现</h2><p><strong>Agent Memory 的实现依赖于多种技术策略和组件，目的是让 AI 代理在与用户的交互中保持智能化的记忆能力。</strong><br>它的核心机制包括 存储、更新、遗忘和检索，并通过不断优化这些环节来提供个性化和高效的服务。关键技术要素包括：</p><ol><li><p><strong>存储机制</strong>，将用户交互转化为长期可访问的格式：</p><ul><li>向量数据库：如 <strong>FAISS</strong>、<strong>Pinecone</strong>，通过 <strong>embedding</strong> 向量存储，支持高效相似度检索。</li><li>知识库：存储通用知识，如语法、历史事实等，增强记忆能力。</li></ul></li><li><p><strong>信息更新与遗忘策略</strong>，根据新的交互进行动态更新和优化：</p><ul><li>增量更新：将新信息有针对性地加入，而非覆盖已有记忆。</li><li>遗忘机制：删除过时或无用数据，避免记忆膨胀。</li><li>信息压缩：整理和压缩冗余信息，提高存储效率。</li></ul></li><li><p><strong>信息检索机制</strong>，高效检索存储数据：</p><ul><li>向量检索：通过 <strong>FAISS</strong> 等工具，根据相似度快速检索相关信息。</li><li>查询优化：结合 <strong>contextual embeddings</strong> 精确理解用户意图，提升检索精度。</li></ul></li><li><p><strong>多模态信息整合</strong>，结合图像、语音、文本等多种数据形式，提升 AI 的综合响应能力。</p><ul><li>例如，<strong>多模态图像识别系统</strong>，同时处理文本与图像信息，提供精准反馈。</li></ul></li><li><p><strong>外部持久记忆与分布式存储</strong><br>随着数据量的增加，部分记忆会存储到外部系统（如云数据库），保证跨设备、跨平台的持续更新与访问。</p></li></ol><br/><br/><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过合理的 Agent Memory 管理，人工智能可以从单纯的工具进化为智能的交互伙伴，能够理解用户需求、提供个性化的反馈并实现更高效的互动。<br>未来，随着技术的不断进步，Agent Memory 将会在个性化服务、智能化交互以及多代理协作中发挥越来越重要的作用。<br>毕竟，<strong>让“智能”真的成为智能是需要经验积累的，那个“经验”正是在记忆之上的思考。</strong><br>当然现在 Agent Memory 在提升智能代理的能力和用户体验方面仍然面临一些挑战：比如记忆膨胀问题。随着时间推移，AI 可能会积累大量信息，这可能导致记忆库膨胀，影响性能。</p><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><a href="https://arxiv.org/abs/2504.19413">Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</a></li><li><a href="https://arxiv.org/abs/2502.12110">A-MEM: Agentic Memory for LLM Agents</a></li><li><a href="https://www.researchgate.net/publication/388144017_Memory_Architectures_in_Long-Term_AI_Agents_Beyond_Simple_State_Representation">Memory Architectures in Long-Term AI Agents</a></li><li><a href="https://mem0.ai/blog/memory-in-agents-what-why-and-how">AI Agent Memory: What, Why and How It Works</a></li><li><a href="https://www.ibm.com/think/topics/ai-agent-memory">What Is AI Agent Memory? | IBM</a></li><li><a href="https://medium.com/@nirdiamant21/building-an-ai-agent-with-memory-and-adaptability-cdbc428bc36c">Building an AI Agent with Memory and Adaptability</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/10/Understanding_Agent_Memory_The_Mechanisms_Behind_AI%E2%80%99s_Ability_to_Remember/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent 记忆检索策略：怎样学会想起？</title>
      <link>https://blog.liluhui.cn/2025/10/08/Memory-as-Thinking-How-Agents-Retrieve-What-Matters/</link>
      <guid>https://blog.liluhui.cn/2025/10/08/Memory-as-Thinking-How-Agents-Retrieve-What-Matters/</guid>
      <pubDate>Wed, 08 Oct 2025 09:32:32 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;LLM Agent 的记忆系统关键不在“能记多少”，而在“能否精准想起”。&lt;/strong&gt;&lt;br&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p><strong>LLM Agent 的记忆系统关键不在“能记多少”，而在“能否精准想起”。</strong><br>本文总结了 <strong>三类触发机制</strong> 与 <strong>六种检索策略</strong>，构成了一个从“何时想起”到“如何检索”的完整框架：</p><p>🧠 触发机制（When to Recall）</p><ol><li>规则触发 —— 窗口容量不足时检索历史</li><li>反思触发 —— 模型自觉遗忘时主动检索</li><li>事件触发 —— 特定情境或失败日志触发回忆</li></ol><p>🔍 检索策略（How to Retrieve）</p><ol><li>语义相似检索</li><li>混合检索</li><li>图式检索</li><li>元数据过滤</li><li>重排</li><li>反思式检索</li></ol><p>这些策略共同构成了 RAG（Retrieval-Augmented Generation） 的多种实现形态，从简单的语义召回到带有反思与路由能力的自适应检索。<br>记忆不是仓库，而是过滤器——检索策略定义了 Agent 的注意力边界，也塑造了它的“思考深度”。</p><br/><br/><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>LLM Agent 记忆系统的挑战不在“能存多少”，而在“<strong>能否精准想起</strong>”。<br>AutoGPT 曾经尝试将所有任务摘要向量化存入 Pinecone，但检索常常召回“语义相似但任务无关”的记录。<br>Voyager 在 Minecraft 中的“技能库”检索则恰好相反：每次仅召回可执行的技能模块，显著提升学习效率。</p><br/><p><strong>检索的关键不是容量，而是相关性。</strong><br><br/></p><p> 一个好的记忆检索系统，应该能回答三件事：</p><ol><li>什么时候需要想起？</li><li>想起哪些内容？</li><li>想起后如何用？</li></ol><br/><br/><h2 id="触发机制：Agent-何时“想起”"><a href="#触发机制：Agent-何时“想起”" class="headerlink" title="触发机制：Agent 何时“想起”"></a>触发机制：Agent 何时“想起”</h2><p>一个有记忆的 Agent，并不会在每次对话都去翻遍自己的“记忆仓库”。<br>真正聪明的系统，知道<strong>什么时候该想起</strong>。<br>触发检索的时机，其实构成了 Agent 的“意识边界”。<br>我们可以把它想象成人类的三种“回忆瞬间”：</p><h3 id="1-规则触发：因为“容量不够”"><a href="#1-规则触发：因为“容量不够”" class="headerlink" title="1. 规则触发：因为“容量不够”"></a>1. 规则触发：因为“容量不够”</h3><p>最常见的情况，是模型的上下文窗口装不下了。<br>当对话历史变长、任务链变复杂时，旧的信息被挤出窗口，模型开始遗忘。<br>于是我们让 Agent 在这种情况下主动去检索——像是人类“翻笔记”的瞬间。<br>在工程上，这通常由一个简单的规则触发：</p><blockquote><p>当上下文长度接近阈值（比如 80% 的窗口容量）时，调用检索模块，把最相关的历史片段重新召回。</p></blockquote><p>这是一种<strong>节制型的记忆唤醒</strong>。<br>它让 Agent 在不增加计算负担的前提下，维持对过去的最小感知。</p><br/><h3 id="2-反思触发：当-Agent-自觉“忘了什么”"><a href="#2-反思触发：当-Agent-自觉“忘了什么”" class="headerlink" title="2. 反思触发：当 Agent 自觉“忘了什么”"></a>2. 反思触发：当 Agent 自觉“忘了什么”</h3><p>更有意思的情况是 Agent 自己意识到记忆的缺口。<br>比如模型在生成中评估到不确定性上升、连续几步推理逻辑断裂，或自己产生了矛盾。<br>这时，它可能会触发一次“反思式检索”（Reflective Retrieval）：<br>让大模型生成一句类似</p><blockquote><p>“我好像需要回忆一下之前用户提过的限制条件。”</p></blockquote><p>然后根据这句话，再去检索相关记忆。<br>这种机制不靠固定阈值，而靠<strong>自我监控（self-monitoring）</strong>。<br>它更接近人类思维里的那种模糊直觉——“等等，好像哪儿不太对”。<br>在实现上，我们会给模型一个简单的函数调用接口，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> model.confidence &lt; <span class="number">0.5</span>:</span><br><span class="line">    memories = retrieve(query)</span><br></pre></td></tr></table></figure><p>让反思和检索之间形成闭环。<br>这正是 Self-RAG、A-Mem 这类系统的核心思想：<strong>Agent 主动决定何时“想起”</strong>。</p><br/><h3 id="3-事件触发：因为“当下需要”"><a href="#3-事件触发：因为“当下需要”" class="headerlink" title="3. 事件触发：因为“当下需要”"></a>3. 事件触发：因为“当下需要”</h3><p>最后一种触发是情境性的。<br>有些记忆并不是因为遗忘才被召回，而是<strong>因为情境再次出现</strong>。<br>比如，当一个任务执行失败、某个工具调用报错，Agent 会去搜索之前的失败记录：</p><blockquote><p>“上次这个 API 报 403 错时，我是怎么修的？”</p></blockquote><p>又或者，当用户在不同会话中再次提到某个主题，Agent 会识别关键词，自动检索该主题下的历史交互。<br>这类触发往往和<strong>事件监听</strong>或<strong>日志回放机制</strong>相关。<br>它让 Agent 的记忆像一个条件反射系统： “相同的信号 → 激活相似的记忆 → 快速反应。”</p><br/><br/><h2 id="检索策略：从“找得到”到“找得准”"><a href="#检索策略：从“找得到”到“找得准”" class="headerlink" title="检索策略：从“找得到”到“找得准”"></a>检索策略：从“找得到”到“找得准”</h2><p>以下策略可按复杂度逐级叠加：</p><h3 id="1-语义相似检索（Dense-Retrieval）"><a href="#1-语义相似检索（Dense-Retrieval）" class="headerlink" title="1. 语义相似检索（Dense Retrieval）"></a>1. 语义相似检索（Dense Retrieval）</h3><p>最基础的做法：将每条记忆文本编码为向量，通过余弦相似度召回前 K 条。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 工程核心代码</span></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"><span class="keyword">import</span> faiss, numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型和索引</span></span><br><span class="line">embedder = SentenceTransformer(<span class="string">&quot;all-MiniLM-L6-v2&quot;</span>)</span><br><span class="line">index = faiss.IndexFlatIP(<span class="number">384</span>)  <span class="comment"># 内积相似度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入记忆</span></span><br><span class="line">docs = [<span class="string">&quot;用户喜欢猫&quot;</span>, <span class="string">&quot;昨天搜索了寿司餐厅&quot;</span>, <span class="string">&quot;会议纪要：讨论新功能&quot;</span>]</span><br><span class="line">embeddings = embedder.encode(docs, normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">index.add(np.array(embeddings))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询检索</span></span><br><span class="line">query = <span class="string">&quot;找一家日本料理店&quot;</span></span><br><span class="line">q_emb = embedder.encode([query], normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">scores, idx = index.search(q_emb, k=<span class="number">2</span>)</span><br><span class="line">retrieved = [docs[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx[<span class="number">0</span>]]</span><br><span class="line"><span class="built_in">print</span>(retrieved)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改进方向：加上时间/任务过滤，形成 Hybrid 检索。</span></span><br></pre></td></tr></table></figure><p>✅ 优点：语义泛化强，易实现<br>⚠️ 缺点：可能召回“看似相关”的错误记忆（例如“猫”与“寿司”都和“喜欢”同义）</p><br/><h3 id="2-稀疏检索与混合检索（Sparse-Dense-Hybrid）"><a href="#2-稀疏检索与混合检索（Sparse-Dense-Hybrid）" class="headerlink" title="2. 稀疏检索与混合检索（Sparse + Dense Hybrid）"></a>2. 稀疏检索与混合检索（Sparse + Dense Hybrid）</h3><p>BM25 等稀疏方法对关键词精确匹配更可靠，可与向量检索融合：<br>最终得分 &#x3D; α * dense_score + (1 - α) * bm25_score</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hybrid 检索</span></span><br><span class="line"><span class="keyword">from</span> rank_bm25 <span class="keyword">import</span> BM25Okapi</span><br><span class="line"></span><br><span class="line">corpus = [doc.split() <span class="keyword">for</span> doc <span class="keyword">in</span> docs]</span><br><span class="line">bm25 = BM25Okapi(corpus)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hybrid_search</span>(<span class="params">query, topk=<span class="number">3</span>, alpha=<span class="number">0.6</span></span>):</span><br><span class="line">    q_emb = embedder.encode([query], normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">    dense_scores, idx = index.search(q_emb, k=<span class="built_in">len</span>(docs))</span><br><span class="line">    bm25_scores = bm25.get_scores(query.split())</span><br><span class="line"></span><br><span class="line">    combined = []</span><br><span class="line">    <span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(docs):</span><br><span class="line">        dense = <span class="built_in">float</span>(np.dot(q_emb, embedder.encode([doc], normalize_embeddings=<span class="literal">True</span>).T))</span><br><span class="line">        score = alpha * dense + (<span class="number">1</span> - alpha) * bm25_scores[i]</span><br><span class="line">        combined.append((score, doc))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(combined, key=<span class="keyword">lambda</span> x: -x[<span class="number">0</span>])[:topk]</span><br></pre></td></tr></table></figure><p>✅ 优点：平衡语义与精确匹配，适合问答与代码检索<br>⚠️ 缺点：参数 α 需按语料调节，否则会失衡</p><br/><h3 id="3-图式检索（Graph-Retrieval）"><a href="#3-图式检索（Graph-Retrieval）" class="headerlink" title="3. 图式检索（Graph Retrieval）"></a>3. 图式检索（Graph Retrieval）</h3><p>将记忆组织为事件或知识图谱，更适合任务型 Agent（如工具调用日志、人物关系、因果事件）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"></span><br><span class="line">G = nx.Graph()</span><br><span class="line">G.add_edge(<span class="string">&quot;任务A&quot;</span>, <span class="string">&quot;任务B&quot;</span>, relation=<span class="string">&quot;前置&quot;</span>)</span><br><span class="line">G.add_edge(<span class="string">&quot;用户A&quot;</span>, <span class="string">&quot;任务B&quot;</span>, relation=<span class="string">&quot;触发&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">graph_retrieve</span>(<span class="params">node, relation=<span class="literal">None</span></span>):</span><br><span class="line">    neighbors = []</span><br><span class="line">    <span class="keyword">for</span> n, attr <span class="keyword">in</span> G[node].items():</span><br><span class="line">        <span class="keyword">if</span> relation <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> attr[<span class="string">&quot;relation&quot;</span>] == relation:</span><br><span class="line">            neighbors.append(n)</span><br><span class="line">    <span class="keyword">return</span> neighbors</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(graph_retrieve(<span class="string">&quot;任务A&quot;</span>))  <span class="comment"># -&gt; [&#x27;任务B&#x27;]</span></span><br></pre></td></tr></table></figure><p>✅ 优点：可解释、支持多跳推理<br>⚠️ 缺点：构建与维护成本高，适合结构化日志&#x2F;工具链场景</p><br/><h3 id="4-元数据过滤与上下文路由"><a href="#4-元数据过滤与上下文路由" class="headerlink" title="4. 元数据过滤与上下文路由"></a>4. 元数据过滤与上下文路由</h3><p>在语义检索前，先用元信息过滤搜索空间：<br>如「同会话」「同用户」「近7天」「主题&#x3D;工具调用」等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">metadata_filter</span>(<span class="params">memory, user=<span class="literal">None</span>, topic=<span class="literal">None</span>, since=<span class="literal">None</span></span>):</span><br><span class="line">    results = memory</span><br><span class="line">    <span class="keyword">if</span> user:</span><br><span class="line">        results = [m <span class="keyword">for</span> m <span class="keyword">in</span> results <span class="keyword">if</span> m[<span class="string">&quot;user&quot;</span>] == user]</span><br><span class="line">    <span class="keyword">if</span> topic:</span><br><span class="line">        results = [m <span class="keyword">for</span> m <span class="keyword">in</span> results <span class="keyword">if</span> topic <span class="keyword">in</span> m[<span class="string">&quot;tags&quot;</span>]]</span><br><span class="line">    <span class="keyword">if</span> since:</span><br><span class="line">        results = [m <span class="keyword">for</span> m <span class="keyword">in</span> results <span class="keyword">if</span> m[<span class="string">&quot;timestamp&quot;</span>] &gt;= since]</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><p>✅ 优点：降低搜索噪声、提升召回精度<br>⚠️ 缺点：依赖高质量标注（tagging&#x2F;日志结构）</p><br/><h3 id="5-重排（Reranking）"><a href="#5-重排（Reranking）" class="headerlink" title="5. 重排（Reranking）"></a>5. 重排（Reranking）</h3><p>使用一个额外模型（Cross-Encoder 或 LLM）重新打分前 K 条结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;cross-encoder/ms-marco-MiniLM-L-6-v2&quot;</span>)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;cross-encoder/ms-marco-MiniLM-L-6-v2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rerank</span>(<span class="params">query, candidates</span>):</span><br><span class="line">    inputs = tokenizer([[query, c] <span class="keyword">for</span> c <span class="keyword">in</span> candidates], padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    scores = model(**inputs).logits.squeeze().detach().numpy()</span><br><span class="line">    ranked = [c <span class="keyword">for</span> _, c <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">zip</span>(scores, candidates), reverse=<span class="literal">True</span>)]</span><br><span class="line">    <span class="keyword">return</span> ranked</span><br></pre></td></tr></table></figure><p>✅ 优点：提升最终 Top-1 准确度<br>⚠️ 缺点：增加计算成本；通常仅用于检索后的 re-ranking</p><br/><h3 id="6-反思式检索（Self-RAG-x2F-Agentic-Retrieval）"><a href="#6-反思式检索（Self-RAG-x2F-Agentic-Retrieval）" class="headerlink" title="6. 反思式检索（Self-RAG &#x2F; Agentic Retrieval）"></a>6. 反思式检索（Self-RAG &#x2F; Agentic Retrieval）</h3><p>让 Agent 先自我评估检索结果是否足够，再决定是否“再搜一次”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reflective_retrieve</span>(<span class="params">query</span>):</span><br><span class="line">    results = hybrid_search(query)</span><br><span class="line">    <span class="comment"># Step 1: 让 LLM 评估是否充分</span></span><br><span class="line">    assessment = llm(<span class="string">f&quot;是否足够回答此问题？结果如下：<span class="subst">&#123;results&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;不足&quot;</span> <span class="keyword">in</span> assessment:</span><br><span class="line">        new_query = llm(<span class="string">f&quot;请改写更精确的查询：<span class="subst">&#123;query&#125;</span>&quot;</span>)</span><br><span class="line">        results = hybrid_search(new_query)</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><p>✅ 优点：适合复杂任务或模糊查询<br>⚠️ 缺点：多次调用 LLM，延迟较高；但往往带来最自然的“记忆感”</p><br/><h3 id="7-六大检索策略选型"><a href="#7-六大检索策略选型" class="headerlink" title="7. 六大检索策略选型"></a>7. 六大检索策略选型</h3><table><thead><tr><th>场景</th><th>推荐策略</th><th>说明</th></tr></thead><tbody><tr><td>对话类 Agent</td><td>Dense + Metadata Filter</td><td>快速、足够准确</td></tr><tr><td>企业知识问答</td><td>Hybrid + Rerank</td><td>平衡精度与覆盖</td></tr><tr><td>工具日志分析</td><td>Graph + 时间过滤</td><td>可解释、结构化</td></tr><tr><td>自反式 Agent</td><td>Self-RAG + Hybrid</td><td>智能自修正</td></tr></tbody></table><br/><br/><h2 id="六种策略，正是-RAG-的六种变体"><a href="#六种策略，正是-RAG-的六种变体" class="headerlink" title="六种策略，正是 RAG 的六种变体"></a>六种策略，正是 RAG 的六种变体</h2><p>到这里我们已经看完六种检索策略。<br>它们从语义相似、混合检索，到图式、重排、反思式检索——构成了一个完整的“找回记忆”的谱系。<br>那这些策略，与我们常说的 <strong>RAG（Retrieval-Augmented Generation）</strong> 又是什么关系？</p><br/><p>RAG 的核心逻辑其实很简单：<strong>先检索，再生成</strong>。<br>也就是说，当模型面对一个问题时，先到外部知识库中“查资料”，把相关文本取回来，再把这些资料和问题一起交给语言模型，生成最终回答。<br>这是一个流程定义，而不是具体算法。<br>换句话说，RAG 规定了“查资料”的框架，但没有规定“怎么查”。于是你看到的那些检索策略——Dense、Hybrid、Graph、Rerank、Self-RAG——其实都是在实现这个“查”的部分。它们是 RAG 框架的不同实现形态。</p><table><thead><tr><th>策略</th><th>对应的 RAG 阶段</th><th>说明</th></tr></thead><tbody><tr><td>语义相似检索</td><td>基础 RAG 检索</td><td>最标准、也是最常见的实现</td></tr><tr><td>稀疏&#x2F;混合检索</td><td>Hybrid RAG</td><td>融合语义与关键词得分，适合精准问答</td></tr><tr><td>图式检索</td><td>Graph RAG &#x2F; Knowledge RAG</td><td>以关系为索引的多跳检索</td></tr><tr><td>元数据过滤</td><td>Query Routing &#x2F; Filtered RAG</td><td>检索前预筛选，减少噪声</td></tr><tr><td>重排</td><td>RAG 后处理阶段（Reranker）</td><td>让模型重新打分、排序结果</td></tr><tr><td>反思式检索</td><td>Self-RAG &#x2F; Agentic Retrieval</td><td>让模型主动决定是否检索与改写查询</td></tr></tbody></table><br/><br/><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>记忆不是仓库，而是过滤器。<br>检索策略的设计，其实是在定义 Agent 的“注意力”模式，它决定 Agent 记忆的边界，也定义了它的思考深度。</p><br/><br/><p>❤ 下一篇，我会分享 Agent Memory 如何避免记忆漂移，让零散的信息变成稳定的知识。</p><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><strong>Self-RAG：自反式检索</strong><br>Asai et al., Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection（2023）—提出在推理时自判是否需要检索，并对检索结果与生成进行自我评估&#x2F;再检索的闭环。可直接启发“反思触发+多轮检索”设计。</li><li><strong>A-MEM：Agentic Memory</strong><br>Xu et al., A-MEM: Agentic Memory for LLM Agents（2025）—借鉴 Zettelkasten，把记忆组织成相互链接的卡片网络，新记忆写入会触发历史记忆的动态更新与重连，增强检索与演化。适合做“结构化记忆+链接检索”。代码与评测开源。</li><li><strong>Generative Agents（经典）</strong><br>Park et al., Generative Agents（2023）—首次系统展示观察→记忆→反思→检索→计划的完整闭环，强调“反思生成更高层摘要记忆”。适合作为代理记忆系统的总体架构参考。</li><li><strong>Voyager（技能库检索）</strong><br>Wang et al., Voyager: An Open-Ended Embodied Agent（2023）—在 Minecraft 中用技能库（代码+描述Embedding）语义检索以复用行为。适合“可执行技能记忆”的检索范式对照。</li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/08/Memory-as-Thinking-How-Agents-Retrieve-What-Matters/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>把 MCP 放回它该在的位置</title>
      <link>https://blog.liluhui.cn/2025/10/03/Quick-Clarity-The-Key-Difference-Between-Agent-and-MCP/</link>
      <guid>https://blog.liluhui.cn/2025/10/03/Quick-Clarity-The-Key-Difference-Between-Agent-and-MCP/</guid>
      <pubDate>Fri, 03 Oct 2025 12:08:10 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;前阵子有朋友问我：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Agent 和 MCP 到底有什么区别啊？”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我当时愣了一下。&lt;/p&gt;
&lt;p&gt;这问题乍听很简单，甚至让人觉得是不是在“混淆概念”。&lt;/p&gt;
&lt;p&gt;但转念一想，没错啊。对于</description>
        
      
      
      
      <content:encoded><![CDATA[<p>前阵子有朋友问我：</p><blockquote><p>“Agent 和 MCP 到底有什么区别啊？”</p></blockquote><p>我当时愣了一下。</p><p>这问题乍听很简单，甚至让人觉得是不是在“混淆概念”。</p><p>但转念一想，没错啊。对于刚入行的人来说，<strong>各种 buzzword 像弹幕一样飞来</strong>，Agent、MCP、RAG、LangChain……乍一看都差不多，背后却完全不是一回事。</p><p>我决定写下这篇文章。</p><p>不是为了定义概念，而是想<strong>把它们放回该在的位置</strong>。这样以后遇到选择，就不会懵逼。</p><br/><br/><h2 id="Agent-是人，MCP-是语言"><a href="#Agent-是人，MCP-是语言" class="headerlink" title="Agent 是人，MCP 是语言"></a>Agent 是人，MCP 是语言</h2><p>如果把 Agent 比作一个人：</p><ul><li>眼睛耳朵是 <strong>感知</strong>（输入）</li><li>大脑是 <strong>认知与推理</strong>（记忆、判断）</li><li>决定去做什么是 <strong>决策</strong></li><li>动手去做是 <strong>执行</strong></li><li>事后总结经验是 <strong>反馈</strong></li></ul><p>而 <strong>MCP</strong> 呢？</p><p><strong>它不是另一个“大脑”，而是这人和外部世界说话时，用的通用语言。</strong></p><p>Agent 想用锤子、笔记本、数据库……如果没有通用语言，就只能每次“手舞足蹈”解释一遍。但有了 MCP，像插 USB 一样——对接标准统一了，沟通就不再痛苦。</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/3/08_56_48%20PM.png" class="left" width="500"><br/><br/> <h2 id="三种方式接工具"><a href="#三种方式接工具" class="headerlink" title="三种方式接工具"></a>三种方式接工具</h2><p>作为开发者，我自己踩过的坑大概分三种：</p><ul><li><strong>直连 API</strong>：最省事，写几行代码就能跑。但工具一多，就像每个家电都要单独插线，很快乱成一团。</li><li><strong>自建网关</strong>：给工具统一写一层包装，好一点，但维护起来累。</li><li><strong>MCP 协议</strong>：就像买了插排，谁来都能插，但前提是大家都认这个标准。</li></ul><p>所以 MCP 解决的不是“有没有 Agent”，而是“工具怎么接得更省心”。</p><br/><br/><h2 id="什么时候该用-MCP？"><a href="#什么时候该用-MCP？" class="headerlink" title="什么时候该用 MCP？"></a>什么时候该用 MCP？</h2><p>我很喜欢把问题拆成“要不要现在就做”。</p><p><strong>推荐用 MCP 的场景：</strong></p><ul><li>你要接的工具特别多、特别杂。</li><li>你想做一个“插件市场”，别人开发的工具也能接进来。</li><li>你的团队很大，需要统一接口和权限。</li></ul><p><strong>不急着用 MCP 的场景：</strong></p><ul><li>你只有 1-2 个工具，跑起来最重要。</li><li>原型阶段，时间比优雅更关键。</li><li>场景完全内网，不需要和别人兼容。</li></ul><p>说白了：<strong>MCP 是标准，不是魔法</strong>。 如果你的产品还在“验证能不能跑”，先别急着上。</p><br/><br/><h2 id="渐进路线（我自己走过的）"><a href="#渐进路线（我自己走过的）" class="headerlink" title="渐进路线（我自己走过的）"></a>渐进路线（我自己走过的）</h2><p>做产品的时候，我的演进路线大概是：</p><ol><li><strong>先直连 API</strong>：把东西跑起来，先看到结果。</li><li><strong>工具变多了</strong>：抽象一层接口，否则维护起来要崩溃。</li><li><strong>产品要规模化</strong>：再考虑 MCP，把“工具接入”完全标准化。</li></ol><p>我不是一开始就追 MCP，而是把它当作未来的一步。<br>有点像盖房子：先把砖垒起来，等真的要接水电气，再考虑用不用品质更好的插座系统。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>所以，Agent 和 MCP 不是一回事：</p><ul><li><strong>Agent 是大脑+身体，去做事。</strong></li><li><strong>MCP 是语言+插口，帮它对接外部世界。</strong></li></ul><br/><p>理解这一点，你就不会再问“Agent 和 MCP 是不是一个东西”，而会问：</p><p><strong>“我的产品阶段，需不需要 MCP？”</strong></p><p>我觉得这才是更有价值的问题。</p><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/03/Quick-Clarity-The-Key-Difference-Between-Agent-and-MCP/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/09 Review</title>
      <link>https://blog.liluhui.cn/2025/10/02/202509/</link>
      <guid>https://blog.liluhui.cn/2025/10/02/202509/</guid>
      <pubDate>Thu, 02 Oct 2025 07:47:22 GMT</pubDate>
      
      <description>鸟站在枝头宁静安详，不是因为它知道树枝不会断，而是因为它知道自己有翅膀。</description>
      
      
      
      <content:encoded><![CDATA[<p><em>鸟站在枝头 宁静安详<br>不是因为它知道树枝不会断<br>而是因为它知道自己有翅膀</em></p><br/><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01<br><strong>绝大部分小企业从未被吃掉，大鱼想吃的是其它大鱼。</strong></p><p>在开始一滩事情之前就害怕被各路资本抄袭没什么好处，不如期待做大了就共舞，做不大就守护好自己的先进奶牛，先把事情跑起来再说。<br>今天，我想赚小而美的90%，而不是大而全的0.1%。</p><br/><p>02<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/2/002.png" alt="image"></p><p>最近的自媒体数据非常稳定的增长中，再次见证复利的力量，第一个月积累100粉，第二月达成500粉，即将完成第三个月1000粉。看着自己的产品有人使用，还是蛮开心的。也加入了科技薯的创作者社区，和前辈们交流取经。<br>目前内容方面精力有限，优先还是产品侧投入，内容主要是 数学几何画板的功能宣传 和 Agent构建相关知识科普，Q4 再继续探索探索。</p><br/><p>03<br>不舒服，让自己动起来。<br>舒服了，也让自己动起来。<br>经历过就会感谢 <strong>“体验-创造-分享-爱”</strong> 这个循环，在此之前，第一步是行动起来。</p><br/><p>04<br><strong>会买才会卖。</strong></p><p>当你真真正正能辨析自己的消费背后的需求和投射的情绪和认知，一个是会降低很多莫名其妙的消费，第二个是你也能在业务中间去创建一个理解用户需求的过程，同时你还学会了不去评价这些事情 。</p><p><strong>精分十年不出门，占星三年打死人。</strong></p><p>很嘲讽是吧，学精神分析十年都没把自己搞明白，学星盘三年就能出来干收费1500的咨询，为什么？</p><p>这是两个产品，一个要求自己被精神分析的人，是希望通过把理性照入自己的潜意识里，深度看到自己人性的善与恶后，通过自己的意志力来改变。而星盘解决的是，我有好多想法，甚至是有确定的想法，我需要一个人给我答案。<br><br/><br><br/></p><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>把最近Agent工程实践中学习和落地的知识进行梳理，这个月写了四篇文章，同步更新到博客、公众号、小红书了，这个方向随着书写越挖越深，可以写的东西也从最开始的框架选择、架构思路越来越细越拓越宽，这就是 Learn In Public 的魔力吧。</p><ul><li><a href="https://blog.liluhui.cn/2025/09/05/6-Design-Principles-I-Learned-from-Claude-Code/">从 Claude Code 学的 6 个设计铁律（含 prompts&#x2F;tool 清单）</a></li><li><a href="https://mp.weixin.qq.com/s/dy2q4omQ_GzT9alPyLJB5w">22 个顶级 Agent 工作流框架整理</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU2NTg1ODk2Mg==&mid=2247483810&idx=1&sn=1a7e92a0a2d875e17b531a618e8b632c&chksm=fcb4065fcbc38f4996d86bff572567f7125f8ab5963014098c10f078d7411e9a1ffb72cbb2a5&cur_album_id=4172082496760184844&scene=189#wechat_redirect">Agent 记忆的五层模型</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU2NTg1ODk2Mg==&mid=2247483835&idx=1&sn=acd4c7440435a8f721ea3cb9d8609126&chksm=fcb40646cbc38f50d7449907372c9e00ec7ebdd958f46c376f64a06657761695ff269eb5699a&cur_album_id=4172082496760184844&scene=189#wechat_redirect">Agent 记忆写入三大策略，决定“记什么”的工程学</a></li></ul><br/><p>02<br>大角几何画板9月份重点都放在了构建全新的 Agent workflow 流程以及各种 Bad Case 的优化，也已经进入积累用户反馈的问题&#x2F;需求池，逐步解决的情况中了，目前属于有很多可以做的也缺很多对标功能，但在优势上依然没有放大到验证有效的阶段。<br>截止9月底产品已经有3000+有效用户了，其中1000+认为是有效激活的，用户画像也开始更丰富，从基本是教师和数学从业者，增加了一些爱好者、学生和家长。<br>目前产品算是持续有进展，但还没到阶段性里程碑的程度，小团队内部的分歧也导致发力方向的混乱，这些都是客观存在的问题，希望产品发展越来越好。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/2/003.png" alt="image"><br><br/><br><br/></p><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>加入了霖子的瑜伽工作室，开始加强训练，这个月取得了蛮不错的进步。轮式的耐力明显增长了很多个呼吸；尝试了很多从没做过的进阶动作，以前不敢的鹤蝉式转头倒立也不害怕了；倒立的跳跃也明显少了些恐惧，虽然也很难一步到位就不害怕了，但我清晰充分感受到自己的变化 … 这就是瑜伽提示练习带来的增加勇气吧！<br>要夸夸自己本月取得的阶段成就：肘倒立能独立定住3秒、三点头倒立髋腿能自己立起了、不规则头倒立稳定可以起来、骆驼转轮从力量控不住起不来到能独立完成、站立上下轮完成、鸽王从非常紧张到能自己去深入的感觉。虽然有非常多可以继续努力的力量和伸展，也有很多不完美，总之感激本月取得的进步！</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/2/004.png" alt="image"><br><br/></p><p>02<br>吃到了好爽的战斧牛排 ！ （太腻了，下次不吃了）</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/2/005.png" alt="image"></p><br/><p>03<br>团建去海边吃了海鲜、放了烟花 ~（其实很想回家干活）</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/2/006.jpg" alt="image"></p><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/02/202509/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent 记忆写入策略：如何决定“记什么”？</title>
      <link>https://blog.liluhui.cn/2025/10/01/What-Should-an-Agent-Remember-Writing-Strategies-Inspired-by-Human-Memory/</link>
      <guid>https://blog.liluhui.cn/2025/10/01/What-Should-an-Agent-Remember-Writing-Strategies-Inspired-by-Human-Memory/</guid>
      <pubDate>Wed, 01 Oct 2025 09:27:33 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;在&lt;a href=&quot;https://blog.liluhui.cn/2025/09/19/Agent-Memory-Five-Layer-Model-and-Engineering-Implementation/&quot;&gt;上一篇文章&lt;/a&gt;里，我聊过 &lt;strong&gt;Agent </description>
        
      
      
      
      <content:encoded><![CDATA[<p>在<a href="https://blog.liluhui.cn/2025/09/19/Agent-Memory-Five-Layer-Model-and-Engineering-Implementation/">上一篇文章</a>里，我聊过 <strong>Agent 记忆的五层模型</strong>。那篇文章更多是框架视角：我们需要区分短期记忆、长期记忆、情景记忆、语义记忆、技能记忆等等。<br>但是，当真正开始在工程里落地的时候，很多人会遇到一个更具体的问题：</p><blockquote><p>Agent 每天都在接收大量输入，但不可能把所有内容都存下来。那，究竟应该“记什么”？</p></blockquote><p>这个问题听起来简单，背后却很关键。因为一旦写入策略没设计好，要么记忆库很快“垃圾堆积”，要么遗漏了用户真正关心的事实。今天就来聊聊 <strong>记忆写入策略</strong>。</p><br/><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>多轮对话 Agent 不可能“什么都记”，关键是设计写入策略。<br>本文总结了三种常用方法，并给出 LangGraph + 向量库的代码示例：</p><ul><li><strong>LLM 打分（重要性）</strong>：让大模型自己评估一条信息对未来是否重要，重要才写入。</li><li><strong>相似度检测（新颖性）</strong>：用 embedding 检查新信息是否与已有记忆相似，避免重复存储。</li><li><strong>计数与蒸馏（高频性）</strong>：对重复出现的信息进行计数，达到阈值后用 LLM 总结为一条稳定事实。</li></ul><p>这三种策略往往要<strong>混合使用</strong>，才能既不遗漏关键信息，也避免记忆库膨胀。</p><br/><h2 id="策略一：LLM-打分（重要性）"><a href="#策略一：LLM-打分（重要性）" class="headerlink" title="策略一：LLM 打分（重要性）"></a>策略一：LLM 打分（重要性）</h2><p>第一种策略很直接：<strong>让模型自己判断</strong>。<br>做法是：在对话后，把新信息丢给一个 LLM，让它用自然语言理解的能力打一个 “重要性分数”。比如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">请根据以下事件对未来是否重要打分（1-10）：</span><br><span class="line">“用户告诉你她最喜欢喝黑咖啡。”</span><br></pre></td></tr></table></figure><p>测试设定一个合适的阈值，分数高于这个阈值 → 写入长期记忆，分数低于这个阈值 → 丢掉或只保留在短期上下文。<br>这种方法在 Generative Agents 论文里就被用过。研究发现，模型给出的打分，和人类直觉其实挺接近的。</p><p>优点：</p><ul><li>能理解上下文的语义，不只是关键词匹配。</li><li>灵活，可以针对不同任务改写 prompt。</li></ul><p>缺点：</p><ul><li>每次都要额外调用 LLM，成本较高。</li><li>打分可能不稳定（受 prompt 影响）。</li></ul><p>但即便如此，作为第一道“筛子”，它仍然很实用。</p><br/><h2 id="策略二：相似度检测（新颖性）"><a href="#策略二：相似度检测（新颖性）" class="headerlink" title="策略二：相似度检测（新颖性）"></a>策略二：相似度检测（新颖性）</h2><p>第二种方法是<strong>避免重复</strong>。<br>我们可以把每条候选记忆编码成向量，存入一个向量数据库（比如 FAISS、Chroma、Weaviate）。<br>当新内容出现时，先检索一下：如果相似度很高，说明这东西之前就存过。→ 不新建，只更新一下计数或时间戳。如果相似度很低，说明这是全新的信息。→ 写入为新记忆。<br>例如：第一次用户说“我喜欢咖啡” → 存下来。后来又说“我很爱喝黑咖啡” → 与已有记忆相似度很高 → 不新建，而是给原记忆增加一个“+1 出现次数”。</p><p>优点：</p><ul><li>避免记忆冗余。</li><li>让记忆库保持精炼。</li></ul><p>缺点：</p><ul><li>阈值难调。过高会漏掉细微差别，过低又容易写入重复。</li></ul><br/><h2 id="策略三：计数与蒸馏（高频性）"><a href="#策略三：计数与蒸馏（高频性）" class="headerlink" title="策略三：计数与蒸馏（高频性）"></a>策略三：计数与蒸馏（高频性）</h2><p>第三种思路是<strong>重复即重要</strong>。<br>人类也是这样：别人随口说一次的话我们可能忘记，但如果一再强调，我们会牢牢记住。<br>在 Agent 中，可以通过“计数器”实现：每次遇到类似的事实，就给计数 +1。当 count ≥ 阈值（比如 3），触发一次总结：用 LLM 把这几次重复的信息，合并成一条稳定的长期记忆。</p><p>比如： “我喜欢咖啡” 出现了 3 次 → 总结为一条稳定记忆：“用户偏好：咖啡”。</p><p>这样做的好处是：</p><ul><li>记忆越来越浓缩，避免无限膨胀。</li><li>高频事实得到强化，和人类习惯接近。</li></ul><br/><h2 id="实战示例：LangGraph-向量库"><a href="#实战示例：LangGraph-向量库" class="headerlink" title="实战示例：LangGraph + 向量库"></a>实战示例：LangGraph + 向量库</h2><p>下面是我写的一个简化的 demo，展示如何把三种策略放到一个 写入节点 里。这里我用 LangGraph 来组织流程，用 FAISS 来做相似度搜索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 LLM 和向量库</span></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>)</span><br><span class="line">vectorstore = FAISS.load_local(<span class="string">&quot;memory_index&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">memory_writer</span>(<span class="params">state</span>):</span><br><span class="line">    text = state[<span class="string">&quot;observation&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 重要性打分</span></span><br><span class="line">    score = llm.predict(<span class="string">f&quot;请对以下内容打分（1-10）重要性：<span class="subst">&#123;text&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">int</span>(score) &lt; <span class="number">6</span>:</span><br><span class="line">        <span class="keyword">return</span> state  <span class="comment"># 分数太低，不写入</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 相似度检测</span></span><br><span class="line">    docs = vectorstore.similarity_search(text, k=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> docs <span class="keyword">and</span> docs[<span class="number">0</span>].score &gt; <span class="number">0.9</span>:</span><br><span class="line">        <span class="comment"># 已存在 → 更新计数</span></span><br><span class="line">        docs[<span class="number">0</span>].metadata[<span class="string">&quot;count&quot;</span>] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> docs[<span class="number">0</span>].metadata[<span class="string">&quot;count&quot;</span>] &gt;= <span class="number">3</span>:</span><br><span class="line">            summary = llm.predict(<span class="string">f&quot;总结以下信息为一条稳定事实：<span class="subst">&#123;docs[<span class="number">0</span>].page_content&#125;</span>&quot;</span>)</span><br><span class="line">            vectorstore.update(docs[<span class="number">0</span>].<span class="built_in">id</span>, summary)</span><br><span class="line">        <span class="keyword">return</span> state</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 新建记忆</span></span><br><span class="line">    vectorstore.add_texts([text], metadatas=[&#123;<span class="string">&quot;count&quot;</span>: <span class="number">1</span>&#125;])</span><br><span class="line">    <span class="keyword">return</span> state</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 LangGraph 中注册节点</span></span><br><span class="line">graph = StateGraph()</span><br><span class="line">graph.add_node(<span class="string">&quot;memory_writer&quot;</span>, memory_writer)</span><br></pre></td></tr></table></figure><p>这只是最简版骨架，真正的系统里可以扩展：</p><ul><li>importance_score 和 similarity_score 可以做加权平均。</li><li>高频性总结可以用更复杂的聚类 + LLM 总结。</li><li>遗忘机制（衰减 &#x2F; FIFO 替换）也可以接上。</li></ul><br/><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>大语言模型的上下文长度是有限的。就算用上 100k、甚至百万级上下文，你也不会想把所有历史对话原样塞进去。<br>写入策略决定了 Agent 的“人格”——它记什么，忘什么，直接影响了用户体验。<br>写入必须有一个“门槛”，类似人脑的选择性记忆。我们往往只会保留：</p><ul><li><strong>重要的</strong>：影响未来决策的事情 → 避免噪音。</li><li><strong>新鲜的</strong>以前没出现过的事实 → 避免冗余</li><li><strong>高频的</strong>：多次重复强调的偏好 → 提炼知识</li></ul><p>工程上，三种策略往往是混合使用。<br>我的建议是：</p><ul><li>从最简单的相似度检测开始，先解决重复存储问题。</li><li>再加上 LLM 打分和高频总结，让记忆越来越“聪明”。</li></ul><p>下一篇，我会聊 <strong>记忆的检索与遗忘</strong> ——如何从一大堆记忆里找回最相关的那几条，以及如何避免记忆库无限膨胀。</p><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><a href="https://arxiv.org/abs/2304.03442">Generative Agents (Park et al. 2023)</a> —— 引入重要性打分与反思机制  </li><li><a href="https://arxiv.org/abs/2404.13501">A Survey on Memory Mechanism of LLM-based Agents (Zhang et al. 2024)</a> —— 全景综述  </li><li><a href="https://github.com/ALucek/agentic-memory">AgenticMemory (GitHub)</a> —— 动态记忆库的开源实现  </li><li><a href="https://github.com/letta-ai/letta">Letta&#x2F;MemGPT (GitHub)</a> —— 带长期记忆的 Agent 框架</li></ul><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/01/What-Should-an-Agent-Remember-Writing-Strategies-Inspired-by-Human-Memory/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent 记忆五层模型与工程落地</title>
      <link>https://blog.liluhui.cn/2025/09/19/Agent-Memory-Five-Layer-Model-and-Engineering-Implementation/</link>
      <guid>https://blog.liluhui.cn/2025/09/19/Agent-Memory-Five-Layer-Model-and-Engineering-Implementation/</guid>
      <pubDate>Fri, 19 Sep 2025 03:27:20 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;对话式大模型 Agent 的「记忆」问题，本质就是：&lt;strong&gt;如何让机器像人一样，不只是即时回答，而是能记住过去、理解现在、影响未来。&lt;/strong&gt;&lt;br&gt;过去两年，学术界和工业界已经逐渐形成一个共识：记忆需要分层，不同层次解决不同问题。下面我用一个五层模型，带你</description>
        
      
      
      
      <content:encoded><![CDATA[<p>对话式大模型 Agent 的「记忆」问题，本质就是：<strong>如何让机器像人一样，不只是即时回答，而是能记住过去、理解现在、影响未来。</strong><br>过去两年，学术界和工业界已经逐渐形成一个共识：记忆需要分层，不同层次解决不同问题。下面我用一个五层模型，带你快速建立完整认知。</p><br/><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>Agent Memory 的五层模型，就像人类大脑的结构化记忆系统。</p><ul><li><strong>工作记忆负责即时</strong></li><li><strong>情景记忆负责事件</strong></li><li><strong>语义记忆负责知识</strong></li><li><strong>程序性记忆负责技能</strong></li><li><strong>外部持久记忆负责归档</strong></li></ul><p>业界工程实践已经逐步达成共识：<strong>分层 + 策略化 + 可控</strong>。这是未来对话式 Agent 真正“有记忆力”的基础。</p><br/><h2 id="1-工作记忆（Working-Memory）"><a href="#1-工作记忆（Working-Memory）" class="headerlink" title="1. 工作记忆（Working Memory）"></a>1. 工作记忆（Working Memory）</h2><ul><li><strong>定义</strong>：Agent 当前正在处理的上下文，类似人类的「短时记忆」。</li><li><strong>特点</strong>：存活时间短、容量有限，常常就是一段对话上下文窗口。</li><li><strong>类比</strong>：你在和朋友聊天时，能记住刚才说的两三句话。</li><li><strong>实现方式</strong>：<ul><li>LLM prompt 窗口（context window）</li><li>Scratchpad 技巧（中间步骤写出来）</li></ul></li><li><strong>代表案例</strong>：LangChain 的 <code>ConversationBuffer</code>、ChatGPT 的当前对话历史。</li></ul><br/><h2 id="2-情景记忆（Episodic-Memory）"><a href="#2-情景记忆（Episodic-Memory）" class="headerlink" title="2. 情景记忆（Episodic Memory）"></a>2. 情景记忆（Episodic Memory）</h2><ul><li><strong>定义</strong>：记录「事件」和「经历」，带时间线，能追溯发生了什么。</li><li><strong>特点</strong>：和具体时刻挂钩，经常包含“失败&#x2F;成功的经验”与“反思日志”。</li><li><strong>类比</strong>：你记得昨天在咖啡馆聊过天，记得过程和情绪。</li><li><strong>实现方式</strong>：<ul><li>把对话片段存档，并打上时间戳</li><li>提炼重要片段（反思）作为索引</li></ul></li><li><strong>代表论文</strong>：<ul><li><a href="https://arxiv.org/abs/2304.03442"><em>Generative Agents</em>（2023）</a>：Agent 在虚拟小镇中记忆、反思、规划。</li><li><a href="https://arxiv.org/abs/2303.11366"><em>Reflexion</em>（2023）</a>：让 Agent 自己“写日志”，从错误中学习。</li></ul></li></ul><br/><h2 id="3-语义记忆（Semantic-Memory）"><a href="#3-语义记忆（Semantic-Memory）" class="headerlink" title="3. 语义记忆（Semantic Memory）"></a>3. 语义记忆（Semantic Memory）</h2><ul><li><strong>定义</strong>：长期积累的「事实、知识、用户画像」。</li><li><strong>特点</strong>：和时间无关，更像百科或人物设定。</li><li><strong>类比</strong>：你记得朋友喜欢喝拿铁，这是稳定的知识。</li><li><strong>实现方式</strong>：<ul><li>向量数据库（Qdrant、Weaviate、FAISS）存储事实与偏好</li><li>知识图谱记录“人-事-关系”</li></ul></li><li><strong>代表论文&#x2F;项目</strong>：<ul><li><a href="https://arxiv.org/abs/2305.10250"><em>MemoryBank</em>（2023）</a>：持续对话中生成用户画像</li><li><a href="https://docs.mem0.ai/open-source/graph_memory/overview"><em>Mem0 Graph Memory</em>（2024）</a>：结构化保存人物与事件关系</li></ul></li></ul><br/><h2 id="4-程序性记忆（Procedural-Memory）"><a href="#4-程序性记忆（Procedural-Memory）" class="headerlink" title="4. 程序性记忆（Procedural Memory）"></a>4. 程序性记忆（Procedural Memory）</h2><ul><li><strong>定义</strong>：Agent 学到的「技能」或「操作套路」。</li><li><strong>特点</strong>：不是单纯事实，而是“知道如何做”。</li><li><strong>类比</strong>：你不用思考就能骑自行车、写一封邮件。</li><li><strong>实现方式</strong>：<ul><li>把常见操作总结成工具调用范式</li><li>动态 Few-Shot 示例库（常用推理模板）</li></ul></li><li><strong>工程案例</strong>：<ul><li><a href="https://www.langchain.com/langgraph">LangGraph</a> 中，Agent 会学习常用任务的状态流转</li><li><a href="https://github.com/microsoft/autogen">Copilot&#x2F;AutoGen</a> 把编辑-执行-调试形成可复用的模式</li></ul></li></ul><br/><h2 id="5-外部持久记忆（External-x2F-Persistent-Memory）"><a href="#5-外部持久记忆（External-x2F-Persistent-Memory）" class="headerlink" title="5. 外部持久记忆（External&#x2F;Persistent Memory）"></a>5. 外部持久记忆（External&#x2F;Persistent Memory）</h2><ul><li><strong>定义</strong>：超越上下文的长期存储，像“硬盘”。</li><li><strong>特点</strong>：支持无限扩展，必须解决检索与整理问题。</li><li><strong>类比</strong>：你写日记、建笔记本，未来可以随时翻查。</li><li><strong>实现方式</strong>：<ul><li>向量库 + 图数据库 + 文档存储</li><li>分层存储（高频缓存 vs. 历史归档）</li></ul></li><li><strong>代表论文&#x2F;工程</strong>：<ul><li><a href="https://arxiv.org/abs/2310.08560"><em>MemGPT</em>（2023）</a>：提出“内存分页&#x2F;中断”机制，像操作系统一样管理记忆</li><li>LangGraph × MongoDB（2024）：提供标准化的 <a href="https://www.mongodb.com/company/blog/product-release-announcements/powering-long-term-memory-for-agents-langgraph">Memory Store</a></li></ul></li></ul><br/><h2 id="工程落地的常见共识"><a href="#工程落地的常见共识" class="headerlink" title="工程落地的常见共识"></a>工程落地的常见共识</h2><h3 id="1-分层设计"><a href="#1-分层设计" class="headerlink" title="1. 分层设计"></a>1. 分层设计</h3><p>人类的记忆本来就是分层的：我们能在几秒钟里复述一串电话号码，却会在多年后仍然记得一次重要的旅行。工程上的 Agent 也是这样。短期的、随时会遗忘的“工作记忆”，与长期的、可以反复翻阅的“档案室”，需要用不同的方式去保存与检索。于是，分层设计成为共识：即时信息放在快取里，长期知识则交给更稳定的存储系统。就像书桌上的便签和书架上的厚厚档案，功能完全不同，却缺一不可。</p><p><strong>短期记忆 vs 长期记忆，采用不同存储与检索策略。</strong><br><br/></p><h3 id="2-写入策略"><a href="#2-写入策略" class="headerlink" title="2. 写入策略"></a>2. 写入策略</h3><p>想象一下，如果我们把每一句废话都写进日记，很快就会被无穷无尽的记录淹没。Agent 也是如此，它不能也不应该记住一切。于是，工程师们给它设定了选择标准：只有重要的（关系到目标或用户画像）、新颖的（之前未曾出现）、高频的（重复多次的事实）才会进入长期记忆。换句话说，Agent 的记忆更像是一本精选集，而不是毫无筛选的逐字稿。</p><p><strong>写入策略不是所有内容都要保存，只记“重要&#x2F;新颖&#x2F;高频”的。</strong><br><br/></p><h3 id="3-检索策略"><a href="#3-检索策略" class="headerlink" title="3. 检索策略"></a>3. 检索策略</h3><p>当记忆被写入，另一个问题随之而来：如何在需要时找到它？最初的做法往往依赖语义相似度，把提问和记忆做 embedding 比较。但很快，人们发现这还不够。因为有些信息不仅要“像”，还要“新”——时间因素不可或缺；有些信息要靠“重要性”来区分，避免被琐碎淹没。所以今天的检索策略更像是一场多维度的评分：语义相关、时间新鲜、重要程度，三者一起决定结果。</p><p><strong>检索策略不仅仅靠语义相似度，还要结合时间性和重要性。</strong><br><br/></p><h3 id="4-整理与巩固"><a href="#4-整理与巩固" class="headerlink" title="4. 整理与巩固"></a>4. 整理与巩固</h3><p>长久使用下来，任何系统都会遇到“记忆漂移”的问题：重复、矛盾、甚至遗忘关键信息。解决办法和人类差不多——定期整理。工程实践里，这意味着定时运行批处理，把零散的对话压缩成主题摘要，把重复的事实合并为一个清晰的条目。就像我们清理电脑文件夹，把散落各处的文档归档成整洁的文件夹，方便未来的自己。</p><p><strong>定期把零散内容压缩为摘要，避免“记忆漂移”</strong><br><br/></p><h3 id="5-透明与可控"><a href="#5-透明与可控" class="headerlink" title="5. 透明与可控"></a>5. 透明与可控</h3><p>最后，还有一个关乎信任的问题。人们普遍认为，Agent 的记忆不能是“黑箱”，用户必须能看见并掌控。用户应该知道系统记住了什么，也能在必要时删除、修改或屏蔽。ChatGPT、Claude 等主流产品已经在这一点上给出实践：用户可以进入设置界面，看到自己的“记忆”，甚至要求系统“忘掉”某些事实。这种透明和可控，不仅是工程最佳实践，更是赢得用户信赖的关键。</p><p><strong>产品化下用户能看到、编辑甚至删除 Agent 的记忆。</strong></p><br/><h2 id="延伸阅读与资料"><a href="#延伸阅读与资料" class="headerlink" title="延伸阅读与资料"></a>延伸阅读与资料</h2><ul><li><strong>论文</strong>：<ul><li><a href="https://arxiv.org/abs/2304.03442"><em>Generative Agents</em> (2023)</a></li><li><a href="https://arxiv.org/abs/2303.11366"><em>Reflexion</em> (2023)</a></li><li><a href="https://arxiv.org/abs/2310.08560"><em>MemGPT</em> (2023)</a></li><li><a href="https://arxiv.org/abs/2305.10250"><em>MemoryBank</em> (2023)</a></li><li><a href="https://arxiv.org/abs/2402.17753"><em>LoCoMo</em> (2024, 长对话记忆评测基准)</a></li></ul></li><li><strong>框架文档</strong>：<ul><li><a href="https://langchain-ai.github.io/langgraph/concepts/memory/">LangGraph Memory</a></li><li><a href="https://developers.llamaindex.ai/python/framework/module_guides/deploying/agents/memory/">LlamaIndex Memory</a></li><li><a href="https://microsoft.github.io/autogen/0.2/docs/ecosystem/agent-memory-with-zep/">AutoGen Memory &amp; Zep</a></li><li><a href="https://docs.crewai.com/en/concepts/memory">CrewAI Long-term Memory</a></li></ul></li></ul><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/09/19/Agent-Memory-Five-Layer-Model-and-Engineering-Implementation/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>🌅 22 个顶级 Agent 工作流框架整理</title>
      <link>https://blog.liluhui.cn/2025/09/12/22-top-level-Agent-workflow-frameworks-organized/</link>
      <guid>https://blog.liluhui.cn/2025/09/12/22-top-level-Agent-workflow-frameworks-organized/</guid>
      <pubDate>Fri, 12 Sep 2025 01:56:16 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近在做一个新的独立开发产品，核心功能里涉及到 多智能体协作。&lt;br&gt;在调研阶段，我发现相关框架和工具特别多：有的强调团队角色分工，有的走可视化无代码路线，还有的专注研究“涌现行为”。&lt;br&gt;于是我干脆整理了一份 Agent 框架全景图，按时间和特点做了分类。&lt;br&gt;如果要</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近在做一个新的独立开发产品，核心功能里涉及到 多智能体协作。<br>在调研阶段，我发现相关框架和工具特别多：有的强调团队角色分工，有的走可视化无代码路线，还有的专注研究“涌现行为”。<br>于是我干脆整理了一份 Agent 框架全景图，按时间和特点做了分类。<br>如果要做 B2B&#x2F;B2C 的 Agent 产品，可以判断该走 可视化无代码（Flowise、n8n）、多角色分工（Meta-GPT、ChatDev）、还是 通用框架（AutoGen、Agno、CrewAI）路线。<br>结合框架生态热度（Star 数、厂商背景）做市场判断，避免选型踩坑。</p><br/><p><a href="https://github.com/agentuniverse-ai/agentUniverse">AgentUniverse</a>（2023，开源 1.6k⭐）：支持多智能体协作，具备完整的规划、工具使用和内存管理能力，部署以本地为主，角色定义包括 PEER、DOE 等。<br><a href="https://github.com/OpenBMB/AgentVerse">Agentverse</a>（2023，开源 4.7k⭐）：聚焦多智能体协同与涌现行为，角色涵盖专家、决策者等，采用基于阶段的控制流架构，支持跨平台部署。<br><a href="https://github.com/agno-agi/agno">Agno</a>（2024，开源 32.5k⭐）：轻量级多模态智能体框架，支持 “团队模式” 的多智能体协作，采用混合流（数据 + 控制）架构，基于 Python 实现工具调用。<br><a href="https://github.com/microsoft/autogen">AutoGen</a>（2023，开源 49.2k⭐）：以多智能体对话为核心，角色包括指挥官、工作者、评论家等，采用控制流架构和 DAG（有向无环图）表示，支持代码执行和人机协作。<br><a href="https://github.com/camel-ai/camel">CAMEL</a>（2023，开源 14.2⭐）：专注于智能体间 “心智探索” 的协作，角色分为规划者、执行者等，基于 MCP 协议通信，支持模块化任务分解。<br><a href="https://github.com/OpenBMB/ChatDev">ChatDev</a>（2023，开源 27.4k⭐）：模拟软件开发团队协作，角色包括 CEO、CTO、程序员等，采用类 DAG 的控制流，侧重代码生成与调试。<br><a href="https://github.com/coze-dev/coze-studio">Coze</a>（2024，开源 16.7k⭐）：支持对话式智能体工作流，角色以会话型为主，采用节点式表示和 API 协议，可部署于 Web、移动端等多平台。<br><a href="https://github.com/crewAIInc/crewAI">CrewAI</a>（2024，开源 38k⭐）：强调角色化协作（规划者、团队成员），基于 Python DSL 定义流程，采用控制流架构和任务规划图，支持多 LLM 集成。<br><a href="https://openai.com/index/introducing-deep-research/">DeepResearch</a>（2025，闭源）：聚焦协作推理任务，角色包括搜索者、分析者等，依赖 OpenAI 生态，支持部分自我反思能力。<br><a href="https://github.com/langgenius/dify">Dify</a>（2023，开源 114k⭐）：以提示链为核心的单智能体框架，采用控制流和 JSON 配置，支持函数调用，部署方式包括 SaaS 和本地。<br><a href="https://github.com/stanfordnlp/dspy">DSPy</a>（2023，开源 28.1k⭐）：通过声明式语言定义工作流，角色涵盖规划者、检索者等，采用混合流架构，支持模块化图表示。<br><a href="https://github.com/PaddlePaddle/ERNIE-SDK">ERNIE-SDK</a>（2024，开源 370⭐）：百度推出的智能体框架，角色设计较隐式，采用流程图表示，支持多模态交互和本地部署。<br><a href="https://github.com/FlowiseAI/Flowise">Flowise</a>（2023，开源 43.5k⭐）：基于 LangChain 的可视化工具，以数据流为核心，采用 DAG 表示，适合无代码构建工作流。<br><a href="https://github.com/langchain-ai/langgraph">LangGraph</a>（2023，开源 18.5k⭐）：LangChain 生态的扩展，以节点为单位定义智能体角色，采用 DAG 和状态共享机制，支持复杂循环与条件路由。<br><a href="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one">Magnetic-One</a>（2024，开源 49.7k⭐）：多智能体通用框架，角色包括编排者、编码者等，基于 AutoGen 对话协议，采用 DAG 架构。<br><a href="https://github.com/FoundationAgents/MetaGPT">Meta-GPT</a>（2023，开源 58.4k⭐）：模拟软件开发的元编程框架，角色包括产品经理、工程师等，采用类定义的控制流，支持命令行部署。<br><a href="https://github.com/n8n-io/n8n">n8n</a>（2019，开源 137k⭐）：非 LLM 驱动的工作流自动化工具，按节点定义功能，支持 Webhook、REST 等协议，部署灵活（云、本地）。<br><a href="https://github.com/om-ai-lab/OmAgent">OmAgent</a>（2024，开源 2.5k⭐）：专注视频理解的多模态框架，角色包括规划者、检索者等，采用文本计划表示，适用于特定系统集成。<br><a href="https://github.com/openai/swarm">OpenAI Swarm</a>（2024，开源 20.4k⭐）：OpenAI 推出的多智能体框架，角色包括工作者、路由器，采用封装式表示，支持 Python&#x2F;YAML 配置。<br><a href="https://github.com/QwenLM/Qwen-Agent">Qwen-agent</a>（2024，开源 11.4k⭐）：阿里推出的智能体框架，支持自定义角色，基于 MCP 协议，采用代码驱动的控制流，部署灵活。<br><a href="https://github.com/facebook/react">ReAct</a>（2022，开源 239k⭐）：早期提示框架，无明确角色划分，以 “推理 - 行动” 循环为核心，是现代智能体工作流的基础。<br><a href="https://github.com/microsoft/semantic-kernel">Semantic Kernel</a>（2023，开源 26.1k⭐）：微软推出的 SDK，角色设计隐式，采用 DAG 表示和函数调用协议，支持多语言与跨平台部署。</p><br/><p>🤔 如果让你来选，你更倾向哪一类路线？</p><ul><li>无代码可视化</li><li>多角色团队协作</li><li>通用框架</li><li>厂商生态</li></ul><p>👇评论留言 </p><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/09/12/22-top-level-Agent-workflow-frameworks-organized/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从 Claude Code 学的 6 个设计铁律（含 prompts/tool 清单）</title>
      <link>https://blog.liluhui.cn/2025/09/05/6-Design-Principles-I-Learned-from-Claude-Code/</link>
      <guid>https://blog.liluhui.cn/2025/09/05/6-Design-Principles-I-Learned-from-Claude-Code/</guid>
      <pubDate>Fri, 05 Sep 2025 10:28:12 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h2&gt;&lt;p&gt;Claude Code 之所以“顺滑”，核心不是模型，而是架构和设计哲学。&lt;br&gt; 6 条可以直接迁移到你自己 Ag</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>Claude Code 之所以“顺滑”，核心不是模型，而是架构和设计哲学。<br> 6 条可以直接迁移到你自己 Agent 的铁律：</p><ol><li><strong>保持单一主循环</strong> —— 一条主 loop，最多一条分支，调试优先。</li><li><strong>小模型，大闭环</strong> —— 80% 的读&#x2F;扫&#x2F;总结都交给小模型，关键时刻才用大模型。</li><li><strong>上下文文件（claude.md）</strong> —— 用一个 context 文件固化团队约定与偏好。</li><li><strong>LLM Search 胜于 RAG</strong> —— 用 ripgrep&#x2F;jq&#x2F;find + LLM，而不是复杂的向量检索。</li><li><strong>分层工具设计</strong> —— 高频动作单独做工具，低频留给 Bash，高层工具保证 determinism。</li><li><strong>显式 To-Do 清单</strong> —— 让模型自己维护待办，防止长会话跑偏。</li></ol><br/><h2 id="一、保持单一主循环"><a href="#一、保持单一主循环" class="headerlink" title="一、保持单一主循环"></a>一、保持单一主循环</h2><p>我看到很多人做 Agent 时，喜欢搞多智能体、复杂 orchestrator。结果是：看 demo 很炫，真要调试时一团糟。Claude Code 完全反其道而行之：<strong>一条主循环，最多一条分支</strong>。</p><p>它的策略是：如果遇到复杂任务，就 spawn 一个“子自己”，<strong>最多一层</strong>，跑完再把结果写回消息历史。好处是整个控制流一目了然，出错时能很快定位。</p><p><strong>经验教训</strong>：debuggability &gt; 架构炫技。</p><br/><h2 id="二、小模型，大闭环"><a href="#二、小模型，大闭环" class="headerlink" title="二、小模型，大闭环"></a>二、小模型，大闭环</h2><p>Claude Code 超过一半的调用走的都是 Haiku（小模型），用来读文件、parse JSON、总结 git 历史。只有关键生成（比如复杂编辑）才用大模型。<br> 这有两个好处：</p><ul><li>成本能降 70–80%。</li><li>反馈速度快，闭环很短，用户觉得“爽”。<br> 对我来说，这其实是个很实用的模式：<strong>别浪费大模型去干小活。</strong></li></ul><br/><h2 id="三、上下文文件：claude-md-模式"><a href="#三、上下文文件：claude-md-模式" class="headerlink" title="三、上下文文件：claude.md 模式"></a>三、上下文文件：claude.md 模式</h2><p>Claude Code 的另一个 killer feature 是 claude.md。它是一个上下文文件，写清楚所有人类无法从代码直接推断出的规则：</p><ul><li>哪些目录要忽略</li><li>用哪些库</li><li>代码风格&#x2F;注释习惯<br> 每次请求都会把 claude.md 附上。效果非常明显：有和没有，差距是天与地。<br> 如果你做自己的 Agent，可以试着加个 agent.md，让它成为团队偏好的“单一真相源”。</li></ul><br/><h2 id="四、LLM-Search-胜于-RAG"><a href="#四、LLM-Search-胜于-RAG" class="headerlink" title="四、LLM Search 胜于 RAG"></a>四、LLM Search 胜于 RAG</h2><p>Claude Code 在搜索上做了一个非常“逆潮流”的选择：不用 RAG。<br>它直接让模型写 <code>ripgrep/jq/find</code>，就像你在终端里手敲一样。模型理解代码和正则，足以定位绝大多数问题。</p><p>为什么这样更好？因为 RAG 有很多隐形失效点（相似度函数、chunk 粒度、reranker…），而 LLM Search 可见、可调试。</p><p>我现在越来越相信：<strong>别急着上 RAG，先让模型像人一样搜索。</strong></p><br/><h2 id="五、分层工具设计"><a href="#五、分层工具设计" class="headerlink" title="五、分层工具设计"></a>五、分层工具设计</h2><p>Claude Code 的工具集很讲究层次感：</p><ul><li>低层：Bash、Read、Write</li><li>中层：Edit、Grep、Glob</li><li>高层：Task、WebFetch、diagnostics<br> 原则是：<strong>高频动作 → 独立工具</strong>（比如 Grep），这样更准确；低频场景交给 Bash 即可。</li></ul><p> 这点给我的启发是：设计工具时不要贪心，<strong>把 agent 最容易用错&#x2F;用频繁的动作，做成独立工具。</strong></p><br/><h2 id="六、显式-To-Do-清单"><a href="#六、显式-To-Do-清单" class="headerlink" title="六、显式 To-Do 清单"></a>六、显式 To-Do 清单</h2><p>长会话的一个大坑是“上下文腐烂”。Claude Code 的解法是：<strong>让模型自己维护 To-Do list</strong>。<br>它会频繁检查和更新 To-Do，这样就能保持方向一致，还能在中途插入&#x2F;删除子任务。<br>相比多 agent 接力，这种方式简单、直观，而且能充分利用 LLM 的“边想边写”能力。</p><br/><h2 id="附录：Prompts-amp-Tools-清单"><a href="#附录：Prompts-amp-Tools-清单" class="headerlink" title="附录：Prompts &amp; Tools 清单"></a>附录：Prompts &amp; Tools 清单</h2><p>Claude Code 的核心提示：太长评论留下邮箱，我会定期回复</p><p>Claude Code 的核心 Tools 提示：太长评论留下邮箱，我会定期回复</p><p>Claude Code 的 prompt 工程很值得参考：</p><ul><li><code>&lt;system-reminder&gt;</code>：定期提醒模型某些状态（但不暴露给用户）。</li><li><code>&lt;good-example&gt;/&lt;bad-example&gt;</code>：用示例来 steer 模型选择。</li></ul><p>工具方面，它的必备集包括：</p><ul><li>低层：Bash、Read、Write</li><li>中层：Edit、Grep、Glob</li><li>高层：Task、WebFetch、diagnostics</li></ul><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/09/05/6-Design-Principles-I-Learned-from-Claude-Code/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/08 Review</title>
      <link>https://blog.liluhui.cn/2025/09/02/202508/</link>
      <guid>https://blog.liluhui.cn/2025/09/02/202508/</guid>
      <pubDate>Tue, 02 Sep 2025 13:23:21 GMT</pubDate>
      
      <description>早早交付，经常交付</description>
      
      
      
      <content:encoded><![CDATA[<p><em>早早交付，经常交付</em></p><br/><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01<br>更新目前在维护的几个公开媒体渠道</p><ul><li><a href="https://www.xiaohongshu.com/user/profile/5678ff4a50c4b4434936b793">小红书@Luhui Dev</a> 开发者视角的短平快内容</li><li><a href="https://x.com/LuhuiDev">推特@LuhuiDev</a> 海外看到有意思的东西发发</li><li><a href="https://mp.weixin.qq.com/s/4m0Vpj8-mvUrZtlHVh59PA">公众号@芦荟的自留地</a> 更新长文内容，部分同步博客这边的内容</li><li><a href="https://liluhui.cn/">个人博客 liluhui.cn</a> 不方便向外发的</li></ul><br/><p>02<br>好热的8月，明明入秋了天天逼近40℃的高温，一整个不是很想出门，但是竟然还跑了两趟上海，新找了5公里外的瑜伽工作室，生命果然不至于折腾。</p><br/><p>03<br>在互联网上，1%的人创作，9%的人贡献，90%的人消费。要想融入圈子和社区，就要去贡献和创作，不要吝啬任何微小的行为，每一个都是反馈和互动的种子。我现在的创业理念是：深入和圈子互动，服务好我的1000个核心商业价值用户。</p><br/><p>04<br>现在想找人深度聊天越来越难，很多人没有deepthinking，他就不可能deeptalk，最后就都去deepseek了。<br>很扎心吧 U•ェ•*U</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>这个月大部分时间都在疯狂迭代大角几何画板和做小红书内容上。交互上推进了工具、多画板、登录等能力，新上了重构版的几何绘图Agent，借助推理模式能力有突破式进展，希望能覆盖更多数学绘图群体的需求。截止8月底已经有400个登录用户，1000个访问用户了，3个月不到的时间就收到了很多用户反馈蛮惊喜的。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/02/Pasted%20image%2020250902202358.png" alt="image"></p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/02/mmexport1756299569887.png" alt="image"></p><br/><p>02<br>去参加了上海的 Google I&#x2F;O 2025 开发者大会，上午主要在工作坊学习了下整个谷歌生态AI能力的SDK，下午一整个在面基网友们，上海又多了一群在干事的新朋友。<br>这次还有个小插曲，大会前2天临时起意做了个大会日程APP，正常借此做内容，24小时转化了第一批30多个用户，详细的复盘可以看<a href="https://blog.liluhui.cn/2025/08/18/googleio/">这篇</a>。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/02/IMG_20250814_183840.jpg" alt="image"></p><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>我的新头像系列 （感谢生图模型的迅速进展）</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/02/Weixin%20Image_2025-08-21_212357_7.png" alt="image"></p><br/><p>02<br>重启羽毛球，最近停药后疯狂长胖，加入有氧训练！<br>其实也想学网球，一直没找到合适的机会和老师 …<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/02/IMG_20250809_150254.jpg" alt="image"></p><br/><p>03<br>家里小植物们长得好好，每天看着桌上和阳台能感受到生命力就在这里。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/02/IMG_20250825_093257.jpg" alt="image"></p><br/><p>04<br>这个月看来《捕风追影》《浪浪山小妖怪》《戏台》《南京照相馆》，我的月度最佳是《南京照相馆》。哦对，还蹭了暑假孩子们的8k星空展览，带上设备在宇宙间看星系的感觉也非常棒哈哈哈。</p><p>一句话推荐：<br>《捕风追影》不要带脑子的爽片，拳拳到肉，成龙+鲜肉，反派超帅。<br>《浪浪山小妖怪》小孩子的快乐，成年人的悲伤，雅俗共赏的分层电影。以及，上海美术电影制片厂。<br>《戏台》陈佩斯，陈佩斯，陈佩斯！<br>《南京照相馆》不以视角切入沉重的历史，跟随小人物的沉浸，看宏观世界的规律，看走出影院后的太平盛世。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/02/IMG_20250823_162658.jpg" alt="image"></p><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E7%94%9F%E6%B4%BB/">生活</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/09/02/202508/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>构建 Agent 中的方法论陷阱</title>
      <link>https://blog.liluhui.cn/2025/08/22/Traps-of-Methodology-in-Agent-Development/</link>
      <guid>https://blog.liluhui.cn/2025/08/22/Traps-of-Methodology-in-Agent-Development/</guid>
      <pubDate>Fri, 22 Aug 2025 03:49:54 GMT</pubDate>
      
      <description>看似聪明的 Agent 方法论，可能是陷阱。少即是多。</description>
      
      
      
      <content:encoded><![CDATA[<p><em>看似聪明的 Agent 方法论，可能是陷阱。少即是多。</em></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前几天在刷推的时候，看到了 cline 的开发者 Ara 提到的一段话。<br>他说，在构建 AI Agent 时，有三种听起来很聪明的想法，其实常常是「思维毒药」。</p><p>“思维毒药” 这个词总归怪怪的，我觉得“陷阱”这个比喻让我更有共鸣。因为这些思维确实很有吸引力，就像远远看去是一条捷径，但走进去才发现，里面布满了泥潭。</p><p>作为一个开发者，我一边读，一边在回想自己做 Agent 的过程——踩过的坑，走过的弯路，几乎都能在这里找到影子。</p><h2 id="一、关于多代理编排"><a href="#一、关于多代理编排" class="headerlink" title="一、关于多代理编排"></a>一、关于多代理编排</h2><p>想象一下这样的画面：<br>有一支虚拟团队，里面有“分析员 Agent”“执行员 Agent”“总调度 Agent”。它们像人类小组一样，分工协作、互相传递消息，最后汇总出完美的结果。</p><p>听上去是不是很酷？我当初也实践过。</p><p>可现实是：<strong>大多数真正有价值的 Agent 工作，本质上还是单线程的</strong>。</p><p>复杂的编排，不仅增加了技术上的不确定性，还让我们更难解释模型的行为。<br>换句话说，它让项目背上了两份负担：<strong>实现复杂度 + 解释复杂度</strong>。</p><p>曾经做过一个 Team Demo 来完成 AI Coding 这件事，尝试用多个 Agent 分工协作。结果没走几步，就被“状态同步”和“出错恢复”折腾得焦头烂额。调试的时候，根本不知道到底是哪个 Agent 在搞鬼。</p><p>到最后，我还是把逻辑简化掉，留给一个更强的单体 Agent 去完成。意外的是，效果比多代理方案还要稳定。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/22/2.png"></p><h2 id="二、关于-RAG-的补丁效应"><a href="#二、关于-RAG-的补丁效应" class="headerlink" title="二、关于 RAG 的补丁效应"></a>二、关于 RAG 的补丁效应</h2><p>过去两年，RAG（检索增强生成）几乎成了构建 Agent 的标配。好像不加上它，整个系统就“不完整”。</p><p>但真实的情况是：<strong>RAG 更像是一块补丁</strong>。<br>很多时候，一个简单的 grep 命令，或者一张设计良好的数据库表，就能解决问题。<br>如果你的数据和知识管理一团乱，RAG 只会把混乱放大，它不是万能钥匙。重要的数据，数据，还是 TMD 数据。</p><p>我刚开始用 RAG 的时候，觉得它是“救命稻草”。模型终于可以“记住”外部知识了。可随着项目复杂度增加，我发现问题并没有减少，反而多了新的麻烦：检索不准、上下文过载、延迟变高…… 慢慢我意识到：<strong>真正重要的，其实是信息组织方式</strong>。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/22/3.png"></p><h2 id="三、关于「更多指令-x3D-更好结果」"><a href="#三、关于「更多指令-x3D-更好结果」" class="headerlink" title="三、关于「更多指令 &#x3D; 更好结果」"></a>三、关于「更多指令 &#x3D; 更好结果」</h2><p>还有一个陷阱，是很多人都会踩的：<br>我们常常相信，只要写更长、更详细的 prompt，模型就会更聪明。</p><p>现实往往相反。<br>长 prompt 增加了 token 成本，响应速度更慢，结果也不一定更好。更糟糕的是，它掩盖了真正的问题：<strong>模型本身的能力边界</strong>。</p><p>我刚开始玩 LLM 的时候，我也写过像“说明书”一样长的 prompt，把能想到的规则全堆上去。<br>但最后发现，简洁、清晰的三句话，往往比十段废话更有效。</p><p>现在我更看重的是 <strong>任务建模和上下文管理</strong>。<br>比如明确拆分任务，合理设计输入输出，这些往往比写一个“超级提示词”来得靠谱。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/22/4.png"></p><h2 id="总结：少即是多"><a href="#总结：少即是多" class="headerlink" title="总结：少即是多"></a>总结：少即是多</h2><p>回顾这三点：</p><ul><li><strong>多代理编排</strong>：是幻觉，现实里常常多余。</li><li><strong>RAG</strong>：是补丁，价值有限，不该当成金科玉律。</li><li><strong>长 prompt</strong>：是迷思，解决不了模型的根本限制。</li></ul><p>别被“听起来聪明”的概念绑架，真正有效的，往往是简单、直接、可解释的方案。<br>作为开发者，我更愿意拥抱这种“少即是多”的原则。<br>因为时间和资源有限，每一行代码、每一个设计选择，都需要指向真正的核心价值。</p>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/08/22/Traps-of-Methodology-in-Agent-Development/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>48 小时小工具实录：我做了个大会助手</title>
      <link>https://blog.liluhui.cn/2025/08/18/googleio/</link>
      <guid>https://blog.liluhui.cn/2025/08/18/googleio/</guid>
      <pubDate>Mon, 18 Aug 2025 15:11:51 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;img src=&quot;https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/18/asdaweq.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;起心动念&quot;&gt;&lt;a href=&quot;#起心动念&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/18/asdaweq.png" alt="image"></p><h3 id="起心动念"><a href="#起心动念" class="headerlink" title="起心动念"></a>起心动念</h3><p>周一晚上，本来想早点休息，结果脑子突然冒出个念头：</p><blockquote><p>“要是有个离线可用、免登录的大会日程助手就好了。”</p></blockquote><p>于是干脆一口气写了 5 个小时，把一个可用版本搞了出来。核心约束很明确：免登录、离线、轻交互。目标很单纯，就是现场能救急。</p><br/><h3 id="临时开工"><a href="#临时开工" class="headerlink" title="临时开工"></a>临时开工</h3><p>整个技术方案非常 indie：</p><ul><li>React + PWA + Vercel 一键部署</li><li>数据靠 GPT 整理的大会官网，然后导入 JSON</li><li>没有后台，没有账号，全在前端跑</li></ul><p>代码写得像夜宵拼盘，能跑就行。为什么选纯前端？为了离线和快速交付。上线第一天就接到 iOS 用户反馈打不开，着急忙慌修问题，才发现自己紧急开发没有充分测试设备的问题。那一晚边修边想：“这样上线真的有人用吗？”</p><h3 id="上线与运营"><a href="#上线与运营" class="headerlink" title="上线与运营"></a>上线与运营</h3><p>周二白天，我赶紧补了点运营内容。发到 v2ex，几乎没人理；同时小红书，反而慢慢起来了。陆陆续续有人私信问“怎么用”，我只好一边当客服一边补文案。没想到借着这件事，还面基了不少人。</p><p>这让我意识到：即便是独立开发的小工具，找到合适的分发渠道也比想象中重要。</p><h3 id="两天的数据"><a href="#两天的数据" class="headerlink" title="两天的数据"></a>两天的数据</h3><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/08/18/zxcqsa.png" alt="image"></p><p>结果出来后有点意外：</p><ul><li>76 位用户</li><li>841 次访问</li><li>平均使用时长 2 分 10 秒</li><li>主题页 438 次访问，详情页 93 次</li></ul><p>比我自己刷朋友圈的时长还长，说明不是点开就走人。尤其主题页访问量很高，说明用户确实想“逛全局”，不是只搜一场。</p><br/><h3 id="What-went-well"><a href="#What-went-well" class="headerlink" title="What went well"></a>What went well</h3><ul><li><strong>免登录 + 离线可用</strong>：如设想成了最大卖点。</li><li><strong>需求真实驱动</strong>：临时上线也有人用，证明确实解决了痛点。</li><li><strong>小红书意外带量</strong>：运营不是我擅长的，但确实帮我接住了用户。</li></ul><br/><h3 id="What-didn’t-go-well"><a href="#What-didn’t-go-well" class="headerlink" title="What didn’t go well"></a>What didn’t go well</h3><ul><li><strong>预热不足</strong>：要是提前一周发布，效果可能翻倍。</li><li><strong>数据采集缺失</strong>：只接了 GA4 页面浏览，没法更准确地知道预约行为。</li></ul><p>结果就是：复盘时我只能靠各个页面数据来猜激活。根因很简单：我当时的“成功定义”还停留在“能上线可用”，而不是“被看见、被采用”。</p><br/><h3 id="反思-amp-下一步"><a href="#反思-amp-下一步" class="headerlink" title="反思 &amp; 下一步"></a>反思 &amp; 下一步</h3><p>这次实验让我真正意识到，复盘在于两件事：<strong>如何更快挖掘真实需求</strong>，以及<strong>如何把运营节奏踩准</strong>。</p><ul><li><strong>需求发现</strong>：这次是自己用痛点驱动临时开发，下次应该提前去看不同社区、社交平台上用户在哪些地方吐槽过类似问题，尽早发现“隐形需求”，最好提前一周留出验证窗口。</li><li><strong>运营节奏</strong>：节奏感比功能更关键。T-72 小时就该放预热帖，T-24 小时要有“一图一文案”准备好，正式开幕当天要能推出“今日变更&#x2F;热门场次”的内容。</li><li><strong>分发优先级</strong>：这次 v2ex 没起量，小红书反而爆发，说明不同渠道效果差异巨大，下次要更明确主力渠道，少浪费子弹。</li></ul><p>下一步的方向：</p><ol><li><strong>继续验证不同类型大会</strong>：看看这个模式在学术会议、技术沙龙、大规模大会上效果是否一致。</li><li><strong>优化快速部署能力</strong>：缩短从临时起意到可上线的时间，把常用组件和脚手架打包好。</li><li><strong>打磨一套数据解析与生成工具</strong>：针对不同大会的日程结构，统一导入、解析和生成，避免重复劳动。</li></ol><br/><h3 id="收尾"><a href="#收尾" class="headerlink" title="收尾"></a>收尾</h3><p>说白了，这 76 个用户不是因为我运营厉害，而是需求实在强烈。上线前一晚还在修 bug，上线后疯狂回私信解释。典型 indie dev：<strong>边做边慌，边慌边学</strong>。</p><p>但这次让我明白了一件事：上线不等于成功。下次要把“被看见、被采用”写进 Definition of Done。</p><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/08/18/googleio/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>