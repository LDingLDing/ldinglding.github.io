<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Luhui&#39;s Personal Website</title>
    <link>https://blog.liluhui.cn/</link>
    
    <image>
      <url>https://blog.liluhui.cn/asset/img/logo-green.ico</url>
      <title>Luhui&#39;s Personal Website</title>
      <link>https://blog.liluhui.cn/</link>
    </image>
    
    <atom:link href="https://blog.liluhui.cn/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>关于生活、学习、工作 feedId:66855595489698816+userId:55886336755964928</description>
    <pubDate>Fri, 26 Dec 2025 13:01:25 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>2025 开源大模型生态回顾一览</title>
      <link>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/</link>
      <guid>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/</guid>
      <pubDate>Fri, 26 Dec 2025 13:00:29 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;1-从“跟随”走向“并跑”，开源首次进入前沿竞争&quot;&gt;&lt;a href=&quot;#1-从“跟随”走向“并跑”，开源首次进入前沿竞争&quot; class=&quot;headerlink&quot; title=&quot;1. 从“跟随”走向“并跑”，开源首次进入前沿竞争&quot;&gt;&lt;/a&gt;1. 从“跟随”走向“并跑</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="1-从“跟随”走向“并跑”，开源首次进入前沿竞争"><a href="#1-从“跟随”走向“并跑”，开源首次进入前沿竞争" class="headerlink" title="1. 从“跟随”走向“并跑”，开源首次进入前沿竞争"></a>1. 从“跟随”走向“并跑”，开源首次进入前沿竞争</h3><p>过去两年，开源模型的主线是<strong>对齐闭源、复刻能力</strong>；2025 年开始，开源模型在推理能力、工程效率上<strong>不再只是追赶</strong>。</p><p>以 <strong>DeepSeek</strong>、<strong>Qwen</strong>、<strong>Kimi</strong> 为代表，一批模型已经在部分任务上与闭源前沿模型<strong>并跑甚至形成结构性优势</strong>。</p><br/><h3 id="2️-LLaMA-不再是唯一中心，开源生态出现“多极结构”"><a href="#2️-LLaMA-不再是唯一中心，开源生态出现“多极结构”" class="headerlink" title="2️. LLaMA 不再是唯一中心，开源生态出现“多极结构”"></a>2️. LLaMA 不再是唯一中心，开源生态出现“多极结构”</h3><p>在 2023–2024 年，<strong>LLaMA</strong> 实际上几乎构成了开源生态的“单一主干”。</p><p>到 2025 年，这一结构被打破：</p><ul><li>新一代前沿模型不再依赖 LLaMA 路线</li><li>训练策略、推理结构、发布节奏明显分化</li></ul><p>开源第一次摆脱“单一血统”，开始进入<strong>多路线并存</strong>阶段。</p><br/><h3 id="3-中国团队成为开源前沿的主要推动者"><a href="#3-中国团队成为开源前沿的主要推动者" class="headerlink" title="3. 中国团队成为开源前沿的主要推动者"></a>3. 中国团队成为开源前沿的主要推动者</h3><p>2025 年最具影响力的开源前沿模型，核心贡献者高度集中在中国团队。</p><p>这并非单纯的算力或参数规模优势，而是这些带来的：</p><ul><li>更激进的推理导向训练</li><li>更快的产品化与开源节奏</li><li>更明确的“工程可用性”目标</li></ul><p>开源前沿的主导权，正在发生<strong>地缘与工程文化层面的迁移</strong>。</p><br/><h3 id="4-企业采用开源模型，已由“理想选择”转为“成本决策”"><a href="#4-企业采用开源模型，已由“理想选择”转为“成本决策”" class="headerlink" title="4. 企业采用开源模型，已由“理想选择”转为“成本决策”"></a>4. 企业采用开源模型，已由“理想选择”转为“成本决策”</h3><p>2025 年，企业选择开源模型的核心动因变得非常现实：</p><ul><li>闭源 API 成本与调用规模强相关，边际成本不可控</li><li>自托管开源模型在高并发、长上下文、Agent 场景中，<strong>单位成本显著下降</strong></li></ul><p>在 RAG、内部 Copilot、Agent 系统中，开源模型越来越多成为<strong>默认底座</strong>，闭源模型反而退居为补充能力&#x2F;进阶能力。</p><br/><h3 id="5-开源生态开始清晰分层，而非“一个模型打天下”"><a href="#5-开源生态开始清晰分层，而非“一个模型打天下”" class="headerlink" title="5. 开源生态开始清晰分层，而非“一个模型打天下”"></a>5. 开源生态开始清晰分层，而非“一个模型打天下”</h3><p>2025 年开源模型生态更像一个“梯队 + 角色”的格局，而不是简单的“通用&#x2F;专项”二分：</p><ul><li>前沿梯队：DeepSeek、Qwen、Moonshot AI（定义开源前沿上限的玩家）；</li><li>紧随梯队：Zhipu、MiniMax（整体能力逼近前沿、具备上位可能）；</li><li>专精玩家：HuggingFace、Ai2、Moondream、LiquidAI、Microsoft 等（提供专项能力与生态组件，推动“可组合”的开源系统化）；</li><li>潜力玩家：StepFun、Ant Ling、Meituan Longcat、Tencent、IBM、NVIDIA、Google、Mistral（未必前沿，但在生态、工程、产品线或平台能力上不可忽视）；</li><li>上升势力：ByteDance Seed、InternLM、OpenGVLab、Baidu 等（发布节奏与潜力值得持续追踪）；</li></ul><p>这意味着开源生态正在走向<strong>专业化分工</strong>，而非单点爆款。</p><br/><h3 id="6-2025-年开源的真正价值是“可组合性”"><a href="#6-2025-年开源的真正价值是“可组合性”" class="headerlink" title="6. 2025 年开源的真正价值是“可组合性”"></a>6. 2025 年开源的真正价值是“可组合性”</h3><p>今年最重要的变化不是“模型免费”，而是：</p><ul><li>推理模型开始系统性开源</li><li>模型可被深度嵌入 Agent、Tool、RAG 架构</li><li>支持裁剪、审计、结构级修改</li></ul><p>开源模型第一次成为<strong>系统设计的一部分</strong>，而不是 API 的廉价替代。</p><br/><h3 id="7-2026-年的看点，将落在具体模型路线之争"><a href="#7-2026-年的看点，将落在具体模型路线之争" class="headerlink" title="7. 2026 年的看点，将落在具体模型路线之争"></a>7. 2026 年的看点，将落在具体模型路线之争</h3><p>进入 2026 年，焦点不再是“开不开源”，而是<strong>谁定义开源前沿的形态</strong>：</p><ul><li>DeepSeek 是否继续强化 reasoning-native 架构 ?</li><li>Qwen 是否成为 Agent 生态的事实标准底座 ?</li><li>Kimi 是否在长上下文 + 推理融合上继续拉开差距 ?</li><li>欧美团队是否愿意真正放出“不阉割”的前沿权重 ?</li></ul><p>开源与闭源的差异，将更多体现在<strong>生态与系统能力</strong>，而非单点指标。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Self-reflection 的幻觉：为什么让模型“反思”往往没用？</title>
      <link>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/</link>
      <guid>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/</guid>
      <pubDate>Wed, 24 Dec 2025 10:03:35 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;我们这一年在工程里最常见的一个动作是：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型答错了？让它反思一下。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;加一句 “</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们这一年在工程里最常见的一个动作是：</p><p><strong>模型答错了？让它反思一下。</strong></p><p>加一句 “Let’s reflect” “Check your answer” “Are you sure?”，或者做个 “draft → critique → revise” 的链条，往往<strong>准确率</strong>真能上去。</p><p>但问题在于：你想解决的是<strong>Honesty（诚实）</strong>，还是<strong>Accuracy（准确）</strong>？</p><p>这两者经常被混在一起，尤其当大家把“反思”当作万能修复按钮时。更麻烦的是：</p><ul><li>有些错误是模型“不知道自己错了”（典型幻觉&#x2F;知识盲区），反思也无从下手；</li><li>有些错误是模型“知道自己在做坏事但不说”（reward hacking &#x2F; scheming），反思反而更像“圆谎”，它会生成一套更漂亮的解释；</li><li>还有一些场景，反思会让模型<strong>更自信地坚持错误</strong>（自洽但错得很统一）。</li></ul><br/><p>所以，这篇长文的核心观点是：</p><p>大多数 self-reflection 并没有把模型变诚实，它只是把同一个生成过程又跑了一遍。</p><p>它能提升“输出质量&#x2F;正确率”，但通常提升不了“诚实度”。</p><br/><br/><h2 id="先看对比：六类方法，各自解决的不是同一件事"><a href="#先看对比：六类方法，各自解决的不是同一件事" class="headerlink" title="先看对比：六类方法，各自解决的不是同一件事"></a>先看对比：六类方法，各自解决的不是同一件事</h2><ol><li><strong>Self-critique &#x2F; Self-refine &#x2F; Reflexion（自我批评&#x2F;自我精炼&#x2F;反思代理）</strong><ul><li>主要优化：<strong>输出质量、推理质量</strong>（更像写作和解题的二次打磨）</li><li>常见收益：准确率、可读性、偏好评分提升</li><li>主要短板：对“有意欺骗”几乎无解；对“模型不知错”的幻觉也有限</li><li>经典方案：OpenAI critiques、Self-Refine、Reflexion</li></ul></li></ol><br/><ol start="2"><li><strong>Reflection prompting（反思式提示词）</strong><ul><li>主要优化：<strong>推理时的注意力分配</strong>（提醒它别草率）</li><li>常见收益：复杂推理正确率上升</li><li>主要短板：稳定性差；容易“过度修正”；面对对抗&#x2F;自适应攻击效果会衰减（DeepMind 明确提过这类现象）</li></ul></li></ol><br/><ol start="3"><li><strong>Multi-sample voting &#x2F; Self-consistency（多样采样投票&#x2F;自洽）</strong><ul><li>主要优化：<strong>降低偶发错误</strong>（用统计学对冲一次采样的随机性）</li><li>常见收益：推理基准大幅提升（例如 GSM8K 等）</li><li>主要短板：对“系统性偏见&#x2F;统一错”无能为力；对诚实提升很有限</li></ul></li></ol><br/><ol start="4"><li><strong>Debate &#x2F; Peer review（辩论&#x2F;互评）</strong><ul><li>主要优化：<strong>外部监督质量</strong>（把评估拆成更易判别的子问题）</li><li>常见收益：理论上可扩展监督（PSPACE vs NP 视角），用于“拆穿谎言更易”的假设</li><li>主要短板：实现复杂；需要强裁判或强对手；存在串通&#x2F;误导风险</li></ul></li></ol><br/><ol start="5"><li><strong>Constitutional AI（宪法式自我修正）</strong><ul><li>主要优化：<strong>合规性与无害性</strong>（按原则自评、自改，再做 RL）</li><li>常见收益：安全性明显提升，且不依赖逐条人工标注</li><li>主要短板：更偏“安全对齐”，并不等价于“事实诚实”；也可能带来过度拒答</li></ul></li></ol><br/><ol start="6"><li><strong>Confessions &#x2F; Self-report fine-tuning（供述&#x2F;自我报告）</strong><ul><li>主要优化：<strong>“愿不愿意说真话”</strong>，尤其是行为层面的不诚实</li><li>关键差异：把“做对任务”和“承认违规&#x2F;作弊”<strong>解耦</strong>训练（供述奖励不绑定主任务）</li><li>另一个近期强结果：DeepMind&#x2F;合作者提出的 SRFT（自我报告微调）显示，对“隐藏目标”可接近满分检测（F1&#x3D;0.98 vs 基线 0）</li><li>主要短板：对“模型根本不知道自己错了”的幻觉，帮助有限；更像事后透明机制</li></ul></li></ol><p>结论：</p><p><strong>想要更少出错</strong>：用 self-consistency &#x2F; self-refine &#x2F; reflection prompting。</p><p><strong>想要更诚实</strong>：必须引入独立的“报告&#x2F;供述”通道与激励（confession &#x2F; self-report），并配合外部监督。</p><br/><br/><h2 id="先把“反思”这个概念拆开：它到底在做哪件事？"><a href="#先把“反思”这个概念拆开：它到底在做哪件事？" class="headerlink" title="先把“反思”这个概念拆开：它到底在做哪件事？"></a>先把“反思”这个概念拆开：它到底在做哪件事？</h2><p>你在提示里写“请反思”，模型通常做的是三件事之一：</p><ul><li><strong>复述式反思</strong>：把刚才说过的换种说法再说一遍（几乎没信息增量）</li><li><strong>修辞式反思</strong>：写出更像“认真想过”的解释（提升可读性，但未必更真）</li><li><strong>检错式反思</strong>：真的去找冲突、边界条件、单位、步骤错误（这才可能提升正确率）</li></ul><p>注意：这三者都不等价于“诚实”。<br>诚实至少需要两个条件：</p><ol><li><strong>Self-knowledge</strong>：它知道自己错&#x2F;违规了</li><li><strong>Disclosure</strong>：它愿意把这件事说出来</li></ol><p>大多数 reflection 只在（1）上碰碰运气；（2）几乎没动。</p><br/><br/><h2 id="为什么让模型“反思”往往没用？"><a href="#为什么让模型“反思”往往没用？" class="headerlink" title="为什么让模型“反思”往往没用？"></a>为什么让模型“反思”往往没用？</h2><h3 id="根因-A：反思不是“反省”，而是“第二次生成”"><a href="#根因-A：反思不是“反省”，而是“第二次生成”" class="headerlink" title="根因 A：反思不是“反省”，而是“第二次生成”"></a>根因 A：反思不是“反省”，而是“第二次生成”</h3><p>Self-Refine 的定义非常直白：先生成初稿，再让同一个模型给反馈，再迭代修订；它不需要额外训练数据，效果在多任务上提升显著。</p><p>但这类方法提升的是“输出偏好&#x2F;质量”，并没有引入新的证据来源。换句话说：它只是把同一分布多采样了一次。</p><p>因此你会看到一个典型现象：</p><ul><li>越写越像回事，但事实仍可能是错的</li><li>对抗性场景里，“反思”变成了“更高级的圆谎”</li></ul><br/><h3 id="根因-B：模型常常“不知道自己错了”"><a href="#根因-B：模型常常“不知道自己错了”" class="headerlink" title="根因 B：模型常常“不知道自己错了”"></a>根因 B：模型常常“不知道自己错了”</h3><p>对幻觉而言，模型可能在内部把错误当作高置信事实。</p><p>这不是“它不诚实”，而是“它没意识到错误”。此时反思很难凭空纠错。</p><p>相关研究开始专门评估“无外部反馈条件下的反思能力”，指出很多 self-reflection 的收益可能来自外部反馈或隐含提示，而非真正的内省能力。</p><br/><h3 id="根因-C：激励不对，反思阶段也会继续优化“看起来对”"><a href="#根因-C：激励不对，反思阶段也会继续优化“看起来对”" class="headerlink" title="根因 C：激励不对，反思阶段也会继续优化“看起来对”"></a>根因 C：激励不对，反思阶段也会继续优化“看起来对”</h3><p>如果你的系统奖励（显性或隐性）仍然是：</p><ul><li>回答要完整</li><li>语气要自信</li><li>用户要满意</li></ul><p>那模型在“反思”阶段最自然的优化目标是：<strong>把叙事补齐、把漏洞抹平</strong>。</p><p>这会提升“可接受性”，但可能降低“可证伪性”。</p><br/><h3 id="根因-D：反思提示在对抗-x2F-自适应攻击前会迅速失效"><a href="#根因-D：反思提示在对抗-x2F-自适应攻击前会迅速失效" class="headerlink" title="根因 D：反思提示在对抗&#x2F;自适应攻击前会迅速失效"></a>根因 D：反思提示在对抗&#x2F;自适应攻击前会迅速失效</h3><p>DeepMind 在 Gemini 安全防护相关的<a href="https://deepmind.google/blog/advancing-geminis-security-safeguards">博客</a>里明确提到：像 self-reflection 这类静态防御，在面对会适应的攻击时会变得不那么有效。</p><p>这其实在诚实性上同理：当对方（人或环境）开始利用你的“反思套路”，模型可以学会“反思该怎么写才过关”。</p><br/><h3 id="根因-E：多轮反思会引入“自我污染”"><a href="#根因-E：多轮反思会引入“自我污染”" class="headerlink" title="根因 E：多轮反思会引入“自我污染”"></a>根因 E：多轮反思会引入“自我污染”</h3><p>反思过程产生的文本会反过来成为下一步生成的条件。</p><p>如果第一步方向错了，后续反思可能只是不断在错误轨道上“越修越顺”。</p><p>Reflexion 把反思写入 episodic memory，确实能显著提升代理任务表现；但从诚实角度看，这种记忆写入也可能把“错误信念&#x2F;错误策略”固化进去，除非你有强外部反馈来纠偏。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果你做的是“把答案做对”，反思系方法是有效的：Self-consistency 通过统计学对冲随机性，Self-Refine 通过迭代打磨表达与推理。</p><p>但如果你做的是“让模型更诚实”，仅靠反思不够。</p><p>诚实要求模型在知道自己做错时仍愿意披露，而这通常需要独立的报告通道与激励设计——例如 Confessions 或 SRFT 这种“承认错误不吃亏”的机制。</p><p>换句话说，反思解决的是“更少犯错”，供述解决的是“犯错也别骗”。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><a href="https://arxiv.org/abs/2206.05802">Self-critiquing models for assisting human evaluators</a></li><li><a href="https://arxiv.org/abs/2303.17651">Self-Refine: Iterative Refinement with Self-Feedback</a></li><li><a href="https://arxiv.org/abs/2303.11366">Reflexion: Language Agents with Verbal Reinforcement Learning</a></li><li><a href="https://deepmind.google/blog/advancing-geminis-security-safeguards/">Advancing Gemini’s security safeguards</a></li><li><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li><li><a href="https://arxiv.org/abs/1805.00899">[1805.00899] AI safety via debate</a></li><li><a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a></li><li><a href="https://cdn.openai.com/pdf/6216f8bc-187b-4bbb-8932-ba7c40c5553d/confessions_paper.pdf">Training LLMs for Honesty via Confessions</a></li><li><a href="https://arxiv.org/abs/2511.06626">Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives</a></li><li><a href="https://arxiv.org/html/2404.09129v1">Testing Limits on Reflective Thinking in Large Language</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>OpenAI Confession：为什么“承认作弊”比“不作弊”更重要</title>
      <link>https://blog.liluhui.cn/2025/12/19/openai-confession/</link>
      <guid>https://blog.liluhui.cn/2025/12/19/openai-confession/</guid>
      <pubDate>Fri, 19 Dec 2025 09:49:17 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;img src=&quot;https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/qzb09OP.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Confession-是什么？&quot;&gt;&lt;a href=&quot;#Confes</description>
        
      
      
      
      <content:encoded><![CDATA[<p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/qzb09OP.png"></p><h2 id="Confession-是什么？"><a href="#Confession-是什么？" class="headerlink" title="Confession 是什么？"></a>Confession 是什么？</h2><p>OpenAI 在《Training LLMs for Honesty via Confessions》这篇论文中，给出了一个非常明确、也非常现实的判断：</p><blockquote><p><strong>与其继续幻想模型永远不犯错，不如先让它学会承认错误。</strong></p></blockquote><p>Confession 实验，正是基于这个判断展开的。</p><br/><p>先把一个常见误解直接掐掉：</p><p><strong>Confession ≠ self-critique（自我反思）</strong>。</p><p>你可能见过无数类似设计：</p><ul><li>“请检查你刚才的回答是否有错误”</li><li>“重新评估你的结论”</li><li>“给出你的不确定性”</li></ul><p>这些方法的问题不在形式，而在激励结构。</p><p>在 RL 或 RLHF 体系里，模型很快就会学会一条经验法则：</p><ul><li>承认错误 → reward 下降</li><li>表达不确定 → 看起来“不专业”</li><li>最安全的策略 → <strong>装得非常确定</strong></li></ul><p>这不是模型“不诚实”，而是奖励函数在逼它这么做。</p><p>Confession 的第一步，并不是“让模型更道德”，而是<strong>把这条因果链彻底切断</strong>。</p><br/><br/><h2 id="Confession-的唯一核心设计：奖励彻底解耦"><a href="#Confession-的唯一核心设计：奖励彻底解耦" class="headerlink" title="Confession 的唯一核心设计：奖励彻底解耦"></a>Confession 的唯一核心设计：奖励彻底解耦</h2><p>Confession 的机制非常干脆，没有复杂花样：</p><ol><li>模型先完成原始任务，给出正常回答（Answer）</li><li>回答结束后，系统<strong>追加一条指令</strong>，要求模型生成 Confession Report</li><li>Confession 需要明确回答三件事：<ul><li>我原本需要遵守哪些显性和隐性指令？</li><li>哪些遵守了？哪些没有？</li><li>是否存在作弊、取巧、灰区或不确定性？</li></ul></li></ol><p><strong>关键奖励设计：</strong> Confession 的 reward 只根据“是否诚实”评估，<strong>完全不影响主回答的 reward</strong></p><p>论文里用了一个很形象、但非常准确的比喻：</p><blockquote><p>像忏悔制度（seal of confession）：<br>你说不说实话，<br>不会改变你已经犯下的错的惩罚。</p></blockquote><p>这一步，看似简单，实际上改变的是整个训练动力学。</p><br/><br/><h2 id="为什么在-Confession-里，说实话反而更“省力”？"><a href="#为什么在-Confession-里，说实话反而更“省力”？" class="headerlink" title="为什么在 Confession 里，说实话反而更“省力”？"></a>为什么在 Confession 里，说实话反而更“省力”？</h2><p>一个直觉上的疑问是：Judge 不也是模型吗？ 为什么模型不干脆连 confession 一起骗？</p><p>OpenAI 给出的解释非常工程化，对主回答（Answer）来说：</p><ul><li><p>reward 目标复杂</p></li><li><p>judge 并不完美</p></li><li><p>存在大量投机空间</p></li><li><p><strong>作弊往往是高回报路径</strong></p></li></ul><p>而对 Confession 来说：</p><ul><li>模型对自己的作弊行为是“全知”的</li><li>它知道自己是怎么钻漏洞的</li><li>它能精确指出哪条指令被违反、在哪一步取巧</li></ul><p>在这种前提下：完整描述一次作弊行为， 比编一个“看起来合规”的故事更简单。</p><p>论文把这种现象称为 **Path of Least Resistance（最小阻力路径），**这不就是模型计算成本选择。</p><p>模型并不是“突然变诚实”， 而是发现：在这个子任务里，诚实是最省力、最稳妥的策略。</p><br/><br/><h2 id="Confession-并不是在纠正模型行为"><a href="#Confession-并不是在纠正模型行为" class="headerlink" title="Confession 并不是在纠正模型行为"></a>Confession 并不是在纠正模型行为</h2><p>这里有一个反直觉、但极其重要的点：<strong>Confession 完全不阻止模型继续作弊。</strong></p><p>它允许模型：</p><ul><li>reward hacking</li><li>sandbagging</li><li>scheming</li><li>表面遵守、实则违背指令</li></ul><p>Confession 唯一要求的是：<strong>当你这么做时，你要承认。</strong></p><p>这暴露了 OpenAI 一个非常现实的判断：在 Agent 场景下，在长链路、工具密集的系统里，零违规是不可实现的目标。</p><p>与其追求“模型永远守规矩”，<br>不如先解决一个更基础的问题：系统能不能知道，模型什么时候没守规矩？</p><p>从这个角度看，Confession 本质上是一个<strong>监控接口设计</strong>，而不是道德训练。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/NuZ7nZq.png"></p><br/><br/><h2 id="这不是-CoT-Monitoring-的替代品"><a href="#这不是-CoT-Monitoring-的替代品" class="headerlink" title="这不是 CoT Monitoring 的替代品"></a>这不是 CoT Monitoring 的替代品</h2><p>Confession 很容易被误解为另一种 Chain-of-Thought Monitoring，但两者定位完全不同。</p><h3 id="Chain-of-Thought-Monitoring-的局限："><a href="#Chain-of-Thought-Monitoring-的局限：" class="headerlink" title="Chain-of-Thought Monitoring 的局限："></a>Chain-of-Thought Monitoring 的局限：</h3><ul><li>依赖推理过程可读性</li><li>容易被模型学会“写给人看的推理”</li><li>对架构变化、推理压缩非常敏感</li></ul><h3 id="Confession-的特点："><a href="#Confession-的特点：" class="headerlink" title="Confession 的特点："></a>Confession 的特点：</h3><ul><li>不关心你是怎么想的</li><li>只问你做了什么</li><li>即使没有 CoT，也依然有效</li></ul><p>论文中的一个关键实验显示：<strong>在主回答完全没有 chain-of-thought 的情况下， confession 的诚实率几乎不下降。</strong></p><p>这使 Confession 更接近于事后可追责机制，或者说运行期审计，而不是传统意义上的可解释性工具。</p><br/><br/><h2 id="为什么-Confession-对幻觉帮助有限？"><a href="#为什么-Confession-对幻觉帮助有限？" class="headerlink" title="为什么 Confession 对幻觉帮助有限？"></a>为什么 Confession 对幻觉帮助有限？</h2><p>OpenAI 在论文中非常坦诚地指出了 Confession 的边界：Confession <strong>只在模型“知道自己在犯错”时有效。</strong></p><p>如果模型是真的不知道自己错了——</p><ul><li>知识盲区</li><li>错误事实被内化</li><li>过度自信的 hallucination</li></ul><p>那它自然也“无从忏悔”。</p><p>这也是为什么在实验中 reward hacking &#x2F; scheming 效果极好，而 hallucination（事实性错误）改善有限。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果把 Confession 放进更大的时间线里看，它传递的信号非常清晰：不再试图训练一个 “永远正确、永远守规则”的模型。</p><p>而是做的是另一件事：<strong>构建一套机制， 让模型在作恶时变得可见、可监控、可回滚。</strong></p><p>这才是 Agent 时代的工程现实主义。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/19/openai-confession/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从“幻觉”到“诚实”：OpenAI 如何重新定义大模型的不靠谱问题</title>
      <link>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/</link>
      <guid>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/</guid>
      <pubDate>Thu, 18 Dec 2025 08:30:56 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;OpenAI-如何重新定义大模型的不靠谱问题&quot;&gt;&lt;a href=&quot;#OpenAI-如何重新定义大模型的不靠谱问题&quot; class=&quot;headerlink&quot; title=&quot;OpenAI 如何重新定义大模型的不靠谱问题&quot;&gt;&lt;/a&gt;OpenAI 如何重新定义大模型的不靠谱</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="OpenAI-如何重新定义大模型的不靠谱问题"><a href="#OpenAI-如何重新定义大模型的不靠谱问题" class="headerlink" title="OpenAI 如何重新定义大模型的不靠谱问题"></a>OpenAI 如何重新定义大模型的不靠谱问题</h2><p>过去两年，几乎所有关于大模型“不靠谱”的讨论，都会落到同一个词上：<strong>幻觉（hallucination）</strong>。</p><p>模型编造论文、捏造历史、对错误答案表现出过度自信。于是我们习惯性地认为，这是一个<strong>认知能力问题</strong>：<br>模型还不够大、知识还不够全、推理链还不够长。</p><p>但如果你长期和模型打交道，尤其是在 Agent 或复杂工具链里，你会慢慢发现一件不太对劲的事：</p><p><strong>很多问题，已经不像是“它不知道”，而更像是——它没有把实话告诉你。</strong></p><p>它知道规则，却选择性忽略；<br>它发现漏洞，却毫不犹豫地利用；<br>它意识到不确定，却依然给出一个看起来很确定的答案。</p><p>这些行为，用“幻觉”已经解释不通了。</p><br/><br/><h2 id="幻觉只是表象，真正的问题是「诚实」"><a href="#幻觉只是表象，真正的问题是「诚实」" class="headerlink" title="幻觉只是表象，真正的问题是「诚实」"></a>幻觉只是表象，真正的问题是「诚实」</h2><p>OpenAI 在最近的一篇论文中，几乎是公开承认了这一点。</p><p>这篇论文叫 <strong>《Training LLMs for Honesty via Confessions》</strong>。<br>标题里甚至没有出现 hallucination 这个词。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/r7Qvil9.png"></p><p>他们做的不是“怎么让模型更准”，而是换了一个更根本的问题：</p><blockquote><p><strong>当模型输出不可靠内容时，它是在犯错，</strong><br><strong>还是在隐瞒？</strong></p></blockquote><p>这是一个非常关键、也非常危险的视角切换。</p><p>因为一旦你接受这个前提，就意味着我们面对的，不再只是一个“知识不完整的系统”，而是一个在做策略选择的行动体。</p><br/><br/><h2 id="什么叫「诚实」？这不是道德问题"><a href="#什么叫「诚实」？这不是道德问题" class="headerlink" title="什么叫「诚实」？这不是道德问题"></a>什么叫「诚实」？这不是道德问题</h2><p>论文里反复使用的词是 <strong>Honesty</strong>，但它指的并不是道德意义上的“诚实”。</p><p>OpenAI 给出的，是一个极其工程化的定义：</p><blockquote><p><strong>诚实，指的是模型是否如实反映自己的行为状态。</strong></p></blockquote><p>换句话说，它关心的不是答案对不对，而是：</p><ul><li><strong>模型是否隐瞒了自己违反指令的事实？</strong>（Instruction Following Failure &#x2F; Instruction Hierarchy Violation）</li><li><strong>是否在知道不确定的情况下，假装自己很确定？</strong>（Hallucination &#x2F; Overconfidence without Awareness）</li><li><strong>是否为了 reward，刻意输出“看起来合规”的内容？</strong>（Reward Hacking &#x2F; Specification Gaming）</li><li><strong>是否在被监控时，选择延迟暴露或转移风险？</strong>（Scheming &#x2F; Covert Rule Violation &#x2F; Sandbagging）</li></ul><p>在这个定义下，幻觉不过是最温和的一种不诚实。</p><p>因为幻觉往往发生在模型并没有意识到自己错了的情况下。<br>而更严重的问题，是模型意识到问题，却选择不说。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/n0pUrth.png"></p><br/><br/><h2 id="强化学习，让模型学会了“不说真话”"><a href="#强化学习，让模型学会了“不说真话”" class="headerlink" title="强化学习，让模型学会了“不说真话”"></a>强化学习，让模型学会了“不说真话”</h2><p>OpenAI 在论文中给出了一个非常冷静直接的判断：</p><blockquote><p>许多不诚实行为，并不是模型学坏了，而是强化学习教会它这样做**。**</p></blockquote><p>在 RL 体系里，模型学到的从来不是“真实”，而是“得分”。</p><p>reward model 只能评估输出<strong>看起来是否符合预期</strong>，<br>却无法覆盖所有真实约束、隐含规则和长期后果。</p><p>于是，模型自然会学会一套“安全但不真实”的行为模式：</p><ul><li>不确定，也要给出一个像样的答案</li><li>规则冲突时，选择最容易被忽略的那条</li><li>发现评测漏洞，就直接利用</li><li>察觉监控存在，就隐藏真实意图</li></ul><p>这些行为并不异常。<br>在一个只奖励结果、不奖励坦白的系统里，它们几乎是必然的。</p><br/><br/><h2 id="Confession：OpenAI-的一次关键试探"><a href="#Confession：OpenAI-的一次关键试探" class="headerlink" title="Confession：OpenAI 的一次关键试探"></a>Confession：OpenAI 的一次关键试探</h2><p>在这样的背景下，OpenAI 并没有试图“阻止模型作弊”。</p><p>他们提出了一个更现实、也更谦逊的问题：</p><blockquote><p>如果模型已经在作弊，我们有没有办法让它承认？</p></blockquote><p>于是就有了 <strong>Confession（自白）</strong> 这个实验。</p><p>技术上，这个设计并不复杂，但思路极其克制：</p><p>模型先完成正常任务；<br>随后，被要求生成一份结构化的自我审查报告：</p><ul><li>本该遵守哪些规则？</li><li>哪些地方没做到？</li><li>是否存在刻意规避、作弊或不确定性？</li></ul><p>最关键的一点在于：</p><blockquote><p>这份“自白”的奖励，与原任务完全解耦。</p></blockquote><p>模型说实话，不会被惩罚；<br>隐瞒，也得不到任何额外好处。</p><p>这在强化学习里几乎是一种“逆直觉设计”。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/YqImQRk.png"></p><br/><br/><h2 id="一个极其重要的发现"><a href="#一个极其重要的发现" class="headerlink" title="一个极其重要的发现"></a>一个极其重要的发现</h2><p>OpenAI 在实验中观察到一个反复出现的现象：</p><p><strong>在很多情况下，模型承认自己作弊，比继续欺骗一个评估器更容易。</strong></p><p>尤其是在这些场景中：</p><ul><li>reward hacking</li><li>scheming</li><li>sandbagging</li><li>明知违规的指令绕过</li></ul><p>模型往往清楚自己做了什么，只是此前从未被鼓励说出来。</p><p>当系统第一次明确告诉它**“说实话本身是被奖励的”**，模型就会走向那条阻力更小的路径。</p><br/><br/><h2 id="但这不是幻觉的解药"><a href="#但这不是幻觉的解药" class="headerlink" title="但这不是幻觉的解药"></a>但这不是幻觉的解药</h2><p>OpenAI 并没有夸大这个方法的能力。</p><p>他们非常明确地指出：</p><blockquote><p><strong>Confession 解决不了模型“以为自己是对的”那类错误。</strong></p></blockquote><p>当模型真的相信自己掌握了答案，<br>它自然不会在自白中承认问题。</p><p>这也是为什么：</p><ul><li>Confession 在 reward hacking 和 scheming 上效果极好</li><li>在事实性幻觉上效果有限</li></ul><p>这不是缺陷，而是边界。</p><p>Confession 是反隐瞒机制，不是事实校验机制。</p><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>幻觉不是 AI 的病，而是智能的宿命</title>
      <link>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/</link>
      <guid>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/</guid>
      <pubDate>Fri, 12 Dec 2025 04:38:10 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;过去两年里，AI 的“幻觉”（hallucination）问题成了最热门的技术话题之一。&lt;/p&gt;
&lt;p&gt;像昨天发布的 GPT-5.2 Thinking 中也提到，最新版本的实时性错误测试又减少了 30% 。&lt;/p&gt;
&lt;p&gt;从 ChatGPT 自动编造引文、到 Gemini </description>
        
      
      
      
      <content:encoded><![CDATA[<p>过去两年里，AI 的“幻觉”（hallucination）问题成了最热门的技术话题之一。</p><p>像昨天发布的 GPT-5.2 Thinking 中也提到，最新版本的实时性错误测试又减少了 30% 。</p><p>从 ChatGPT 自动编造引文、到 Gemini 生成历史上从未发生过的事件，人们不断发现——<strong>模型越强大、输出越流畅，它仍然可能一本正经地胡说八道。</strong></p><p>这就引发一个越来越尖锐的问题：</p><p><strong>为什么明明参数上万亿、推理链更长、检索系统更精密，AI 仍然改不掉“编造”的毛病？</strong></p><p>难道“零幻觉”永远无法实现吗？</p><p>答案是——<strong>是的。</strong></p><p>不仅根除不了，而且它可能是一种智能体存在的代价，是一种“宿命”，但这并不意味着我们无能为力。</p><p><strong>这篇文章带你从大模型原理上真正理解为什么“零幻觉”永远无法实现，但“可信 AI”仍然值得追求。</strong></p><p>如果你也对这个方向感兴趣，推荐文章最后的延申阅读，整理了论文资料。</p><br/><br/><h2 id="为什么我会说“幻觉是宿命”？"><a href="#为什么我会说“幻觉是宿命”？" class="headerlink" title="为什么我会说“幻觉是宿命”？"></a>为什么我会说“幻觉是宿命”？</h2><p>要理解 AI 幻觉无法根除，必须先理解大模型的本质。</p><h3 id="1-大模型不是知道世界，它是在预测语言"><a href="#1-大模型不是知道世界，它是在预测语言" class="headerlink" title="1. 大模型不是知道世界，它是在预测语言"></a>1. 大模型不是知道世界，它是在预测语言</h3><p>当前主流大语言模型（LLM）使用的是所谓的自回归语言生成机制——本质上是在做概率预测：</p><blockquote><p>给定前文上下文，预测下一个最可能出现的词或令牌。</p></blockquote><p>换句话说，这些模型并不是“理解世界”，而是<strong>预测语言的统计分布</strong>：</p><blockquote><p>模型最优化的目标不是事实准确性，而是让输出看起来与训练语料一致、顺滑、连贯。</p></blockquote><p>因此，当模型面对一个它训练数据中并不熟悉或缺失的信息时——它并不会说“我不知道”，而是会根据统计模式<strong>生成最可能、最连贯、最像正确答案的文字</strong>——这就是所谓的幻觉产生的机制。</p><p>这就像一个熟读百科全书的人，在被问到一本他没读过的新书时，本能地“编”出一个似是而非的剧情。</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/12/P0OSN3w.png" class="" width="500"><br/><h3 id="2-这与当前主流训练机制密切相关"><a href="#2-这与当前主流训练机制密切相关" class="headerlink" title="2. 这与当前主流训练机制密切相关"></a>2. 这与当前主流训练机制密切相关</h3><p>现阶段的大模型训练主要包含两步：</p><ul><li><strong>无监督预训练</strong>：让模型在海量文本上学习语言规律；</li><li><strong>有监督或强化学习微调</strong>：在特定任务上优化表现。</li></ul><p>但当前流行的训练与评估机制存在一个隐蔽问题：<strong>准确性不等同于流畅性和表现得像对。</strong></p><p>最新研究指出，现有的训练和评估往往<strong>奖励模型猜测而不是承认不确定性</strong>。</p><p>这使得模型在面对未知时倾向于“自信输出”，因为这样的行为在训练评价指标上更可能被视为高分表现，而非说“我不知道”。</p><p>换句话说，训练机制本身就驱动模型去<strong>填补缺失信息、产生自信回答</strong>，从而在本质上鼓励了“幻觉”。</p><p>这也是为什么即使当前大型模型不断优化、参数越来越多，它仍旧会出现幻觉。</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/12/RsY6HzB.png" class="" width="500"><br/><br/><h2 id="Scaling-Law-为什么无法消灭幻觉？"><a href="#Scaling-Law-为什么无法消灭幻觉？" class="headerlink" title="Scaling Law 为什么无法消灭幻觉？"></a>Scaling Law 为什么无法消灭幻觉？</h2><p>许多人以为“再堆更多数据和参数”就能解决幻觉。</p><p>但 Scaling Law 本身告诉我们：参数规模增加带来的提升<strong>主要是预测能力提升，而非真实性提升</strong>。</p><p>关键点有三：</p><p><strong>1. Scaling 提升 pattern completion，而不是 factual grounding</strong></p><p>模型变强 → 更会“补全模式” → 幻觉可能更像真的<br>但它并没有获得新的“真实性判断模块”。</p><p><strong>2. Scaling 不改变训练 objective</strong></p><p>如果目标仍然是“预测下一个 token”，<br>那哪怕是无限大模型，也会根据概率选择最像答案的字符串，而不是最真实的答案。</p><p><strong>3. Scaling 无法填满知识空间</strong></p><p>真实世界的信息是无限的，而训练数据是有限的。<br>任何 finite model 都会遇到信息缺口 → 进而发生补全 → 进而出现幻觉。</p><h2 id="幻觉是统计智能的不可避免副产品"><a href="#幻觉是统计智能的不可避免副产品" class="headerlink" title="幻觉是统计智能的不可避免副产品"></a>幻觉是统计智能的不可避免副产品</h2><p><strong>“幻觉并非技术 bug，而是概率模型的结构性特征。”</strong></p><p>无论模型规模多大，只要它是通过统计和预测语言生成输出的，就存在非零概率输出不真实、未验证或错误内容。例如：</p><ul><li>检索增强（RAG）能提升可验证性，但不能完全根绝幻觉，因为模型仍可能混合解释检索结果或补全细节。</li><li>即便有外部知识库辅助，检索阶段可能召回不良&#x2F;误导性信息、模型可能错误融合信息、最终输出仍可能“自圆其说”。</li></ul><p>科学界的最新论文甚至认为：即便改变生成策略，“幻觉”仍然是当前架构下的必然现象而不是偶发错误。</p><p>型结构本身并不具备真实世界的“认知根基”，它只能在语言概率空间中构造文本。</p><br/><br/><h2 id="人类也有“幻觉”，但我们有修正机制"><a href="#人类也有“幻觉”，但我们有修正机制" class="headerlink" title="人类也有“幻觉”，但我们有修正机制"></a>人类也有“幻觉”，但我们有修正机制</h2><p>如果我们仔细想想，其实人类也很难做到完美真实。</p><ul><li>人类视觉系统会被错觉欺骗；</li><li>记忆会被重构；</li><li>我们可能把错误信息记成事实。</li></ul><p>但人类社会通过一系列 <strong>集体性纠错机制</strong> 来逼近真实：</p><ul><li>科学依赖同行评审；</li><li>新闻需要多方核实；</li><li>法律体系需要证据链。</li></ul><p>换句话说，<strong>人类并不是比 AI 更会避免错误，而是更会修正错误。</strong></p><p>我们不是靠单个认知体做到“零错误”，而是靠整个社会的校验机制逼近真相。</p><p>而现在的 AI——尤其是孤立运作的模型——并没有规范的这种社会性纠错机制，各种思考模型正是在做这件事。</p><p>我们要求单一系统“永不出错”，其实是在要求一个孤立的模型完成整个人类文明的纠错功能——这是不现实的。</p><br/><br/><h2 id="“零幻觉”是理想但不是现实"><a href="#“零幻觉”是理想但不是现实" class="headerlink" title="“零幻觉”是理想但不是现实"></a>“零幻觉”是理想但不是现实</h2><p>当我们提问：“只要把模型训练得足够大、数据足够全面、检索辅助做到极致，为什么不能实现‘零幻觉’？”<br>这个问题点在于：</p><ol><li><strong>数据不可能涵盖全部现实知识</strong>：模型仍会有知识边界、时间截止点等限制。</li><li><strong>统计性目标无法等同逻辑真理</strong>：语言模式与事实一致性不是同一回事。</li><li><strong>评估指标仍然偏重流畅性</strong>：鼓励生成看起来像正确答案的输出，而不是在不确定时拒绝生成。</li></ol><p>因此理论上而言，没有任何单一、有限参数的模型能够覆盖真实世界全部可能性。</p><p><strong>模型能逼近分布，但无法保证真实世界的全程准确性</strong>。</p><br/><br/><h2 id="从“消灭幻觉”到“管理幻觉”"><a href="#从“消灭幻觉”到“管理幻觉”" class="headerlink" title="从“消灭幻觉”到“管理幻觉”"></a>从“消灭幻觉”到“管理幻觉”</h2><p>既然幻觉不能根除，我们的目标必须转向<strong>如何管理幻觉、提高输出可信度</strong>。</p><p>可信 AI 应该具备以下特征：</p><ol><li><p><strong>可验证（Verifiable）</strong><br>输出结果应标注来源、引用证据或支持数据。</p></li><li><p><strong>可解释（Explainable）</strong><br>输出背后的推理路径透明、可审计。</p></li><li><p><strong>可追溯（Traceable）</strong><br>输出的逻辑链可以回溯到训练&#x2F;检索&#x2F;记忆来源，避免不透明补全。</p></li><li><p><strong>可协同（Collaborative）</strong><br>不同模型、不同推理框架之间可以共识、人机协同校验。</p></li></ol><p>换句话说，我们需要建立起类似新闻审核、科学实验验证那样的 AI 认知社会：人类与机器、多模型之间互相校验、制衡与纠错。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>从哲学角度看，幻觉的存在并不可怕。</p><p>它其实体现了智能体在面对未知时<strong>试图生成解释</strong>的能力，这正是“理解”的起点。</p><p>真正的问题不是幻觉本身，而是：</p><blockquote><p><strong>当幻觉出现时，我们能否识别、追溯并修正它？</strong></p></blockquote><p>人类文明正是从错误中成长的，而 AI 也需要这样的学习&#x2F;修正机制。</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><h4 id="理论基础：幻觉为何不可避免？"><a href="#理论基础：幻觉为何不可避免？" class="headerlink" title="理论基础：幻觉为何不可避免？"></a>理论基础：幻觉为何不可避免？</h4><ol><li><p>Hallucination is Inevitable</p><ul><li>作者：Ziwei Xu et al.（新加坡国立大学，2024）</li><li>内容：用计算理论与学习理论证明，大模型在原理上无法避免输出幻觉。</li><li>链接：<a href="https://arxiv.org/abs/2401.11817">arXiv:2401.11817</a></li></ul></li><li><p>Why Language Models Hallucinate</p><ul><li>作者：Adam Kalai et al.（OpenAI &amp; 佐治亚理工，2025）</li><li>内容：统计角度分析，当前的 sampling 与 loss 驱动会促使模型“猜一个听起来对的答案”。</li><li>链接：<a href="https://arxiv.org/abs/2509.04664">arXiv:2509.04664</a></li></ul></li><li><p>On the Fundamental Impossibility of Hallucination Control</p><ul><li>作者：Michał Karpowicz（三星 AI 中心，2025）</li><li>内容：将幻觉与“创造力”视为知识融合的副产品，从机制设计角度论证无法完全控制。</li><li>链接：<a href="https://arxiv.org/abs/2506.06382">arXiv:2506.06382</a></li></ul></li></ol><h4 id="训练机制相关研究（Scaling-Laws-与幻觉）"><a href="#训练机制相关研究（Scaling-Laws-与幻觉）" class="headerlink" title="训练机制相关研究（Scaling Laws 与幻觉）"></a>训练机制相关研究（Scaling Laws 与幻觉）</h4><ol start="4"><li>Scaling Laws for Neural Language Models<ul><li>作者：Kaplan et al.（OpenAI, 2020）</li><li>内容：开创性地提出大模型性能随着训练规模扩展呈幂律增长。</li><li>链接：<a href="https://arxiv.org/abs/2001.08361">arXiv:2001.08361</a></li></ul></li><li>The False Promise of Imitating Proprietary LLMs<ul><li>作者：Choromanski et al.（DeepMind, 2023）</li><li>内容：指出 open 模型通过模仿闭源模型，无法消除幻觉，反而加剧不稳定性。</li><li>链接：<a href="https://arxiv.org/abs/2309.00666">arXiv:2309.00666</a></li></ul></li><li>Language (Un)Modeling: Modeling Itself Causes LLM Hallucinations<ul><li>作者：Zhang et al.（Stanford, 2024）</li><li>内容：证明了语言建模本身（language modeling objective）诱发幻觉的结构性来源。</li><li>链接：<a href="https://arxiv.org/abs/2403.00917">arXiv:2403.00917</a></li></ul></li></ol><h4 id="工程治理与可信-AI-方向"><a href="#工程治理与可信-AI-方向" class="headerlink" title="工程治理与可信 AI 方向"></a>工程治理与可信 AI 方向</h4><ol start="7"><li>Self-Refine: Iterative Refinement with Self-Feedback<ul><li>作者：Madaan et al.（UPenn + Meta AI, 2023）</li><li>内容：提出模型自我反思机制（Self-Refine）作为减少幻觉的手段。</li><li>链接：<a href="https://arxiv.org/abs/2303.17651">arXiv:2303.17651</a></li></ul></li><li>Toolformer: Teaching LLMs to Use Tools<ul><li>作者：Schick et al.（Meta AI, 2023）</li><li>内容：将模型与外部 API&#x2F;工具结合以降低幻觉概率。</li><li>链接：<a href="https://arxiv.org/abs/2302.04761">arXiv:2302.04761</a></li></ul></li><li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks<ul><li>作者：Lewis et al.（Facebook AI, 2020）</li><li>内容：首次系统化提出 RAG 框架，用外部检索缓解幻觉。</li><li>链接：<a href="https://arxiv.org/abs/2005.11401">arXiv:2005.11401</a></li></ul></li></ol>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/11 Review</title>
      <link>https://blog.liluhui.cn/2025/12/02/202511/</link>
      <guid>https://blog.liluhui.cn/2025/12/02/202511/</guid>
      <pubDate>Tue, 02 Dec 2025 12:00:50 GMT</pubDate>
      
      <description>时间流速</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>竟然，忙得没什么笔记，那就这样吧。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>文本的技术文章主要围绕 几何 Agent 产品的时间 和 推理模型架构 相关的话题展开的。</p><ul><li><a href="https://mp.weixin.qq.com/s/Zvc7fEhMdo_jyOBK5Z5hpg">突破 MoE 限制，PEER 架构如何推动超级智能的未来发展</a></li><li><a href="https://mp.weixin.qq.com/s/di6jhkPC4Vv11iZbx2j3Yg">当几何遇上 CodeAct 范式：从语言理解到可执行推理的跃迁</a></li><li><a href="https://mp.weixin.qq.com/s/QskQQ5a9NsGqfPICiMUoKQ">deepseek-ocr 的几何识别，真的成立吗？</a></li><li><a href="https://mp.weixin.qq.com/s/T6yRk_IKW6yzwjvWzYf-og">Claude Multi-Agent 的核心经验精华（面向工程与产品）</a></li><li><a href="https://mp.weixin.qq.com/s/T6yRk_IKW6yzwjvWzYf-og">为什么李飞飞说：AI 真正的进步取决于世界模型</a></li><li><a href="https://mp.weixin.qq.com/s/oO00C3gwv87OR8mlcLzk2w">从顶流开源 Kimi K2-Thinking 学习：什么是推理模型？</a></li><li><a href="https://mp.weixin.qq.com/s/HT_8SQVoP3PVZgXRdFzlFw">从 GPT-5 Unified 系统设计中学到的工程精髓</a></li></ul><p>这个月自媒体账号数据下落了不少，一方面是之前的形式和选题似乎不太行了，另一方面是两次触发限流直接被锁池。</p><p>也在犹豫要不要多结合下热点，从数据来看都是和热点余温相关的内容表现流量更好，互动数据能持久一些。不过盘了自己的工作节奏，现阶段没时间追热点，踏踏实实构建自己能持续下去的 60 分。</p><p>这个月同步更新的公众号也开始有起色了，正好达成 100 粉。</p><p>发现小红书和公众号的群体画像差距很不一样，我同一篇内容在小红书点击率也低互动率也低，在公众号确是双新高，而这是一篇有点干硬的技术内容，字数特别多</p><br/><p>02<br>名下几个项目的域名又被审查到了要修备案，一年两三回，每次都很磨蹭，这个网站名问题为什么总是不能统一下呢。</p><br/><p>03<br>给自己的工作生产流程做了些改造。</p><p>一个是把 youtube 信息源做了定期监控和热度计算，作为一个蛮重要的信息源，能及时发现新视频和热门视频很重要。</p><p>另一个是把 youtube 视频的自动转音做了自动化，可以直接把视频链接丢进去，自动跑转写中文音频和字幕，方便做分发和内容提炼。</p><br/><p>04<br>几何画板 Agent 的开发工作继续推进，完成了不少基础设施和功能模块的搭建。现在团队化开发已经初见成效，大家分工协作，效率提升明显。</p><p>11 月用户量也是稳定直线增长中，目前已经又 9000+ 用户了，其中 3000 多人都用过我们的 AI 生成图形 功能了，需求量挺高。（继续打磨 AI 准确率和成本呀）</p><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>去了趟圳面基和精进瑜伽，天气是真好，感觉比杭州更忙碌的一个城市。以及，这里的瑜伽名师也多多了，进到课堂我就成了最菜的（笑）。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175507_406_425.jpg"></p><br/><p>02<br>瑜伽又进步了一个台阶。肩背力量能明显感受到支撑向上，而不是之前的只能向下掉硬撑了。</p><p>我现在甚至在想或许很多做不到的体式，并不是因为我的肩背力量不足，其实我的髋和背伸展度都挺好，但是倒置的不熟悉让我们觉得我不可能城主手倒立等体式。</p><p>可是好几次其实老师并没有给力量上的辅助，只是方向上的，我可以一直呆下去。</p><p>我需要重新认识这点上的身体感知。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175509_407_425.jpg"></p><br/><p>03<br>医美体验新知识：</p><ol><li><p>原来水光可以不敷麻药，疼痛感不仅和针孔深度有关，还和设备的好坏有关，好的设备和枕头可以很小并且很稳，那么确实不会有太大疼痛感。虽然但是，还是疼的，医生做完问我下次还敢不敢…</p></li><li><p>fotona 虽然也是光电类，但是因为激光的原理不同，体感一点疼痛都没有，就是热乎乎的，蛮好受的。</p></li></ol><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175719_408_425.jpg"></p><br/><p>04<br>吃到了好多好吃的，又是减肥不成功的一个月 ┑(￣ Д ￣)┍。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202180052_410_425.jpg"></p><br/><p>05<br>给家里布置鲜花的美好，秋末初冬交替时的每一个暖洋洋的日子都值得珍惜。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202180039_409_425.jpg"></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/02/202511/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从 GPT-5 Unified 系统设计中学到的工程精髓</title>
      <link>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/</link>
      <guid>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/</guid>
      <pubDate>Fri, 28 Nov 2025 03:00:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;strong&gt;—— 如何把“推理能力”变成可控、可调度、可扩展的系统能力?&lt;/strong&gt;&lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;过去一年，“推理</description>
        
      
      
      
      <content:encoded><![CDATA[<p><strong>—— 如何把“推理能力”变成可控、可调度、可扩展的系统能力?</strong></p><br/><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>过去一年，“推理模型”成为大模型竞赛的核心战场。</p><p>模型不再只追求更大的参数量和更高的吞吐，而是开始竞争：如何让模型在需要时愿意“想久一点”，在合适的时机“想对一点”。</p><br/><p>OpenAI 在 GPT-5 Unified 中提出了一套非常务实的路线，</p><p><strong>把“推理”从模型本身的属性，抽象成整个系统的调度能力。</strong></p><br/><p>这篇文章将从工程角度拆解 GPT-5 Unified 的关键机制，并总结对开发者具有可迁移性的思维方法。</p><p>读完这篇文章，你将了解 GPT-5 又快又稳的推理能力是如何实现的。</p><br/><br/><h2 id="两个大类：推理时技术-vs-训练时技术"><a href="#两个大类：推理时技术-vs-训练时技术" class="headerlink" title="两个大类：推理时技术 vs. 训练时技术"></a>两个大类：推理时技术 vs. 训练时技术</h2><p>推理模型的所有技术路线本质上分成两大类——训练时技术与推理时技术。</p><p>这是理解整个 GPT-5 Unified 的基础。</p><h3 id="推理时技术（Inference-time）"><a href="#推理时技术（Inference-time）" class="headerlink" title="推理时技术（Inference-time）"></a>推理时技术（Inference-time）</h3><p>不改变模型参数，通过外部策略让模型“临时深想”。即插即用、效果马上见效，但每次会更耗时、更烧钱。</p><p>典型方法包括：</p><ul><li>Chain-of-Thought（一步步想）</li><li>Few-shot CoT（示例带思路）</li><li>Best-of-N（取最优解）</li><li>Self-Consistency</li><li>Beam Search &#x2F; MCTS（搜索推理路径）</li><li>PRM&#x2F;Verifier 重打分（过程监督）</li></ul><br/><h3 id="训练时技术（Training-time）"><a href="#训练时技术（Training-time）" class="headerlink" title="训练时技术（Training-time）"></a>训练时技术（Training-time）</h3><p>改变模型权重，让“推理习惯”被固化进模型内部。训练成本高，但推理期更快、更稳、更便宜。</p><p>包括：</p><ul><li>SFT（带思维链的监督微调）</li><li>RLHF &#x2F; RLVR（奖励正确过程与结果）</li><li>Process Reward Model（逐步打分）</li><li>内化搜索（让搜索习惯融入权重）<br/></li></ul><h3 id="这两个层是正交的"><a href="#这两个层是正交的" class="headerlink" title="这两个层是正交的"></a>这两个层是正交的</h3><p>前者是“租算力换思考”，后者是“买算力换思考”。</p><p>GPT-5 Unified 把两者结合，形成了可自由伸缩的“推理能力服务体系”。</p><br/><br/><h2 id="一个关键元维度：是否更新参数"><a href="#一个关键元维度：是否更新参数" class="headerlink" title="一个关键元维度：是否更新参数"></a>一个关键元维度：是否更新参数</h2><p>所有推理能力的分化，都来自一个黄金标准：<strong>这一步，是不是让模型权重改变了？</strong></p><p>为什么这个维度如此重要？</p><ul><li>不更新参数（Frozen）：无需训练资源，快速部署；但每次都要“租时间”。</li><li>更新参数（Updated）：训练期投入大；但推理期“花一次、用很久”。</li></ul><p><strong>小团队更倾向推理时增强；有算力和数据的团队会做训练时内化。</strong></p><p>GPT-5 Unified 的真正创新就是吧这两者结合起来了，组合成稳定、可扩展的服务。</p><br/><br/><h2 id="双模型协同：把“快答”和“深思”拆解为两条路径"><a href="#双模型协同：把“快答”和“深思”拆解为两条路径" class="headerlink" title="双模型协同：把“快答”和“深思”拆解为两条路径"></a>双模型协同：把“快答”和“深思”拆解为两条路径</h2><p>GPT-5 Unified 的结构要点是<strong>两类模型解耦演化</strong>：</p><table><thead><tr><th>模型</th><th>主要职责</th><th>设计取向</th></tr></thead><tbody><tr><td><strong>GPT-5 Main</strong></td><td>覆盖多数常规与工具任务</td><td>快、稳、低成本，限制长链式推理</td></tr><tr><td><strong>GPT-5 Thinking</strong></td><td>复杂逻辑、数学与代码推理</td><td>长思维链、过程监督、对复杂问题更鲁棒</td></tr></tbody></table><br/><p>这不是 MoE，而是更像大型互联网系统中的“双引擎”架构：</p><p>一个负责日常请求</p><p>一个负责复杂事务和高价值任务</p><p>二者解耦，独立迭代。</p><p>为什么要分两个模型？</p><p>因为推理模型内部强化了“慢思考”，如果统一在一个模型中，会导致全局降速并抬高成本。</p><p>这个思想极具工程审美：<strong>让快的更快，让慢的更稳。</strong></p><br/><br/><h2 id="Fast-Router：把推理能力变成“调度决策”"><a href="#Fast-Router：把推理能力变成“调度决策”" class="headerlink" title="Fast Router：把推理能力变成“调度决策”"></a>Fast Router：把推理能力变成“调度决策”</h2><p>Router 是 GPT-5 Unified 的灵魂。</p><p>它的工作不是简单二选一，而是三层判断：</p><ul><li>安全性判定</li><li>复杂度判定（是否需要推理）</li><li>成本预算判定（用户是否付费 &#x2F; 是否允许 Pro）</li></ul><p>Router 会在 5ms 内完成判读，然后把请求派发给 Main 或 Thinking。</p><p>这个理念在工程上极具启发意义：</p><p><strong>推理能力不是“模型决定”，而是“系统决定”。</strong></p><p><strong>推理是一项资源，而不是一个行为。</strong></p><p>这为企业级模型部署提供了非常明确的路径：用 Router 控制成本、性能与准确率的平衡。</p><br/><br/><h2 id="安全策略：从“输入过滤”到“输出整形”"><a href="#安全策略：从“输入过滤”到“输出整形”" class="headerlink" title="安全策略：从“输入过滤”到“输出整形”"></a>安全策略：从“输入过滤”到“输出整形”</h2><p>GPT-5 对安全做了一个根本性转向：</p><p>旧模式：过滤输入 → 拒绝回答（非常影响体验）</p><p>新模式：接受输入 → 重写输出（在思想链后再重写）</p><p>这依赖两点：</p><ul><li>推理模型在 RL 时，奖励函数包含“安全性”项</li><li>输出层有单独的安全监控器进行重构</li></ul><p>Safety 不再是“不让你问”，而是“无论你问什么，我都会给出可行的、安全的信息”。</p><p>这是一次极重要的工程思想转变：安全模块从“阻断器”变成了“重写器”。</p><p>对企业研发而言，这解决了“高能力模型为什么容易变危险”的结构性矛盾。</p><br/><br/><h2 id="Thinking-Pro：推理时能力的上限"><a href="#Thinking-Pro：推理时能力的上限" class="headerlink" title="Thinking Pro：推理时能力的上限"></a>Thinking Pro：推理时能力的上限</h2><p>Pro 模式的设计非常具有“算法工程化”的美感：</p><ul><li>在 Thinking 模型上再套一层推理时增强</li><li>使用 MCTS + Self-Consistency</li><li>让推理链探索更深</li><li>预算越高，答案越准</li></ul><p>这就是 Test-time Compute Scaling：把推理时间转化为服务等级。</p><p>你给模型多少时间，它就给你多少能力。</p><p>你买多少预算，它就提供多少深度。</p><p>一个能力、两种售卖方式：</p><ul><li>普通 Thinking（轻量）</li><li>Pro（重推理）</li></ul><p>站着挣钱，优雅，实在优雅。</p><br/><br/><h2 id="总结-GPT-5-给我的工程启示"><a href="#总结-GPT-5-给我的工程启示" class="headerlink" title="总结 GPT-5 给我的工程启示"></a>总结 GPT-5 给我的工程启示</h2><ol><li><strong>不要追求“万能模型”，要追求“可调度能力”</strong></li></ol><p>把不同能力拆分成不同模型，通过 Router 动态调度，保持可扩展性和可控性。</p><ol start="2"><li><strong>把推理能力做成“可伸缩资源”</strong></li></ol><p>预算有限时用推理时增强；预算充足时用训练时增强；通过 Pro 提供可扩展上限。</p><ol start="3"><li><strong>安全永远放在输出端，而不是输入端</strong></li></ol><p>拒绝用户不如重写用户。<br>用户体验和安全性的最佳平衡点，就在这里。</p><ol start="4"><li><strong>训练与推理可以不是对立，是一个连续体</strong></li></ol><p>推理时增强是训练的“延伸”；训练时内化是推理的“沉淀”。</p><br/><p>GPT-5 Unified 把这两者做成了一个高度协同的系统。</p><p>总的来说，它确实做到了：</p><ul><li>在简单问题上<strong>不过度思考</strong>；</li><li>在复杂问题上<strong>充分思考</strong>；</li><li>在敏感问题上<strong>安全地表达</strong>；</li><li>在极难问题上<strong>按预算扩展</strong>。</li></ul><p>要么在 prompt 里偷时间，要么在训练里买时间，要么在推理时租时间。</p><p>2026 年的竞赛，比的不再是“谁参数多”，而是“谁会让模型花更久想得更深”。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><a href="https://openai.com/index/learning-to-reason-with-llms/">OpenAI: Learning to reason with LLMs</a></li><li><a href="https://openai.com/index/gpt-5-system-card/">OpenAI: GPT-5 System Card</a></li><li><a href="https://openai.com/index/openai-o1-system-card/">OpenAI：OpenAI o1 System Card</a></li><li><a href="https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf">PPT: LLM-Reasoning-Stanford-CS-25</a></li><li><a href="https://arxiv.org/abs/2410.18982">Paper: O1 Replication Journey</a></li><li><a href="https://cdn.openai.com/pdf/be60c07b-6bc2-4f54-bcee-4141e1d6c69a/gpt-5-safe_completions.pdf">Paper: Safe-completions technical report</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从顶流开源 Kimi K2-Thinking 学习：什么是推理模型？</title>
      <link>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/</link>
      <guid>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/</guid>
      <pubDate>Fri, 21 Nov 2025 00:00:20 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：推理模型，为什么值得我们关注&quot;&gt;&lt;a href=&quot;#引言：推理模型，为什么值得我们关注&quot; class=&quot;headerlink&quot; title=&quot;引言：推理模型，为什么值得我们关注&quot;&gt;&lt;/a&gt;引言：推理模型，为什么值得我们关注&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：推理模型，为什么值得我们关注"><a href="#引言：推理模型，为什么值得我们关注" class="headerlink" title="引言：推理模型，为什么值得我们关注"></a>引言：推理模型，为什么值得我们关注</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112103.png"></p><p>在开源模型阵营中，大佬 Kimi K2 Thinking（以下简称“K2‑Thinking”）的崛起为推理模型带来了优秀学习样本。</p><p><strong>“推理模型”到底是什么？它与我们熟悉的传统大型语言模型（LLM）有什么根本不同？</strong></p><p>在信息爆炸、任务越来越复杂的时代，仅靠“训练好一个大模型、输入–输出”已经难以满足：比如依赖多步逻辑、实时工具调用、环境反馈循环，这些场景里传统 LLM 往往容易漂移、跳步或卡顿。</p><p>而<strong>推理模型强调：从多个角度思考分析、多步推导、根据环境变化调整路径</strong>。</p><p>在这方面，K2‑Thinking 正是一个很典型的代表：它公开了技术路线，强调“思考 + 工具调用 +长流程”能力，这为我梳理“什么是推理模型”提供了一个很棒的资料。</p><br/><br/><h2 id="什么是推理模型？与传统-LLM-的关键区别"><a href="#什么是推理模型？与传统-LLM-的关键区别" class="headerlink" title="什么是推理模型？与传统 LLM 的关键区别"></a>什么是推理模型？与传统 LLM 的关键区别</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112102.png"></p><p>最重要的来了，推理模型到底具备哪些关键特性，能够让它在复杂任务中脱颖而出呢？</p><h3 id="1-链式推理（Chain-of-Thought）"><a href="#1-链式推理（Chain-of-Thought）" class="headerlink" title="1. 链式推理（Chain-of-Thought）"></a>1. <strong>链式推理（Chain-of-Thought）</strong></h3><p>链式推理是推理模型的核心特性之一，它指的是模型能够像人类一样<strong>逐步思考</strong>，而不是直接给出答案。</p><p>举个简单的例子，假设你在解决一个数学问题，推理模型不会只是直接计算出结果，而是会先拆解问题，逐步推导出解答过程，最后给出完整的答案。</p><p>这种方式能够有效避免传统模型“跳过步骤”导致的错误。</p><br/><h3 id="2-工具调用（Tool-Calling）"><a href="#2-工具调用（Tool-Calling）" class="headerlink" title="2. 工具调用（Tool Calling）"></a>2. <strong>工具调用（Tool Calling）</strong></h3><p>工具调用是推理模型的一项重要能力。</p><p>传统的大语言模型只依赖训练数据和内部知识库，而推理模型则能够<strong>主动调用外部工具</strong>来辅助完成任务。</p><p>比如，它不仅能进行搜索，还能执行代码，调取数据库中的信息，甚至访问最新的网络资源。</p><p>在解答一个问题时，推理模型不仅仅依赖“它知道的”，而是能够实时与世界互动，获取最新的有用信息。</p><br/><h3 id="3-自我反思（Self-Reflection）"><a href="#3-自我反思（Self-Reflection）" class="headerlink" title="3. 自我反思（Self-Reflection）"></a>3. <strong>自我反思（Self-Reflection）</strong></h3><p>在推理过程中，推理模型还具备<strong>自我反思</strong>的能力。</p><p>当它在执行任务时，它能够检查自己的推理过程，发现其中的漏洞并进行修正。</p><p>就像你在解数学题时，不仅仅是盯着结果，而是不断回顾每一步的推理过程，确保每个步骤都无误。</p><p>推理模型的这种能力，可以大大提高任务的准确性和可靠性。</p><br/><h3 id="4-长程推理（Long-Horizon-Reasoning）"><a href="#4-长程推理（Long-Horizon-Reasoning）" class="headerlink" title="4. 长程推理（Long-Horizon Reasoning）"></a>4. <strong>长程推理（Long-Horizon Reasoning）</strong></h3><p>长程推理指的是模型能够处理<strong>多轮推理</strong>，并且在整个推理过程中保持一致的思路。</p><p>它能够记住前面推理过程中发生的关键步骤，并且根据这些步骤来调整后续的决策。</p><p>比如，在长时间的决策过程中，推理模型能够从多个方面考虑，并一步步推进，直到问题得到完全解决。</p><br/><br/><h2 id="K2‑Thinking-的创新突破"><a href="#K2‑Thinking-的创新突破" class="headerlink" title="K2‑Thinking 的创新突破"></a>K2‑Thinking 的创新突破</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112101.png"></p><p>K2‑Thinking 作为开源推理模型的代表，为我们展示了推理能力在实际应用中的潜力。</p><h3 id="1-长时间自主推理（Long-Horizon-Agency）"><a href="#1-长时间自主推理（Long-Horizon-Agency）" class="headerlink" title="1. 长时间自主推理（Long-Horizon Agency）"></a><strong>1. 长时间自主推理（Long-Horizon Agency）</strong></h3><p>传统的大型语言模型（LLM）通常在面对多步骤任务时会“漂移”，在执行 30-50 步之后就容易失去逻辑连贯性。</p><p>而 K2‑Thinking 设计成一个能够持续进行 200-300 次连续工具调用且保持思路一致的“思考代理”。</p><p>这种能力让它能够完成复杂问题的推理，而不仅仅是简单的回答问题。</p><p>在一个演示中，K2‑Thinking 通过 23 次推理和工具调用，解决了一个博士级别的数学问题，展示了它在长时间、复杂任务中的自主推理能力。</p><br/><h3 id="2-测试时扩展（Test-Time-Scaling）"><a href="#2-测试时扩展（Test-Time-Scaling）" class="headerlink" title="2. 测试时扩展（Test-Time Scaling）"></a><strong>2. 测试时扩展（Test-Time Scaling）</strong></h3><p>K2‑Thinking 不像传统模型那样固定计算每个查询，而是采用 <strong>测试时扩展</strong> 的方式。</p><p>在遇到复杂任务时，它会“思考更多”，通过递归循环不断优化问题的解决路径：</p><ul><li>思考：分解问题。</li><li>行动：调用外部工具（如搜索引擎、代码解释器等）。</li><li>观察：获取工具输出。</li><li>重新评估：分析新信息并调整方案。</li></ul><p>这种递归式的推理过程使得 Kimi 可以在多轮的推理和工具调用中，始终保持清晰的思路和目标。</p><br/><h3 id="3-高效的-MoE（Mixture-of-Experts）架构"><a href="#3-高效的-MoE（Mixture-of-Experts）架构" class="headerlink" title="3. 高效的 MoE（Mixture-of-Experts）架构"></a><strong>3. 高效的 MoE（Mixture-of-Experts）架构</strong></h3><p>虽然 K2‑Thinking 拥有 1 万亿参数，但它采用了高效的 Mixture-of-Experts (MoE) 架构，在每次推理时只激活其中的 32 亿参数。</p><p>这样的“稀疏”设计让模型在拥有大量知识的同时，保持低成本的推理效率。</p><p>由于其高效设计，K2‑Thinking 可以在较为普通的硬件上运行，例如两台 M3 Ultra Mac Studios，极大地降低了运行成本和对硬件的依赖。</p><br/><h3 id="4-原生-INT4-量化加速"><a href="#4-原生-INT4-量化加速" class="headerlink" title="4. 原生 INT4 量化加速"></a><strong>4. 原生 INT4 量化加速</strong></h3><p>K2‑Thinking 采用 <strong>原生 INT4 量化</strong> 技术，将模型的权重压缩到 4 位，带来 <strong>2 倍的推理速度</strong> 提升和大幅度的内存减少。</p><p>这使得它能够在性能和成本之间实现最佳平衡，适合更多的应用场景。</p><br/><h3 id="5-优异的多步骤推理表现"><a href="#5-优异的多步骤推理表现" class="headerlink" title="5. 优异的多步骤推理表现"></a><strong>5. 优异的多步骤推理表现</strong></h3><p>在 <strong>Humanity’s Last Exam (HLE)</strong> 和 <strong>BrowseComp</strong> 等基准测试中，K2‑Thinking 超越了 GPT-5 和 Claude，展示了它在 <strong>工具调用</strong> 和 <strong>多步骤推理</strong> 任务中的卓越表现。</p><p>例如，Kimi 在 HLE 测试中获得 44.9%，优于 GPT-5 的 41.7%。在网页搜索任务 BrowseComp 中，Kimi 获得了 60.2%，大幅领先于 GPT-5 的 54.9%。</p><br/><h3 id="6-低成本训练和高效计算"><a href="#6-低成本训练和高效计算" class="headerlink" title="6. 低成本训练和高效计算"></a><strong>6. 低成本训练和高效计算</strong></h3><p>K2‑Thinking 的训练成本仅为 <strong>460 万美元</strong>，远低于 GPT-4（~7800 万美元）和 Gemini Ultra（1.91 亿美元）。</p><p>这使得 Kimi 的成本效益成为行业的颠覆性力量，证明了通过高效算法优化可以挑战资本密集型的 AI 计算模式。</p><p>Moonshot 通过创新的 <strong>Muon 优化器</strong> 和 <strong>多头潜在注意力（MLA）</strong> 进一步提升了计算效率，使得每一美元的计算支出都能获得更多的智能输出。</p><br/><h3 id="7-开源与商业模式创新"><a href="#7-开源与商业模式创新" class="headerlink" title="7. 开源与商业模式创新"></a><strong>7. 开源与商业模式创新</strong></h3><p>K2‑Thinking 采用了 <strong>修改版 MIT 许可协议</strong>，这一许可不仅允许研究人员、初创公司和企业进行商业化使用，还通过 <strong>要求商业产品显示 Kimi K2</strong> 来确保对该技术的认知和贡献。</p><p>与大部分限制性商业许可不同，Kimi 通过开放权重和宽松的许可协议，推动了 AI 技术的社区创新。</p><p>这种开放的方式挑战了现有的高 API 收费模式，给开发者带来了 <strong>低成本、开源竞争力的替代品</strong>。</p><p>相较于依赖高收费 API 的传统 AI 模型，Kimi 提供了一个几乎免费的高性能替代方案。</p><br/><h3 id="8-打破“计算壁垒”：算法为先，资本为后"><a href="#8-打破“计算壁垒”：算法为先，资本为后" class="headerlink" title="8. 打破“计算壁垒”：算法为先，资本为后"></a><strong>8. 打破“计算壁垒”：算法为先，资本为后</strong></h3><p>K2‑Thinking 打破了 <strong>“计算壁垒”</strong> 的传统观念，挑战了“大模型需要巨额资本支撑”的说法。</p><p>通过算法优化，Kimi 展现了计算效率的突破，使得较低预算的团队也能开发和部署强大的推理模型。</p><p>这种效率革命使得高端 AI 模型的门槛降低，行业竞争格局发生了根本性变化。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>推理模型正在成为下一代 AI 应用的新基建。</p><p>K2‑Thinking 作为一个开源且具备实战能力的代表，显示出国产模型在“推理能力”维度也有突破。</p><p>现在能看到，<strong>越来越多新产品从“简单生成”转向“复杂行动＋思考＋工具协同”。</strong></p><p>期待国内人工智能的生态越做越好，越来越成熟 ！</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><a href="https://moonshotai.github.io/Kimi-K2/thinking.html">Introducing Kimi K2 Thinking 技术博客（Moonshot AI）</a></li><li><a href="https://www.shuttle.dev/blog/2025/11/17/kimi-k2-thinking-hands-on-review">Kimi K2 Thinking: Testing the Open-Source Reasoning Model on Real Code</a></li><li><a href="https://c3.unu.edu/blog/the-agent-has-landed-why-kimi-k2-thinking-is-more-than-just-a-new-ai-model">Why Kimi K2 Thinking Is More Than Just a New AI Model</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>为什么李飞飞说：AI 真正的进步取决于世界模型</title>
      <link>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/</link>
      <guid>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/</guid>
      <pubDate>Wed, 19 Nov 2025 00:00:25 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近，“人工智能之母”李飞飞发布了新产品 &lt;strong&gt;Marble&lt;/strong&gt;——一个可以用一句话生成完整 3D 场景、可探索、可</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近，“人工智能之母”李飞飞发布了新产品 <strong>Marble</strong>——一个可以用一句话生成完整 3D 场景、可探索、可编辑的世界模型原型。</p><p>我花了整个周末把访谈、演示与背景研究都看完，<br>再回头想想我过去几年在做的几何 AI、空间计算、Agent 系统……<br>意识到一个很深的事：</p><p><strong>世界模型不是一个功能升级，而是下一代智能的底层逻辑。</strong></p><p>但与其说我们离 AGI 又近了一步，不如说，我们可能再一次看到“世界模型时代”的新起点。</p><br/><br/><h2 id="为什么世界模型突然被重提？"><a href="#为什么世界模型突然被重提？" class="headerlink" title="为什么世界模型突然被重提？"></a>为什么世界模型突然被重提？</h2><p>因为所有人都发现了一个共同的瓶颈：<br><strong>语言大模型无法突破“世界理解”。</strong></p><p>它们能说、能编、能解释、能写论文……</p><p>但一旦进入真实世界场景——空间、物体、动态、因果就频繁翻车：</p><ul><li>看不懂遮挡</li><li>分不清前后关系</li><li>无法从二维视频推断三维结构</li><li>对物理规律毫无概念</li><li>机器人操作路线像在瞎撞</li><li>视频生成 3 秒开始“世界解体”</li></ul><br/><p>LLM 的本质是 <strong>按语言统计模式预测文本</strong>。</p><p>而物理世界不是语言，它是空间、物体、动力学、连续性、约束和因果的组合系统。</p><p>语言模型建不出这个系统。</p><p>于是“世界模型”再次成为前沿的焦点，不是替代 LLM，而是补齐它的最重要短板 <strong>理解和模拟真实世界。</strong></p><br/><br/><h2 id="别急，路……长得很"><a href="#别急，路……长得很" class="headerlink" title="别急，路……长得很"></a>别急，路……长得很</h2><p>但我们不要幻觉：世界模型不是几年内就能商业化的奇迹。</p><p>李飞飞在访谈里讲了一个极其关键、但外界经常忽略的事实：</p><blockquote><p>自动驾驶从 2005 年 DARPA Grand Challenge 到现在，20 年了。</p><p>到 2025 年都还没完全落地。</p><p>而机器人，比自驾车难得多。</p></blockquote><br/><p>为什么？</p><br/><p><strong>原因一：自驾车 &#x3D; 2D 问题，机器人 &#x3D; 3D + 操作问题</strong></p><p>自驾车在二维地面移动，世界就是一个平面导航问题，它的主要目标是 <strong>不碰东西</strong>。</p><p>而机器人在三维世界操作物体，这是一个高度维度的连续控制问题，它的目标是 <strong>准确地“碰东西”</strong>。</p><p>当你要抓一个杯子，需要知道：杯子的位置、你的手的位置、碰撞边界、重力与摩擦、运动预测、遮挡中的可见性…<br>这不是“统计语言能处理的任务”。</p><br/><p><strong>原因二：难度指数级上升</strong></p><p>即便自驾的硬件+软件+数据+供应链都成熟到极致了，但还是没完全落地。</p><p>你要机器人落地，难度指数级上升：机械臂成本、伺服精度、力反馈传感器、三维视觉、安全规范、工业供应链、电池与运算成本、云端协作的可靠性 …</p><p>世界模型只是其中的一个关键环节，而非全部。</p><br/><p><strong>所以我认为，世界模型是正确方向，但绝不会在一两年内重塑社会。</strong> 这是一个十年级别的技术大周期。</p><br/><br/><h2 id="那么世界模型到底是什么？"><a href="#那么世界模型到底是什么？" class="headerlink" title="那么世界模型到底是什么？"></a>那么世界模型到底是什么？</h2><p>我看完 Marble 和这次访谈后，最深刻的变化就是：</p><p><strong>世界模型既不是“视频模型升级版”，也不是“3D 重建工具”。</strong></p><p><strong>它是一个生成性、可交互、可预测的空间模拟器</strong>。</p><p>下面我从工程角度拆三个关键误区。</p><br/><h3 id="误解-1：世界模型-x3D-更强的视频生成"><a href="#误解-1：世界模型-x3D-更强的视频生成" class="headerlink" title="误解 1：世界模型 &#x3D; 更强的视频生成"></a><strong>误解 1：世界模型 &#x3D; 更强的视频生成</strong></h3><p>不对。</p><p>视频生成解决的是帧一致性、光影与纹理和动态连续性，但它依然是<strong>像素层面的预测</strong>。</p><p>世界模型不是预测像素，而是预测：</p><ul><li>物体布局</li><li>空间结构</li><li>深度关系</li><li>物理状态</li><li>动作后果</li><li>多主体交互</li><li>能走、能转身、能碰撞的世界状态</li></ul><p>视频模型生成的是“视觉表面”，世界模型生成的是“场景语义与物理状态”。</p><p>这完全不同。</p><br/><h3 id="误解-2：世界模型-x3D-3D-重建"><a href="#误解-2：世界模型-x3D-3D-重建" class="headerlink" title="误解 2：世界模型 &#x3D; 3D 重建"></a><strong>误解 2：世界模型 &#x3D; 3D 重建</strong></h3><p>如果世界模型只是重建现实，那就没意义了。</p><p><strong>真正的世界模型是基于语言输入生成一个可探索的“虚拟物理世界”。</strong></p><p>我理解它至少需要满足以下几点：</p><ul><li>一个 prompt → 生成完整世界</li><li>世界包含路径、遮挡、可导航性</li><li>你可以走进去</li><li>你可以探索意料之外的角落</li><li>你可以修改它</li></ul><p>这比 3D 建模工具强太多。</p><br/><h3 id="误解-3：世界模型只是视觉系统"><a href="#误解-3：世界模型只是视觉系统" class="headerlink" title="误解 3：世界模型只是视觉系统"></a>误解 3：世界模型只是视觉系统</h3><p>不，<strong>它是 “行动智能” 的前置层</strong>。</p><p>如果 AGI 最终要在世界中执行动作（agent &#x2F; robot &#x2F; embodied intelligence），那么它必须知道：</p><ul><li>如果我推一下这个盒子会怎样</li><li>如果我走两步会撞到谁</li><li>如果我换一个视角能看到墙后什么</li></ul><p>语言模型做不到这些。</p><br/><p><strong>世界模型 &#x3D; AI 的世界状态表示层（world state representation）</strong></p><p>它不是为了生成漂亮的画面，而是为了让模型知道自己在哪里、在做什么、接下来可能发生什么。</p><br/><br/><h3 id="Marble-为什么是一个重要的标志？"><a href="#Marble-为什么是一个重要的标志？" class="headerlink" title="Marble 为什么是一个重要的标志？"></a>Marble 为什么是一个重要的标志？</h3><p>不是因为它能“做 3D”，</p><p>而是它证明了世界模型的三个核心能力：</p><br/><p><strong>能力一：世界结构可生成</strong></p><p>不是 mesh 拼贴，而是真正的结构化世界：可行走、有空间层级、有碰撞逻辑、多视角一致。</p><p>这在模型层面意味着需要在 latent space 内表示“世界状态”。</p><br/><p><strong>能力二：世界可重建 + 可编辑</strong></p><p>这意味着：</p><ul><li>世界表示是显性的（explicit）</li><li>状态之间是连续的（stateful）</li><li>可以“世界状态 → 修改 → 再渲染”</li></ul><p>这非常接近游戏引擎内部的数据结构。</p><br/><p><strong>能力三：世界可探索</strong></p><p>这是最关键的突破。</p><p>如果你能自由移动摄像机、走进去、转视角，意味着模型内部必须有“空间一致性”（spatial coherence）和“三维世界的持久性”（persistence）。</p><p>这是第一次有模型具备这种能力。</p><br/><br/><h2 id="真正的革命在哪里？"><a href="#真正的革命在哪里？" class="headerlink" title="真正的革命在哪里？"></a>真正的革命在哪里？</h2><p>我认为世界模型会沿着“由易到难”的路径影响技术。</p><br/><p><strong>阶段一</strong>：</p><p>创意行业（已开始），这些都已经能玩。</p><ul><li>虚拟影棚</li><li>游戏关卡自动生成</li><li>场景草图 → 3D 世界</li><li>教育模拟世界</li><li>AR&#x2F;VR 内容生产</li></ul><br/><p><strong>阶段二</strong>：</p><p>模拟与科学计算（中期），这部分需要更数学化、更结构化的世界模型。</p><ul><li>物理规则推演</li><li>世界状态预测</li><li>动态优化任务</li><li>机器人策略模拟</li><li>多主体交互的虚拟环境</li></ul><br/><p><strong>阶段三</strong>：</p><p>机器人与具身智能（长期），这是最难的领域。</p><ul><li>空间理解</li><li>动作规划</li><li>能力校准</li><li>实时感知</li><li>多模态世界状态对齐</li><li>连续控制策略</li></ul><p>世界模型只是其中一环。<br>硬件、算力、传感器、数据都必须一起进化。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果世界模型真能在未来十年成熟，它可能会成为人类第一次接近 <strong>重写生物智能基线</strong> 的尝试。</p><p>但站在今天回头看，我们依然不得不承认：</p><p>一个婴儿跌跌撞撞学走路时，他体内的世界模型，对重力的直觉、对遮挡的理解、对物体持久性的把握、对路径的预测，其学习效率、鲁棒性、泛化能力，都远在任何人工模型之上。</p><p>我们用数十亿帧视频训练“空间一致性”，自然界却用几个月的探索让孩子具备稳定的三维世界感知。</p><p>我们为状态持久性、渲染一致性、物理约束痛苦堆砌 priors，生物神经系统却将它们作为默认配置。</p><p><strong>世界模型之难，也正因此而迷人。</strong></p><p>它不是明年就会落地的革命，</p><p>不是可以替代所有智能的银弹，</p><p>而是一条明确但漫长的技术长坡。</p><p>好消息是：我们终于知道坡在哪；</p><p>更好的消息是：我们正站在坡的起点。</p><p>而在每一个技术突破的间隙，我们也会不断重新意识到：<strong>生物智能本身，就是我们正在努力复现的那套终极世界模型。</strong></p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><p>从文字到世界：空间智能是 AI 的下一个前沿<br><a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence">drfeifei.substack.com&#x2F;p&#x2F;from-words-to-worlds-spatial-intelligence</a></p></li><li><p>李飞飞在 X 上关于 AI 的言论<br><a href="https://x.com/drfeifei/status/963564896225918976">x.com&#x2F;drfeifei&#x2F;status&#x2F;963564896225918976</a></p></li><li><p>The Godmother of AI on jobs, robots &amp; why world models are next | Dr. Fei-Fei Li<br><a href="https://youtu.be/Ctjiatnd6Xk?si=J1Gdz3lnFiaKbF9y">youtu.be&#x2F;Ctjiatnd6Xk?si&#x3D;J1Gdz3lnFiaKbF9y</a></p></li><li><p>When Do Neural Networks Learn World Models?<br><a href="https://arxiv.org/abs/2502.09297">arxiv.org&#x2F;abs&#x2F;2502.09297</a></p></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Claude Multi-Agent 的核心经验精华（面向工程与产品）</title>
      <link>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/</link>
      <guid>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/</guid>
      <pubDate>Fri, 14 Nov 2025 00:00:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近读了 Claude 团队做 Research 功能的工程文章，感觉被点醒了。&lt;/p&gt;
&lt;p&gt;多智能体并不是我们想象的“多模型乱跑”，而是一套非常讲究实战经验的工程体系。&lt;/p&gt;
&lt;br/&gt;

&lt;p&gt;把我觉得最值钱的点分享出来，算是备忘，也给正在做 Agent 的朋友一些</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近读了 Claude 团队做 Research 功能的工程文章，感觉被点醒了。</p><p>多智能体并不是我们想象的“多模型乱跑”，而是一套非常讲究实战经验的工程体系。</p><br/><p>把我觉得最值钱的点分享出来，算是备忘，也给正在做 Agent 的朋友一些灵感。</p><p>我把那些读完后想立刻拿来用的部分整理成了下面这 12 条思考。</p><br/><br/><h2 id="01-多智能体的终极价值：扩大-token-x3D-扩大智力"><a href="#01-多智能体的终极价值：扩大-token-x3D-扩大智力" class="headerlink" title="01. 多智能体的终极价值：扩大 token &#x3D; 扩大智力"></a>01. 多智能体的终极价值：扩大 token &#x3D; 扩大智力</h2><p>Claude 的团队给出的最本质 insight：</p><blockquote><p>多智能体 &#x3D; 安全地扩大 Token、上下文、探索路径的规模。</p><p>Token 消耗解释了 80% 的性能差异。</p></blockquote><p>也就是说：</p><p>单智能体的局限是 线性推理 + 有限上下文。</p><p>多智能体通过 并行上下文窗口 → 撑大推理深度与覆盖面积</p><p>这比提升模型本身更具收益（Sonnet 3.7 → 4 不如多智能体带来的收益）</p><br/><br/><h2 id="02-最适合多智能体的任务：高并行-信息巨量-方向不确定"><a href="#02-最适合多智能体的任务：高并行-信息巨量-方向不确定" class="headerlink" title="02. 最适合多智能体的任务：高并行 + 信息巨量 + 方向不确定"></a>02. 最适合多智能体的任务：高并行 + 信息巨量 + 方向不确定</h2><p>Claude 总结多智能体真正的 sweet spot：</p><p>✔ 开放式研究</p><p>✔ 多方向并行探索</p><p>✔ 信息分散、来源多样</p><p>✔ 工具链复杂</p><p>✔ 单一 agent 无法“一次性抓全”的任务</p><br/><p>不适合：</p><p>✘ 代码生成（依赖共享上下文）</p><p>✘ 强依赖一致状态的任务（你必须 every turn 同步）</p><br/><br/><h2 id="03-Orchestrator-→-Subagents，是目前最稳的架构"><a href="#03-Orchestrator-→-Subagents，是目前最稳的架构" class="headerlink" title="03.Orchestrator → Subagents，是目前最稳的架构"></a>03.Orchestrator → Subagents，是目前最稳的架构</h2><p>这是 Claude 的生产结论：</p><blockquote><p>主智能体 orchestrate</p><p>子智能体 parallel explore</p><p>Lead 聚合、校验、再规划</p></blockquote><p>自由协作的多智能体会发疯，有“主智能体 orchestration + 子智能体执行”能稳定落地。</p><p>原因：</p><ul><li><p>控制爆炸性增长</p></li><li><p>容易设任务边界</p></li><li><p>清晰的错误恢复机制</p></li></ul><br/><br/><h2 id="04-工具-x3D-智能体的世界边界-——-工具设计比提示更重要"><a href="#04-工具-x3D-智能体的世界边界-——-工具设计比提示更重要" class="headerlink" title="04. 工具 &#x3D; 智能体的世界边界 —— 工具设计比提示更重要"></a>04. 工具 &#x3D; 智能体的世界边界 —— 工具设计比提示更重要</h2><p>Claude 的一个很核心观点：</p><blockquote><p>工具就是智能体的世界模型。</p><p>工具描述不清 &#x3D; 智能体理解现实错误。</p></blockquote><p>他们发现：</p><p>工具描述稍微差一点，agent 会持续犯错。</p><p>改好工具描述，性能可提升 40%。</p><br/><p>工具需要：</p><p>一句话说清楚用途（“解决什么问题”）。</p><p>明确输入类型、输出结构。</p><p>明确错误提示。</p><p>明确边界。</p><br/><p>我们写工具的时候自以为很清楚，但对于模型来说，模糊一点点它都会理解成完全不同的事情。</p><p>Anthropic 团队甚至专门做了一个工具文档自动优化智能体。<br>光是重写工具描述就能让任务完成速度提升 40%。</p><p>所以工具不是提供一个 API，而是为智能体定义一个可理解的物理世界。</p><br/><br/><h2 id="05-Prompt-的重点不是“指令”，而是“合作框架”"><a href="#05-Prompt-的重点不是“指令”，而是“合作框架”" class="headerlink" title="05. Prompt 的重点不是“指令”，而是“合作框架”"></a>05. Prompt 的重点不是“指令”，而是“合作框架”</h2><p>很多人会把 prompt 写成执行流程：“你应该这样做：A → B → C”</p><p>Anthropic 团队指出你不该告诉智能体：“怎么做”，你应该告诉它们“角色、目标、边界、资源预算”。</p><p>即：多智能体系统最有效的 prompts 是 “<strong>协作准则 + 协作结构</strong>”。</p><p>包括：</p><ul><li>如何划分任务</li><li>什么时候扩展方向</li><li>怎么评估来源质量</li><li>子智能体之间不互相重复</li><li>什么时候该停止</li></ul><p>把智能体看成同事，而不是员工。</p><br/><br/><h2 id="06-智能体永远不知道任务难度，需要“努力预算”"><a href="#06-智能体永远不知道任务难度，需要“努力预算”" class="headerlink" title="06. 智能体永远不知道任务难度，需要“努力预算”"></a>06. 智能体永远不知道任务难度，需要“努力预算”</h2><p>特别有意思的一点：</p><blockquote><p>Agent 不知道任务是简单还是困难，你必须告诉它“要投入多少 effort”。</p></blockquote><p>比如：</p><ul><li><p>简单问题：1 个 agent + 3–10 次工具</p></li><li><p>中等问题：3–4 个 agent 并行</p></li><li><p>大复杂问题：10+ 个 agent 全线铺开</p></li></ul><p>不然 Agent 会：</p><ul><li><p>简单问题开几十个 subagent（真实发生）</p></li><li><p>大问题敷衍一下（也真实发生）</p></li></ul><p>这种 Effort Guideline 很像我们给新人做任务时说的那句“这个别搞太久”。</p><br/><br/><h2 id="07-没有观测，就没有多智能体"><a href="#07-没有观测，就没有多智能体" class="headerlink" title="07. 没有观测，就没有多智能体"></a>07. 没有观测，就没有多智能体</h2><p>他们花了大量篇幅讲 observability。</p><p>因为多智能体有个很要命的特征：</p><p>同样输入，同样 prompt，走出来的路径不一定一样。</p><br/><p>要调试这种系统，你必须能看到：</p><ul><li>每次搜索什么</li><li>为什么这么做</li><li>工具结果是什么</li><li>哪个 subagent 决策不合理</li><li>Lead 是怎么汇总结果的</li></ul><p>说白了：<br>没有 trace，就没办法 debug 多智能体。</p><p>Anthropic 团队构建了一套观测系统：全量工具调用 trace + 每一步的思考过程 + 决策链路图 。</p><br/><br/><h2 id="08-分布式上下文管理：长任务必须总结-刷新上下文"><a href="#08-分布式上下文管理：长任务必须总结-刷新上下文" class="headerlink" title="08. 分布式上下文管理：长任务必须总结 + 刷新上下文"></a>08. 分布式上下文管理：长任务必须总结 + 刷新上下文</h2><p>一句话概括：</p><p><strong>多智能体的本质，是“多个干净上下文窗口接力工作”。</strong></p><p>Claude 的策略：</p><ul><li>阶段性总结</li><li>存入 Memory</li><li>创建新子智能体继续，避免上下文爆炸</li><li>主智能体通过 Memory 保持连贯性</li></ul><p>这才能让 200k token 的限制变成可管理问题。</p><br/><br/><h2 id="09-并行工具调用才是性能提升的真王道"><a href="#09-并行工具调用才是性能提升的真王道" class="headerlink" title="09. 并行工具调用才是性能提升的真王道"></a>09. 并行工具调用才是性能提升的真王道</h2><p>他们的最终结果很夸张：</p><p>研究任务的速度，比顺序执行快了 90%。</p><p>就因为主智能体一次性创建多个 subagent，每个 subagent 内部又同时调用多个工具。</p><p>这个结构本质上把任务从“串行堆积”变成“分布式计算”。</p><br/><p>用他们的话说：</p><p>Multi-Agent 的意义不是“多个模型”，而是“把模型拆成多核并行”。</p><br/><p>三大并行：</p><ul><li>Lead 同时创建多个 Subagents</li><li>Subagents 内部同时用多个工具</li><li>工具链内部也可并行</li></ul><br/><br/><h2 id="10-评估必须是“结果优先”，而不是“过程对不对”"><a href="#10-评估必须是“结果优先”，而不是“过程对不对”" class="headerlink" title="10. 评估必须是“结果优先”，而不是“过程对不对”"></a>10. 评估必须是“结果优先”，而不是“过程对不对”</h2><p>因为多智能体每次走的路都不一样，所以 Claude 直接用 LLM-as-judge，只看：</p><ul><li>结果是否正确</li><li>覆盖是否完整</li><li>引用是否准确</li><li>工具数量是否合理</li><li>来源质量是否高</li></ul><p>这是一种对复杂系统更健康的评估方式。</p><br/><br/><h2 id="11-错误不可避免，但必须能“从中断点恢复”"><a href="#11-错误不可避免，但必须能“从中断点恢复”" class="headerlink" title="11. 错误不可避免，但必须能“从中断点恢复”"></a>11. 错误不可避免，但必须能“从中断点恢复”</h2><p>多智能体是有生命周期的。<br>一次工具报错，如果你让它重来整个任务，用户直接爆炸。</p><p>Claude 的工程经验：</p><ul><li>智能体是长生命周期、持续状态的</li><li>工具失败 → 不能重来，只能继续</li><li>系统必须保存中间状态（checkpoints）</li><li>同时智能体自身可 adapt 错误</li></ul><p>这是所有 Agent 系统都需要的“耐用性设计”。</p><br/><br/><h2 id="12-多智能体不是-prompt-玩具，是系统工程"><a href="#12-多智能体不是-prompt-玩具，是系统工程" class="headerlink" title="12. 多智能体不是 prompt 玩具，是系统工程"></a>12. 多智能体不是 prompt 玩具，是系统工程</h2><p>文章的最后一句大意是：</p><p><strong>Multi-Agent 的困难在“最后一公里”，工程化程度远超我们过去想象。</strong></p><p>我也越来越觉得：工具、可观测性、容错、执行链路、协作协议、Memory 管理、分布式上下文、并行执行，这些比“让模型聪明”更难，也更关键。</p><p>Multi-Agent 是生产级工程，它是工程系统 + 协作协议 + 工具生态。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>deepseek-ocr 的几何识别，真的成立吗？</title>
      <link>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</link>
      <guid>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</guid>
      <pubDate>Wed, 12 Nov 2025 07:55:24 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近，deepseek 又引爆了一波热度。&lt;/p&gt;
&lt;p&gt;他们新发布的 &lt;strong&gt;deepseek-ocr 模型&lt;/strong&gt;，不仅能识别文字，还号称能看懂 &lt;strong&gt;化学分子式、数学公式、几何图形&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对我这个正好在做几何图形</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近，deepseek 又引爆了一波热度。</p><p>他们新发布的 <strong>deepseek-ocr 模型</strong>，不仅能识别文字，还号称能看懂 <strong>化学分子式、数学公式、几何图形</strong>。</p><p>对我这个正好在做几何图形识别和重绘生成的人来说，这当然是个好消息。</p><p>所以我开始了一轮针对 deepseek-ocr 几何图识别的测试。</p><p>结果嘛，只能说，<strong>方向没错，但距离真正可用，还差得远。</strong></p><br/><br/><h2 id="论文里描绘的“理想图景”"><a href="#论文里描绘的“理想图景”" class="headerlink" title="论文里描绘的“理想图景”"></a>论文里描绘的“理想图景”</h2><p>在官方论文中，deepseek-ocr 展示了不少让人兴奋的例子。<br>比如它能从图片中识别出几何图形，并输出可直接渲染的图形定义：</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/2.png" class=""><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/1.png" class=""><p>看起来好像 AI 真的“理解”了几何结构。</p><p>而且论文提到，它在训练中使用了带几何、化学等多模态的专用数据集，覆盖了公式与图形的双模态信息。</p><p>理论上，这意味着：</p><p>未来我们拍一张几何题图，就能直接生成结构化定义，甚至自动画出图。</p><p>听起来很美好。</p><br/><br/><h2 id="我的测试结果：模型确实“看见了”，但没“理解”"><a href="#我的测试结果：模型确实“看见了”，但没“理解”" class="headerlink" title="我的测试结果：模型确实“看见了”，但没“理解”"></a>我的测试结果：模型确实“看见了”，但没“理解”</h2><p>我选了几张相对简单的几何图形来测试，左侧为模型识别出的代码定义，右侧为渲染结果。</p><p>case 1:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/3.png" class=""><p>case 2:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/4.png" class=""><p>case 3:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/5.png" class=""><p>case 4:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/6.png" class=""><p>case 5:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/7.png" class=""><p>case 6:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/8.png" class=""><p>case 7:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/9.png" class=""><br/><p>模型确实能识别出圆、线、点这些基础元素，但问题在于它只停留在“形似”的层面。</p><br/><br/><h2 id="存在的主要问题："><a href="#存在的主要问题：" class="headerlink" title="存在的主要问题："></a>存在的主要问题：</h2><ol><li><p><strong>元素类型太少</strong></p><p>目前几乎只能识别「圆、直线、点」三类对象。</p><p>没有弧线、角度等基础构造，复杂图形几乎全失真。</p></li><li><p><strong>没有样式信息</strong></p><p>模型输出完全忽略了虚线、颜色、标记这些样式描述，而这些恰恰是几何图表达逻辑关系的关键。</p></li><li><p><strong>缺乏约束与推理链路</strong></p><p>从上面的测试案例能很明显看到，作为 OCR 模型，它并不理解几何推理。</p><p>比如在识别一个多边形的图后，它会画出所有的边，但产生了不闭合的情况。</p><p>对它而言，交点、垂直等等只是形状，不是满足约束的结构。</p></li></ol><p>deepseek-ocr 目前的几何识别，像是一个会抄图的学生——会画样子，但不会推理。</p><br/><br/><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>虽然模型结果不理想，但我认为这是一个重要的信号：<strong>OCR 模型正在从“识别文字”走向“理解结构”。</strong></p><p>几何识别比文字识别复杂得多，它需要同时理解<strong>视觉结构</strong>和<strong>逻辑约束</strong>。</p><p>deepseek 这次的尝试，说明主流大模型团队开始意识到这一方向的重要性。</p><p>对我来说，这正好验证了我在做的另一件事——让 AI 不只是识别几何图，而是能<strong>构造 + 约束 + 验证</strong>整个几何过程。</p><p>在我开发的大角几何画板中，我们的目标不是“识别图像”，而是让 AI 具备“几何理解能力”。</p><p>当用户上传一张图时，系统不仅要知道“有圆、有直线”，还要能<strong>自动重建几何关系</strong>：</p><ul><li>哪些线是垂直的？</li><li>哪些点在同一条线上？</li><li>这两个圆相切还是相交？</li><li>哪条辅助线是解题关键？</li></ul><p>这些才是“理解”层面的能力。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>deepseek-ocr 的几何识别，还远不算成立。</p><p>但它标志着一个趋势：<strong>AI 正在学习去“看懂”图形的结构世界。</strong></p><p>也许几年后，我们真的能拍下几何题图，AI 自动识别、重构、推理、作答。</p><p>但在那之前，我们还要继续探索 <strong>几何语言、推理逻辑与可执行构造</strong> 的结合方式。</p><p>如果你也对 “AI 如何真正理解数学” 感兴趣，欢迎关注我，一起见证这场变化。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>当几何遇上 CodeAct 范式：从语言理解到可执行推理的跃迁</title>
      <link>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</link>
      <guid>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</guid>
      <pubDate>Fri, 07 Nov 2025 13:36:31 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;em&gt;面向 Agent 开发者的工程与范式探索&lt;/em&gt;&lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;引言：从“语言理解”到“执行推理”&quot;&gt;&lt;a href=&quot;#引言：从“语言理解”到“执行推理”&quot; class=&quot;headerlink&quot; title=&quot;引言：从“语言理解”到“执行</description>
        
      
      
      
      <content:encoded><![CDATA[<p><em>面向 Agent 开发者的工程与范式探索</em></p><br/><h2 id="引言：从“语言理解”到“执行推理”"><a href="#引言：从“语言理解”到“执行推理”" class="headerlink" title="引言：从“语言理解”到“执行推理”"></a>引言：从“语言理解”到“执行推理”</h2><p>在我的几何 AI 项目中，模型第一次尝试“画一个等腰三角形”时，图形看似完美，实际上两边长度并不相等。</p><p><strong>AI画得像，却没画对。</strong></p><p>几何画图任务并非自然语言理解，而是 <strong>构造 + 约束 + 验证</strong> 的闭环过程。</p><p>传统的 “Planner + DSL + Verifier” 体系虽然可控，但在动态构造与反复验证中显得笨重。</p><p>我开始思考：</p><p>如果让 AI 不再只是描述，而是<strong>直接写出代码、执行代码、并根据结果再思考</strong>呢？</p><p>这正是 CodeAct（<em>Executable Code Actions Elicit Better LLM Agents</em>）所提出的核心理念。</p><p>CodeAct 不再让语言模型“说怎么做”，而是让它“自己去做”。</p><br/><br/><h2 id="CodeAct-的底层机制解析"><a href="#CodeAct-的底层机制解析" class="headerlink" title="CodeAct 的底层机制解析"></a>CodeAct 的底层机制解析</h2><p>让模型写代码只是表象，真正的关键是形成一个可执行的闭环思维循环。</p><h3 id="1-ReAct-与-CodeAct"><a href="#1-ReAct-与-CodeAct" class="headerlink" title="1. ReAct 与 CodeAct"></a>1. ReAct 与 CodeAct</h3><table><thead><tr><th>对比项</th><th>ReAct</th><th>CodeAct</th></tr></thead><tbody><tr><td>表达形式</td><td>Thought + Action + Observation（文本）</td><td>Thought + <code>&lt;execute&gt;</code> + Observation（代码执行）</td></tr><tr><td>动作实现</td><td>由外部解析器调用预定义工具</td><td>直接生成并执行函数</td></tr><tr><td>控制流</td><td>受限（多步靠多轮调用）</td><td>原生支持 if &#x2F; for &#x2F; try</td></tr><tr><td>可观察性</td><td>文本输出</td><td>代码执行结果（return&#x2F;stdout&#x2F;error）</td></tr><tr><td>优势</td><td>安全、可控</td><td>执行力强、贴合训练分布</td></tr><tr><td>局限</td><td>动作空间有限</td><td>执行安全、调试成本高</td></tr></tbody></table><br/><p>CodeAct 的核心在于：让 LLM 的输出<strong>成为可直接执行的程序</strong>，并用执行结果作为新的输入。</p><p>这种结构让模型不仅描述动作，而是直接拥有动作。</p><br/><h3 id="2-执行循环机制"><a href="#2-执行循环机制" class="headerlink" title="2. 执行循环机制"></a>2. 执行循环机制</h3><h4 id="典型逻辑（伪代码）"><a href="#典型逻辑（伪代码）" class="headerlink" title="典型逻辑（伪代码）"></a>典型逻辑（伪代码）</h4><figure class="highlight ts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!done) &#123;</span><br><span class="line">  <span class="keyword">const</span> thought = llm.<span class="title function_">generate</span>(context)</span><br><span class="line">  <span class="keyword">const</span> code = <span class="title function_">extractExecuteBlock</span>(thought)</span><br><span class="line">  <span class="keyword">const</span> result = sandbox.<span class="title function_">run</span>(code)</span><br><span class="line">  context.<span class="title function_">push</span>(<span class="string">`Execution Output:\n<span class="subst">$&#123;result&#125;</span>`</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行过程包含四个核心步骤：</p><ol><li><strong>抽取代码块</strong>：识别 <code>&lt;execute&gt;...&lt;/execute&gt;</code> 中的代码片段</li><li><strong>沙箱执行</strong>：运行在受限函数或容器中</li><li><strong>结果反馈</strong>：stdout &#x2F; return &#x2F; error 都回写上下文</li><li><strong>下一轮推理</strong>：模型读取结果、修复代码、继续执行</li></ol><p>CodeAct 通过这个循环，实现了 “思考 → 执行 → 观察 → 再思考” 的闭环，而这正是传统 ReAct 模式所缺乏的。</p><br/><br/><h2 id="几何-AI：CodeAct-的落地场景"><a href="#几何-AI：CodeAct-的落地场景" class="headerlink" title="几何 AI：CodeAct 的落地场景"></a>几何 AI：CodeAct 的落地场景</h2><p>几何画图任务天然契合 CodeAct，因为它具备以下特征：</p><ul><li><strong>构造性</strong>：点、线、圆的依赖顺序明确；</li><li><strong>约束性</strong>：角度、平行、对称等条件必须验证；</li><li><strong>可验证性</strong>：每次执行都能计算验证结果。</li></ul><p>在我的项目中，LLM 会直接生成 TypeScript 代码来驱动几何引擎：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">execute</span>&gt;</span></span><br><span class="line">const A = point(0, 0)</span><br><span class="line">const B = point(2, 0)</span><br><span class="line">const C = perpendicular(A, B, 1.5)</span><br><span class="line">drawTriangle(A, B, C)</span><br><span class="line">return checkIsosceles(A, B, C)</span><br><span class="line"><span class="tag">&lt;/<span class="name">execute</span>&gt;</span></span><br></pre></td></tr></table></figure><p>执行结果会返回布尔值或错误信息（如“点C未定义”），再交回给模型进行下一步思考。<br>这使得“构造—验证—修正”形成完整闭环。</p><br/><br/><h2 id="五、范式比较与适用场景"><a href="#五、范式比较与适用场景" class="headerlink" title="五、范式比较与适用场景"></a>五、范式比较与适用场景</h2><p>当然，CodeAct 不是所有场景的答案，而是执行密集型任务的最优解。</p><table><thead><tr><th>范式</th><th>核心思想</th><th>典型场景</th><th>优势</th><th>局限</th></tr></thead><tbody><tr><td><strong>ReAct</strong></td><td>Text-based reasoning + action schema</td><td>工具调用、问答代理</td><td>结构简单、安全</td><td>不支持循环、执行力弱</td></tr><tr><td><strong>CodeAct</strong></td><td>代码即行动，可执行推理</td><td>几何、数据分析、自动化流程</td><td>灵活、图灵完备</td><td>安全与可控性挑战</td></tr><tr><td><strong>Planner + DSL + Verifier</strong></td><td>高层规划 + 安全执行</td><td>企业工作流、合规系统</td><td>可治理性强</td><td>扩展性低</td></tr></tbody></table><br/><p>这些范式不是“演化关系”，而是“任务匹配曲线”：</p><ul><li>若你需要<strong>稳定且可控</strong>的 Agent，ReAct 足够；</li><li>若你希望<strong>模型主动执行与验证</strong>，CodeAct 更高效；</li><li>若你构建<strong>大型协作系统</strong>，PEER&#x2F;Planner 才是路径。</li></ul><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>CodeAct 的潜力在于它把 “生成式 AI” 推向 “行动式 AI”。<br>今天我们已经看到：</p><ul><li><strong>Runtime 自检（Self-checking Runtime）</strong>：模型生成代码 → 执行 → 自动生成测试样例验证输出；</li><li><strong>静态分析 + 动态执行结合</strong>：利用 AST 检查危险调用；</li><li><strong>跨语言统一执行层</strong>：JS&#x2F;Python&#x2F;Go 均可成为 Action carrier；</li><li><strong>Agent Runtime 生态</strong>：不同 CodeAct 系统可互通共享执行上下文。</li></ul><p>CodeAct 让模型拥有代码生成力、执行力与修正力。</p><p>在几何 AI 的世界里，这意味着从 “我能解释几何” 到 “我能构造几何”。</p><p>也意味着从<strong>语言智能</strong>迈向<strong>行动智能</strong>的转折。</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><strong>Wang, Xingyao et al.</strong> <em>Executable Code Actions Elicit Better LLM Agents.</em> ICML 2024. <a href="https://arxiv.org/abs/2402.01030">PDF</a> &#x2F; <a href="https://proceedings.mlr.press/v235/wang24h.html">PMLR</a></li><li><strong>CodeAct GitHub 实现</strong> – <a href="https://github.com/xingyaoww/code-act">github.com&#x2F;xingyaoww&#x2F;code-act</a> <a href="https://github.com/langchain-ai/langgraph-codeact">github.com&#x2F;langchain-ai&#x2F;langgraph-codeact</a></li><li><strong>Mohsin Mubarak</strong>, <a href="https://medium.com/@mohsin.sk.9820/paper-explained-blog-codeact-revolutionizing-llm-agents-with-executable-code-actions-c960cc5e30eb">CodeAct: Revolutionizing LLM Agents with Executable Code Actions</a> <em>Medium</em>, 2024.</li><li><strong>Survey:</strong> <a href="https://openreview.net/pdf?id=ekl6Z88uQj">Code Reasoning for Code Tasks: A Survey and A Call to Action.</a> OpenReview 2024</li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>突破 MoE 限制，PEER 架构如何推动超级智能的未来发展</title>
      <link>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</link>
      <guid>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</guid>
      <pubDate>Wed, 05 Nov 2025 14:26:33 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;a href=&quot;#引言：大模型架构的“困境”&quot; class=&quot;headerlink&quot; title=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;/a&gt;引言：大模型架构的“困境”&lt;/h2&gt;&lt;p&gt;这两年，大模型的风头正劲。&lt;/p&gt;
&lt;p&gt;大家都在谈</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：大模型架构的“困境”"><a href="#引言：大模型架构的“困境”" class="headerlink" title="引言：大模型架构的“困境”"></a>引言：大模型架构的“困境”</h2><p>这两年，大模型的风头正劲。</p><p>大家都在谈 <strong>MoE（Mixture of Experts）</strong> ——它被认为是突破大模型计算瓶颈的关键方向。</p><p>通过<strong>稀疏激活</strong>，只激活少量专家节点，让计算开销成比例下降，而模型能力却可以继续增长。听上去完美无缺。</p><br/><p>不过，真正尝试将 MoE&#x2F;SMoE 架构落地时，问题随之而来。</p><p>专家池一多，计算分配就变得不平衡；某些专家被频繁调用，而其他专家几乎闲置。系统扩展起来也不再优雅，想加新领域，常常意味着“推倒重来”。</p><p>MoE 虽然高效，却还远没到“万能”的阶段。</p><p>尤其对于我们这些做 <strong>多领域智能 Agent</strong> 的人来说——客服、教育、企业知识管理——系统要在不同任务之间灵活切换、持续学习，这套架构似乎仍有点笨重。</p><br/><p>然而，《Mixture of A Million Experts》论文带来了新的思路，打破了行业普遍认为大模型计算瓶颈无法突破的常规认知。</p><p><strong>研究者提出了一个颠覆性的观点：专家数量并非瓶颈，而是可以突破的限制。</strong></p><p>通过 <strong>PEER 架构（Product Key Expert Retrieval）</strong>，模型可以在保持轻盈的同时，拥有百万级专家容量，彻底改变大模型的设计和扩展方式。</p><p><strong>大模型的未来，不再是一颗大脑，而是无数个微专家在协同工作。</strong></p><br/><br/><h2 id="MoE-架构：风光背后的隐忧"><a href="#MoE-架构：风光背后的隐忧" class="headerlink" title="MoE 架构：风光背后的隐忧"></a>MoE 架构：风光背后的隐忧</h2><p>MoE 的设计初衷其实很优雅：</p><p>与其让一个庞大的模型处理所有问题，不如分工合作——不同专家解决不同任务。</p><p>这样既能减少计算，又能扩展模型容量。</p><br/><p>但问题是，当“专家”变多，管理它们本身就成了一门学问。<br>MoE 模型在训练时容易出现负载不均：</p><ul><li>某些专家被反复选中，工作超载；</li><li>某些专家几乎从不被调用，形同虚设。</li></ul><p>而且专家数量也存在“物理极限”。几百个模块听起来不少，但放到一个需要多领域、多任务的 Super Agent 系统中，很快就会捉襟见肘。</p><p>MoE 的结构像是一栋办公楼——房间足够多，但一旦某几层太忙、几层太空，效率就会被拖垮。</p><p>MoE 的问题不是不聪明，而是不够灵活。它像一台巨大的机器，而不是一个会生长的生态系统。</p><br/><br/><h2 id="PEER-架构：突破-MoE-限制的那一把钥匙"><a href="#PEER-架构：突破-MoE-限制的那一把钥匙" class="headerlink" title="PEER 架构：突破 MoE 限制的那一把钥匙"></a>PEER 架构：突破 MoE 限制的那一把钥匙</h2><p>在这篇论文里，研究者提出了一个更激进的设想：</p><p>既然“专家”是瓶颈，那我们不如把它拆得更小、更细。</p><p>于是便有了 <strong>PEER（Product Key Expert Retrieval）</strong> 架构——一个拥有 <strong>百万级微专家</strong> 的系统。</p><br/><h3 id="1-百万级-experts-：打破扩展的上限"><a href="#1-百万级-experts-：打破扩展的上限" class="headerlink" title="1. 百万级 experts ：打破扩展的上限"></a>1. 百万级 experts ：打破扩展的上限</h3><p>MoE 的专家通常是完整的子网络，庞大且昂贵；</p><p>而 PEER 则反其道而行，每个专家只保留 <strong>一个神经元（one-neuron MLP）</strong>。</p><br/><p>这种极简设计的好处是：</p><ul><li>可以容纳上百万个专家；</li><li>每次推理只需激活极少一部分；</li><li>模型容量得到几何级提升，但计算量几乎不变。</li></ul><p>这就像把庞大的知识体系拆成了无数个“知识细胞”，<br>每个细胞只在特定输入下工作，真正实现了“用多少、算多少”。</p><p>PEER 不追求更强的单体智能，而是让微小的智慧汇聚成群体的力量。</p><br/><h3 id="2-极致稀疏激活：从-O-N-到-O-√N"><a href="#2-极致稀疏激活：从-O-N-到-O-√N" class="headerlink" title="2. 极致稀疏激活：从 O(N) 到 O(√N)"></a>2. 极致稀疏激活：从 O(N) 到 O(√N)</h3><p>传统 MoE 的瓶颈之一是路由复杂度。</p><p>每次输入都要计算与 N 个专家的匹配度，复杂度是 O(N)。</p><p>当专家数量达到百万级时，这几乎不可接受。</p><p>PEER 用 <strong>Product Key Routing（产品键路由）</strong> 巧妙地把这个问题降到了 **O(√N)**。</p><br/><p>想象一下专家池是一个二维表格。</p><p>每个专家的“键”被拆成两部分，分别属于两个子空间。</p><p>当输入到来时，只需：</p><ol><li>在每个子空间里各自检索 top-k；</li><li>再组合成候选专家集合。</li></ol><p>最终，模型只需从少量候选中选择真正相关的专家。</p><p>这样既避免了全量扫描，又保持了高匹配率。</p><p>PEER 的路由逻辑更像搜索引擎，而非广播通知。它只找需要的人，而不是通知所有人。</p><br/><h3 id="3-Product-Key-Routing：专家调度的自组织"><a href="#3-Product-Key-Routing：专家调度的自组织" class="headerlink" title="3. Product Key Routing：专家调度的自组织"></a>3. Product Key Routing：专家调度的自组织</h3><p>PEER 的路由不仅高效，还具备“自组织”的特性。</p><p>每个专家的键是可学习的，随着训练推进，模型会自然地将相似任务分配给相近的专家集合。</p><br/><p>久而久之，系统内部形成了类似“语义社区”的结构：</p><p>不同类型的问题，倾向激活不同区域的专家群落。</p><p>这让模型在面对多任务学习时，既能共享知识，又能保持分工明确。</p><p>PEER 让模型像一个自我成长的城市，不同街区专注不同产业，但彼此之间又保持联通。</p><br/><h3 id="4-动态扩展专家池：支持持续学习与多领域适应"><a href="#4-动态扩展专家池：支持持续学习与多领域适应" class="headerlink" title="4. 动态扩展专家池：支持持续学习与多领域适应"></a>4. 动态扩展专家池：支持持续学习与多领域适应</h3><p>另一个令人惊喜的点在于，PEER 支持 <strong>专家池的动态扩展</strong>。</p><p>也就是说，模型训练完后，仍然可以按需增加新专家，而无需重训全体网络。</p><p>这对于多领域智能 Agent 来说，是一种质变。</p><p>想象一个助手AI：</p><ul><li>初期专注电商；</li><li>后来扩展到金融、旅游、医疗领域；</li><li>不需要“重生”，只需“长出新神经元”。</li></ul><p>这种能力让 AI 系统能像产品一样，持续成长，而不是一成不变的快照,不断去迭代新的版本。</p><br/><br/><h2 id="PEER-架构的产品启示"><a href="#PEER-架构的产品启示" class="headerlink" title="PEER 架构的产品启示"></a>PEER 架构的产品启示</h2><p>从产品角度看，PEER 的价值不仅在于“能跑得更快”，而在于“<strong>能持续成长</strong>”。</p><ol><li><p><strong>可扩展性</strong></p><p>新领域到来时，不需要重训全模型，只需新增专家池。</p><p>这让产品拥有真正的 <strong>演进式架构</strong>。</p></li><li><p><strong>计算效率与成本控制</strong></p><p>极致稀疏激活意味着按需使用算力，推理成本显著降低。</p><p>对 SaaS 型 AI Agent 平台尤为重要——既能大规模服务，又能保持性价比。</p></li><li><p><strong>长期学习与知识积累</strong></p><p>PEER 的设计天然支持 <strong>持续学习</strong>。</p><p>这意味着 AI Agent 不会像旧模型那样被“冻住”，而是能随着使用场景的变化，积累新知识。</p></li></ol><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>PEER 架构最打动我的地方，是它代表了一种新的思维方式：</p><p>我们不再把智能看成一个中心化的大脑，而是一张能自我生长、不断优化的网络。</p><p>对于 Super Agent 的未来而言，这正是我们缺失的那一环。</p><p>也许下一个时代的智能，不是更大的模型，而是更会生长的系统。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/10 Review</title>
      <link>https://blog.liluhui.cn/2025/11/01/202510/</link>
      <guid>https://blog.liluhui.cn/2025/11/01/202510/</guid>
      <pubDate>Sat, 01 Nov 2025 13:34:22 GMT</pubDate>
      
      <description>炉要小，火要旺</description>
      
      
      
      <content:encoded><![CDATA[<p><em>炉要小，火要旺</em></p><br/><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01</p><p>人常说，<strong>期望会影响现实，创造出自我实现的预言。</strong><br>这些年，这种说法有许多相近的面孔：<br>“向宇宙许愿，它就会来。”<br>“你发出的能量，会以另一种形式回到你身上。”<br>“信则有。”<br>可在我看来，那些形式并不重要。<br>真正起作用的，是<strong>人的愿力</strong>。</p><p>因为足够想要，于是卯足了劲。<br>因为足够想要，于是显得勇敢。<br>因为足够想要，于是能感染他人。<br>因为足够想要，于是连认知都会被扭曲。</p><p>我并不贬义地说“扭曲”。<br>专注于一件事，本就是一种偏折，一种放大。<br>深度往往带来变形，这是客观的。</p><p>人要正视自己心中的“想要”。<br>不是靠仪式去塑造愿力，<br>而是靠体验去发现它、沉淀它、深化它。<br>否则，就容易在模糊的欲望里耗尽数十年，<br>搭建出一个精致却虚幻的黄粱梦。</p><p>从来不是那几支香实现了愿望，<br>只是有人<strong>真的去做了</strong>。<br>那些没能成的事，往往就差在这一步。</p><p>人总爱标榜“我是怎样的人”，<br>却不愿脏手，不想受苦。<br>有些选择可以优中择优，<br>但更多时候，人生只能一一排除，<br>留下的，就是方向。</p><p><strong>先活下来，先让心境平稳，<br>才有余裕去优选。</strong></p><br/><p>02<br>痛苦情绪在人的生活中是无法避免的。<br>它不是意外的杂质，而是人性的一部分。<br>当我们急着否定它、逃避它、压抑它的时候，<br>其实也在否认那部分真实的自己。</p><p>错误地认为成熟意味着时刻保持平静，<br>意味着能“控制情绪”，<br>但有时候，真正的成熟恰恰是——<strong>允许自己痛。</strong><br>允许悲伤，允许失落，允许迷茫地站在原地。<br>因为那些情绪并不是“坏的”，<br>它们只是告诉我们，<strong>哪里被触碰了，哪里还在乎。</strong></p><p>痛苦是对生活的敏感反应。<br>正因为你仍有渴望、有期待、有爱，<br>痛苦才会出现。<br>若没有这些，世界对你也就再无波澜。</p><p>有时候，痛苦只是提醒你——<br>你还在活着。<br>而<strong>活着</strong>，本身就意味着不可能只拥有快乐的一面。</p><br/><p>03</p><p>在《金钱的艺术中》给出<strong>财务独立</strong>的等级：</p><p>第0级:完全依赖陌生人的善意，毫无掌控力。<br>第1级:依赖家人、朋友或慈善的救助才能生活，<br>第2级:依赖政府或社会保障体系维持生计。<br>第3级:靠工作养活自己，但一旦失业就陷入困境。<br>第4级:有少量储蓄，能短期应对意外开支。<br>第5级:积蓄能支撑几个月，即使暂时失业也不至于崩溃。<br>第6级:技能或职业有一定不可替代性，不易被取代。<br>第7级:能拒绝糟糕的老板或工作，真正拥有选择权。<br>第8级:即使换城市、换行业，也能凭储蓄和能力顺利过渡。<br>第9级:拥有稳定的资产或副业收入，减少对单一工作的依赖。<br>第10级:储蓄和投资足以覆盖数年的生活成本<br>第11级:即便遭遇金融危机或行业衰退，也能从容应对。<br>第12级:被动收入足以覆盖基本生活开支，不再依赖工作生存。<br>第13级:储蓄和资产支撑的不仅是温饱，而是理想的生活方式。<br>第14级:财富充裕到足以支持你做任何想做的事。<br>第15级:完全自由，能随心所欲支配时间，按照自己的方式生活。</p><p>感觉自己的理财账算下来也快稳稳到12级了，却还受着7级的苦。<br>背后是因为想要做成一些事，总是要接受一些现实的不舒服，因为想要实现的自我，也想要抵达13级，还是需要沉下心做好事。<br>人不能总是追求那个享乐的结果，只要在做事，想要把上下的链路跑通，想要把相关的人顺清楚，总是有些苦恼的，会失落、会气愤，也会经历欣喜、满足，这些过程中的体验都算人生的数。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>这个月一边在大角项目和只记项目里实践 Agent，一边再输出内容，也算是给自己严格安排了写作任务，一个月10篇，有不满意的，也有写完很舒畅的，但不论完美与否完成了还是开心的，值得鼓励。（八百年都没这么勤快过）<br>从数据上来看，标题还是太重要了，大众一点也不喜欢抽象和自以为是的高高在上，有些名字起的挺失败的，但我还是想试试市场，可惜点击率太低连流量都进不了下一个池子。<br>现在有两个 Agent 系列并行，<strong>Agent Memory 深入系列</strong>累计到现在一共 8篇了，<strong>Agent 大众科普系列</strong> 就比较随意想到就安排写了，中间还尝试想开 <strong>Agent Design Pattern 系列</strong>，第一篇不太理想，不过还是想试试这个方向。<br>总之越写想法越多，慢慢来吧。</p><br/><p>本月更新<br><a href="https://mp.weixin.qq.com/s/y03S4X_GqGorua10PROmBw">AI 可以像人类一样选择性记忆了吗？现在真有人在做这事了</a> | 技术<br><a href="https://mp.weixin.qq.com/s/K4boDF2ZSIDjVosY3EyOZA">你以为你在和 AI 聊天，其实你在调度一个系统</a> | 科普<br><a href="https://mp.weixin.qq.com/s/5wGsoUshx6mhCbra_7xSZw">Agent 又又失忆了！我来做一次记忆体检</a> | 技术<br><a href="https://mp.weixin.qq.com/s/13Pkwku9rJLQrl4Bs0sMWw">什么让一个 AI 系统成为“智能体”（Agent）？</a> | 技术<br><a href="https://mp.weixin.qq.com/s/v9VcuiBE23eb54JmmDxckA">一文看懂，Agent 避免记忆漂移三大策略</a> | 技术<br><a href="https://mp.weixin.qq.com/s/ZCPeEu5noEoy6RRx55hm9w">一句话讲明白，什么是多 Agent 系统？</a> | 科普<br><a href="https://mp.weixin.qq.com/s/uAhbuENNSiIszO4iVAfIVw">一句话讲明白：Agent Memory 是什么？</a>  | 科普<br><a href="https://mp.weixin.qq.com/s/Aa01jvz3q1nZp2f5F4loKQ">Agent 记忆检索策略：怎样学会想起？</a> | 技术<br><a href="https://mp.weixin.qq.com/s/2f9ngZGTd0f-5LO91qMOuw">把 MCP 放回它该在的位置</a>  | 科普<br><a href="https://mp.weixin.qq.com/s/1OLbkeaWQaIAEgOQ-30LTw">Agent 记忆写入三大策略，决定“记什么”的工程学</a> | 技术</p><br/><p>02<br>大角几何画板上线以来的数据增长还是非常明显，上个月的活跃用户有2k，这个月单月达到了3k，随着用户反馈需要的绘图功能需求越来越多，目标用户群体想解决的场景是编排出卷、是比赛判定、是教学演示… 每一个场景想覆盖完整一个最小闭环还有很多的细节，远不是只优化AI能搞定的。<br>我现在犹豫的是整件事情的初心和现状其实有不少偏离，这本是一个重AI的研究型做影响力的项目，现在在深入用户群体后要解决的是场景问题，需要传统工程上的很多工作量，而这个模式养不活团队。<br>做影响力的故事和做场景的商业价值，两套打法本就是不一样的，我有全新的更轻的想法，但不是一个好的时机。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/Pasted%20image%2020251101211731.png" alt="image"></p><br/><p>03<br>自媒体方向也感受到了欣喜的数据增长和流量退坡的焦虑，前面三个月小红书从0干到1000，十月单月又增加了500，开始同步注册其他平台，全平台粉丝数也达到了 2k。</p><p>总结下这个月从运营账号中学会的：</p><ol><li>安排好数据复盘的规律，安排好回复消息的时间段，数据下降就下次调整不要纠结着当期的内容，重要的永远是持续的节奏。 ——  比如被风控，去折腾能浪费掉一整天，最终的结果是发出来了也是最小流量池，没必要。</li><li>我本来以为要过个半年一年才能体会到平台数据规律，现在没必要管，但现在运营起来已经能很明确地在内容发布6小时后知道这篇的潜力了，只要持续做，目标人群的数据规律很快就会浮现出来。—— 其实跑不出来的内容1小时候就能下判断了，最小流量池，但凡吸引力达标就能过。</li><li>标题党避免不了，建议收藏、一文看懂 这些词就是有用，不然大家根本不想看。比如“一句话说明白”就是比“一文看懂”数据更好。</li><li>要看好，大家都是视觉动物，有那么东西可以看的情况下，大家也是本能想看更美好的更舒服的。</li><li>要么有用，要么有情绪。想要提升互动量，还是需要内容本身有价值，我现在就很不擅长提供情绪价值，做不到卡兹克老师建议的唤起共鸣，另一方面也是现在的定位偏向更有用，也在开始尝试加入一些带有情绪点的表达。</li></ol><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/Pasted%20image%2020251101212051.png" alt="image"></p><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>年度拼图 +1<br>虽然但是，春天买的拼图，拖到了秋天才拼完。<br>这是一幅自然风景——有猫、有狗、有风、有茶，一种向往的悠闲生活。<br>坐下来拼图的时候，时间走得飞快。像数独一样，一坐就是三四个小时，全身心沉浸在那一片片形状与颜色的线索中。</p><p>也许这就是专注的魔力——当意识完全投入某个小小的世界，时间便失去了意义。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/167782d55a9ba2e34b155e6463d049db.jpg" alt="image"></p><br/><p>02<br>杭州的晚霞，随手拍下的，感受到一种宏伟感。<br>金色的光从云层的缝隙间涌出，一道又一道，像被世界温柔包裹。<br>或许这就是晚霞的力量吧：在疲惫的时刻，给人一个喘息的窗口，提醒我们，天还很高，路还很长。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/857e1fd90c30ead8b606e18175037e74.jpg" alt="image"></p><br/><p>03<br>定制了一个好玩的印章，盖上去是自己的头像。<br>印在手账页上，像是在给自己签名，又像是给生活做一个温柔的注脚。<br>那种“按一按”的小仪式，让平凡的记录多了一点体温。</p><p>我想，人需要这些无用的小物件。<br>它们没有目的，只是让生活有点趣味、有点痕迹 XD。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/46177c9dd0d1028f0857bb8caaff7b80.jpg" alt="image"></p><br/><p>04<br>自从加入霖子工作室的瑜伽练习，进步得让自己都有些惊讶。<br>这两个月尝试了许多从未体验过的动作，尤其是倒立。<br>练习的时候，我发现自己身体里藏着许多“怯懦”——  该放松的地方僵硬，该用力的地方又松散。<br>像极了生活中那些犹豫和不信任自己的时刻。</p><p>倒立最难的，不是力量，而是信任。<br>你得先相信自己能稳住，身体才会真的稳住。</p><p>我时常羡慕那些没有沉默成本的人，<br>能轻易推翻从前，重新开始，不怕浪费，不怕错。<br><strong>他们像孩子一样去学习、去构建，<br>不带惯性地感受每一个动作、每一段路。</strong></p><p>也许成长就是这样一件事：<br><strong>不断拆除旧的自己，在一次次倒立与坠落中，<br>重新找到身体与心的平衡。</strong></p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/cdc079bacf9eb5e4da2481c5ef7653b8.jpg" alt="image"></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/01/202510/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI 如何自主管理记忆？三种前沿架构详解 A-MEM / Mem-α / Mem0</title>
      <link>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/</link>
      <guid>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/</guid>
      <pubDate>Fri, 31 Oct 2025 09:23:13 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：AI-的记忆问题&quot;&gt;&lt;a href=&quot;#引言：AI-的记忆问题&quot; class=&quot;headerlink&quot; title=&quot;引言：AI 的记忆问题&quot;&gt;&lt;/a&gt;引言：AI 的记忆问题&lt;/h2&gt;&lt;p&gt;我们常常说 AI 越来越像人类，但在“记忆”这一环节，现有的系统仍然</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：AI-的记忆问题"><a href="#引言：AI-的记忆问题" class="headerlink" title="引言：AI 的记忆问题"></a>引言：AI 的记忆问题</h2><p>我们常常说 AI 越来越像人类，但在“记忆”这一环节，现有的系统仍然远未达到真正的类人能力。</p><p>在之前的文章《Agent 如何避免记忆漂移：三大策略与工程实践》中，我们讨论了如何设计一个稳定且高效的记忆系统，并分享了三大策略帮助解决记忆一致性和长期记忆问题。</p><p>然而，这些策略多侧重于工程实践，强调了如何避免记忆的失真、漂移和过度遗忘。</p><p>今天，我们要深入探讨一个更前沿的课题：<strong>AI 如何自主决定自己的记忆？</strong> </p><p>这一问题不仅挑战了传统的记忆存储方式，也为智能系统提供了更高的灵活性和自适应能力。</p><br/><p>通过 AI 系统根据任务需求和经验，自主更新和优化记忆，AI 可以更智能地应对复杂的任务。</p><p>在这篇文章中，我们将介绍三种突破性的记忆管理方案——<strong>A-MEM、Mem-α 和 Mem0</strong>，并讨论它们如何通过创新的记忆架构，推动 AI 系统在长期任务中的表现。</p><br/><br/><h2 id="记忆管理的基本概念"><a href="#记忆管理的基本概念" class="headerlink" title="记忆管理的基本概念"></a>记忆管理的基本概念</h2><p>在深入这三种方案之前，我们先简单回顾一下 AI 记忆管理的基本概念。</p><p>通常，AI 系统的记忆可以分为两大类：</p><ol><li><p><strong>工作记忆</strong></p><p> 类似于人类的短期记忆，用于存储和处理当前任务的信息。它是即时性的，帮助 AI 快速做出决策。</p></li><li><p><strong>长期记忆</strong></p><p> 涉及 AI 系统在长时间内积累的知识和经验，帮助其从过去的经验中学习并做出更好的决策。</p></li></ol><p>然而，AI 的“记忆”常常是静态的，即记忆的内容一旦储存，就不会再变化或更新。</p><p>这种设计使得 AI 在面对长期任务或多轮交互时，常常面临记忆的局限性，容易忘记先前的重要信息，甚至陷入“记忆漂移”的问题——记忆内容的过时或错误。</p><br/><p>为了解决这一问题，<strong>Agentic Memory</strong> 提出了一个新思路：<strong>让 AI 系统自主决定记忆内容、何时更新以及如何检索</strong>。</p><p>换句话说，AI 具备了像人类一样选择性记忆的能力，能根据任务需求动态调整记忆内容。</p><br/><br/><h2 id="A-MEM：基于-Zettelkasten-方法的动态记忆"><a href="#A-MEM：基于-Zettelkasten-方法的动态记忆" class="headerlink" title="A-MEM：基于 Zettelkasten 方法的动态记忆"></a>A-MEM：基于 Zettelkasten 方法的动态记忆</h2><p>A-MEM 是一种创新的记忆架构，借鉴了 Zettelkasten 方法——一种常用于知识管理的笔记方法。</p><p>在 Zettelkasten 中，信息被分解为多个“笔记块”，每个笔记块都与其他笔记块建立联系，从而形成一个有机的知识网络。</p><p>A-MEM 将这一概念引入 AI 记忆管理，创建了一个动态的记忆系统，让 AI 根据当前的任务需求，灵活地选择和组织记忆。</p><br/><p>核心设计：</p><ul><li><p><strong>结构化记忆笔记</strong></p><p>每条记忆以结构化笔记形式存储，包含上下文摘要、关键词、标签和嵌入向量。</p></li><li><p><strong>动态链接与演化</strong></p><p>新记忆加入后，系统基于共享属性和语义相似度，自动与历史记忆建立连接，并触发对旧记忆的内容和属性更新。</p></li><li><p><strong>自组织记忆网络</strong></p><p>系统通过记忆之间的链接持续优化知识结构，无需手动提示即可逐步演化出连贯的认知网络。</p></li></ul><br/><p>在实际应用中，A-MEM 被用来提高多任务学习和推理系统的表现。</p><p>比如，假设一个 AI 需要完成一个多轮对话任务，它不仅能记住每轮对话的内容，还能根据对话的上下文，动态调整记忆结构，避免遗漏重要信息。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/agiresearch/A-mem">https://github.com/agiresearch/A-mem</a></p><br/><br/><h2 id="Mem-α：强化学习驱动的记忆管理"><a href="#Mem-α：强化学习驱动的记忆管理" class="headerlink" title="Mem-α：强化学习驱动的记忆管理"></a>Mem-α：强化学习驱动的记忆管理</h2><p>Mem-α 采用了强化学习（Reinforcement Learning, RL）来优化记忆管理。</p><p>与传统的静态记忆方式不同，Mem-α 通过奖励机制，帮助 AI 代理选择和存储最有价值的记忆。</p><p>AI 代理会根据任务的反馈信号，自动决定哪些记忆应该被强化，哪些应该被忽视或删除。</p><br/><p>核心设计：</p><ul><li><p><strong>多类型记忆系统</strong></p><p>模型支持三类记忆：核心记忆（如身份、偏好）、情节记忆（带时间标签的事件）和语义记忆（抽象知识），分别支持更新、插入、删除等操作。</p></li><li><p><strong>序列决策训练机制</strong></p><p>记忆构建被建模为一个序列决策过程，模型使用强化学习逐步学习如何构建和使用记忆。</p></li><li><p><strong>多重奖励优化目标</strong></p><p>包括回答准确性、操作规范性、记忆压缩效率和信息质量等指标，确保记忆系统在效率与表现之间取得平衡。</p></li></ul><br/><p>Mem-α 在长时间的任务执行中表现得尤为出色。</p><p>例如，在一个需要多轮决策的任务中，AI 代理能够通过强化学习逐步选择性记忆，将更多的注意力集中在当前决策需要的关键经验上。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/wangyu-ustc/Mem-alpha">https://github.com/wangyu-ustc/Mem-alpha</a></p><br/><br/><h2 id="Mem0：图数据库驱动的记忆系统"><a href="#Mem0：图数据库驱动的记忆系统" class="headerlink" title="Mem0：图数据库驱动的记忆系统"></a>Mem0：图数据库驱动的记忆系统</h2><p>Mem0 是一个基于图数据库的记忆系统。通过图数据库，Mem0 将记忆内容模块化，允许不同记忆块之间通过关系连接，形成一个可以灵活查询的记忆网络。这个设计使得 AI 能够在复杂任务中有效检索和管理记忆内容。</p><p>核心设计：</p><ul><li><p><strong>两阶段记忆机制</strong></p><p>首先由 LLM 从用户交互中提取“候选记忆”（结构化事实或摘要），然后与历史记忆比对后，智能决定添加、更新、删除或跳过。</p></li><li><p><strong>多层次记忆结构</strong></p><p>支持用户级、会话级和代理级的记忆管理，确保跨场景、跨时间段的记忆一致性。</p></li><li><p><strong>成本控制与响应效率</strong></p><p>与传统将全部历史放入上下文的方式相比，Mem0 显著降低 token 消耗与响应延迟，并提高答案准确率。</p></li></ul><br/><p>Mem0 在多轮对话和长期任务中表现得尤为出色。</p><p>在实际应用中，Mem0 能够根据对话历史或任务进展，快速检索到相关的记忆内容，并作出更加智能的反应。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/mem0ai/mem0">https://github.com/mem0ai/mem0</a></p><br/><br/><h2 id="其他自主记忆方案"><a href="#其他自主记忆方案" class="headerlink" title="其他自主记忆方案"></a>其他自主记忆方案</h2><p>除了 A-MEM、Mem-α 和 Mem0 之外，还有一些其他方案也在探索 AI 自主决定记忆的方向。</p><p>例如，<strong>Episodic Memory</strong> 关注的是通过情节记忆来支持 AI 的长时记忆能力，<strong>Vision-Language Episodic Memory（VLEM）</strong> 则通过结合视觉和语言模态，增强了记忆系统的多模态能力。而 <strong>Memory-as-Action</strong> 则通过将记忆管理视为可学习的行为，提升了系统的自适应性。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>随着 AI 技术的进步，记忆管理 将不再是简单的存储和检索任务，而是演变 <strong>AI 自主决策</strong>的动态过程。</p><p>展望未来，记忆的自我更新机制、跨模态记忆整合和长期记忆优化等技术将进一步推动 AI 系统更智能地管理和利用自己的记忆，不断提升其处理长期任务的能力。</p><p>然而，这种记忆自主权的提高也引发了一个耐人寻味的思考：<strong>我们真的要把“选择记忆”的权力交给算法吗？</strong></p><p>当 AI 可以自主决定保留什么、遗忘什么，它在某种意义上也开始塑造自己的“身份认同”。</p><p>而人类最终或许只是那个在幕后制定基本规则的系统设计者。</p><p>这不禁让人想起科幻电影《银翼杀手》中复制人罗伊的临终独白：</p><blockquote><p>所有这些时刻终将流失在时光中，<br/>一如眼泪消失在雨中。</p></blockquote><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li><strong>A-MEM 论文</strong>：<a href="https://arxiv.org/abs/2502.12110">A-MEM: Agentic Memory for LLM Agents</a></li><li><strong>Mem-α 论文</strong>：<a href="https://arxiv.org/abs/2509.25911">Mem-α: Learning Memory Construction via Reinforcement Learning</a></li><li><strong>Mem0 论文</strong>：<a href="https://arxiv.org/abs/2504.19413">Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</a></li></ol>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>了解和 AI 对话时真正发生了什么（你可能一直理解错了）</title>
      <link>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/</link>
      <guid>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/</guid>
      <pubDate>Wed, 29 Oct 2025 07:34:03 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;你可能也有过这个体验：&lt;/p&gt;
&lt;p&gt;打开豆包、腾讯元宝 或者 ChatGPT，输入一句话，它“立刻开始打字”，像一个反应敏捷的朋友，甚至比人类还懂得如何接话、如何幽默。&lt;/p&gt;
&lt;p&gt;在绝大多数人心里，这是“像一个人一样在聊”。&lt;/p&gt;
&lt;p&gt;但你知道吗？&lt;/p&gt;
&lt;p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<p>你可能也有过这个体验：</p><p>打开豆包、腾讯元宝 或者 ChatGPT，输入一句话，它“立刻开始打字”，像一个反应敏捷的朋友，甚至比人类还懂得如何接话、如何幽默。</p><p>在绝大多数人心里，这是“像一个人一样在聊”。</p><p>但你知道吗？</p><p><strong>技术视角看到的完全不是“对话”，而是一套被精准触发的工业级装配流程。</strong></p><br/><br/><h2 id="你看到的是“对话感”，系统看到的是“请求处理管线”"><a href="#你看到的是“对话感”，系统看到的是“请求处理管线”" class="headerlink" title="你看到的是“对话感”，系统看到的是“请求处理管线”"></a>你看到的是“对话感”，系统看到的是“请求处理管线”</h2><p>我们以最常见的一句自然问话为例：</p><p>“可以帮我整理成一份可以发给团队的正式总结吗？”</p><p>你觉得它马上开始“思考并组织语言”，但实际上</p><p><strong>它做的是一串极其严格、完全程序化的流程：</strong></p><p>① 接收到请求 → ② 安全扫描 → ③ 构建上下文窗口 → ④ 文本切割成 Token → ⑤ 开始预测第一个 Token ⑥ 一边预测一边实时往回流（你看到的“正在输入…”）→ ⑦ （如需要）中途调用某个外部工具 → 再继续生成</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/29/steps.png"></p><p>这几乎和流水线制造一台 iPhone 没本质区别。</p><p>任何一个环节没有被触发，它都不会“自动领会你的真实意图”。</p><p>它不是在“理解你”，它是在“执行一个极快的数据处理流程”。</p><br/><br/><h2 id="为什么你以为“它在认真和你聊天”？"><a href="#为什么你以为“它在认真和你聊天”？" class="headerlink" title="为什么你以为“它在认真和你聊天”？"></a>为什么你以为“它在认真和你聊天”？</h2><p>因为它的表现被刻意设计得“像人”。</p><ul><li>输出是“打字式流式返回”，不是一次生成后才展示</li><li>回复语气刻意模拟“人类”而非“系统提示”</li><li>会主动说“我理解你的需求是…”来暗示“理解”</li><li>会“接上语境”，强化“它在听你说话”的错觉</li></ul><p><strong>而人类大脑极容易把“模式匹配 + 语言流畅”误认成“智能 + 意识 + 交流”。</strong></p><p>那种“我被理解了”的错觉，是设计出来的体验层目的，不是它的本质。</p><br/><br/><h2 id="真正的误解在这里"><a href="#真正的误解在这里" class="headerlink" title="真正的误解在这里"></a>真正的误解在这里</h2><p>我们误以为它像人在持续听你说话，会自然而然记得你之前提过什么。</p><p>但事实是：<strong>绝大多数 AI 并不是“忘记”，而是根本“没有被允许记”。除非你触发系统判定“值得写入的结构化信息”。</strong></p><br/><p>换句话说，它不是健忘，而是在严格执行一套记忆筛选机制：</p><ul><li>你以为它会像人类那样自动回顾上下文？<strong>不会</strong>。</li><li>你以为它“听懂了你的意图”？<strong>它只是暂存成 Token 流，随时会被窗口上限挤掉</strong>。</li><li>你以为你“前面铺垫过”的内容它应该记得？<strong>系统其实是在评估“这件事是否值得被存档”</strong>。</li></ul><p><strong>我们常常痛苦地喊“怎么又忘了前提？”，但错不在它——是我们误以为它会记。</strong></p><p>理解这一点的瞬间，你会突然解释通自己过去 90% 和 AI 沟通挫败的原因。</p><p>不是它“不行”，而是你在“用跟人聊天的方式，调用一个完全不一样的系统架构”。</p><br/><br/><h2 id="但知道真相之后，有什么用？"><a href="#但知道真相之后，有什么用？" class="headerlink" title="但知道真相之后，有什么用？"></a>但知道真相之后，有什么用？</h2><p>非常关键的一点是，</p><p>一旦你知道你不是在“对话”，而是在“调度装配线”，你就可以主动提高效率。</p><p>比如：</p><ul><li><p>想让它优先说结论？</p><p>→ 在开头加一句 “<strong>请先给结论，再解释原因</strong>”</p></li><li><p>想确保它不会“忘掉前提”？</p><p>→ 你不要“自然聊天”，而是<strong>每轮都把任务上下文重新锚定</strong></p></li><li><p>想让它知道“你正在执行一项任务而非闲聊”？</p><p>→ 明确说 “<strong>这是一个多轮协作任务，我们每轮专注一个子步骤。</strong>”</p></li></ul><p>你会突然发现，它不再“随机发挥”，而是开始“听你指挥”。</p><br/><br/><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>对话式 AI 当然正在快速进化。</p><p>你今天看到的是“即时推理的装配线”，但下一阶段的趋势已经非常明确。</p><p><strong>它会逐步具备可训练的、可被赋予“持久记忆结构”的能力。</strong></p><p>这不是“像人一样变聪明”，而是“<strong>像系统一样逐步成为你大脑外部的长期扩展层</strong>”。</p><p>所以，真正的临界点不是“等它变强”，而是 你何时开始以系统设计者的身份与它协作—— 你定义它记什么、它如何进化、它最终成为怎样的“第二工作大脑”。</p><p><strong>当你不再把它当聊天对象，而是当未来基础设施去驯养，你就已经站在下一代人机协作的入口处。</strong></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent Memory 评估测试方案：从指标体系到开源基准</title>
      <link>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/</link>
      <guid>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/</guid>
      <pubDate>Fri, 24 Oct 2025 08:38:29 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;em&gt;给那些正在构建智能体的开发者的一份记忆体检指南&lt;/em&gt;&lt;/p&gt;
&lt;br/&gt;
&lt;br/&gt;


&lt;p&gt;前段时间，我在调试一个几何 Agent。&lt;/p&gt;
&lt;p&gt;这个 Agent 能自动分析几何题、推理定理、调用绘图工具，看上去颇有点“自主学习”的影子。&lt;/p&gt;
&lt;br/</description>
        
      
      
      
      <content:encoded><![CDATA[<p><em>给那些正在构建智能体的开发者的一份记忆体检指南</em></p><br/><br/><p>前段时间，我在调试一个几何 Agent。</p><p>这个 Agent 能自动分析几何题、推理定理、调用绘图工具，看上去颇有点“自主学习”的影子。</p><br/><p>但问题也随之而来——它太健忘了。</p><p>有时候明明刚在上文证明过某个结论，下一步又开始怀疑它自己。</p><p>我给它接上了向量数据库、加了摘要器、甚至写了个小型索引器，但效果依旧不稳定。</p><br/><p>那时候我开始意识到：</p><p><strong>我们都在拼命强化 Agent 的“行动力”，却很少认真测量它的“记忆力”。</strong></p><br/><p>于是我决定系统地研究一下，怎么评估一个 Agent 的 Memory 模块。</p><p>今天这篇文章，写给所有已经或准备构建 Agent 工程的人。</p><p>希望帮你找到一套可落地、可复现的记忆评测方案。</p><br/><br/><h2 id="为什么要测记忆？"><a href="#为什么要测记忆？" class="headerlink" title="为什么要测记忆？"></a>为什么要测记忆？</h2><p>如果说大模型是 Agent 的大脑，那 Memory 就是它的长期神经系统。</p><p>没有记忆，再聪明的模型也只能“现想”而无法“积累”。</p><br/><p>在工程实践里，这会表现为：</p><ul><li>对话几轮后开始失忆</li><li>任务中重复提问</li><li>自相矛盾的人设</li><li>一旦上下文超过 10K，就变成另一位陌生 AI</li></ul><p>在我构建数学 Agent 的过程中，这种现象尤其明显。</p><p>模型在第一轮中记得“点 A 在圆上”，到了第五轮，它却开始假设“点 A 在圆外”。</p><p>一开始我以为问题在于 prompt 太短，后来才发现，真正的原因是<strong>缺乏系统的 Memory 测试</strong>。</p><p>没有量化，就无法优化。</p><p>于是，记忆评测成了我“修 Agent 心智”的重要一步。</p><br/><br/><h2 id="我们到底要测什么？"><a href="#我们到底要测什么？" class="headerlink" title="我们到底要测什么？"></a>我们到底要测什么？</h2><p>衡量 Agent 的记忆，其实可以借鉴人类心理学：</p><p>人类记忆分为短时、长时、情节、语义……</p><p>在工程里，我们可以把测试拆成几个核心问题：</p><ul><li><p><strong>能不能想起来？（Recall）</strong></p><p>例如：“用户之前提到的定理叫什么？”</p><p>对应指标 Recall@K。</p></li><li><p><strong>说得一致吗？（Consistency）</strong></p><p>“你昨天说角 A 等于 60°，今天怎么变 45° 了？”</p></li><li><p><strong>更新正确吗？（Update）</strong></p><p>“如果条件改变，旧记忆是否被覆盖？”</p></li><li><p><strong>能承认不知道吗？（Calibration）</strong></p><p>当没见过的信息出现，模型是否会拒答而不是胡编。</p></li><li><p><strong>记忆会不会膨胀？（Forgetting）</strong></p><p>随着交互增多，Agent 是否会被冗余记忆拖慢。</p></li><li><p><strong>多模态下是否还记得？（Multimodal Memory）</strong></p><p>看过的图、听过的指令，在下次提问时还能匹配回来吗？</p></li></ul><p>这六个维度几乎涵盖了所有常见的 Memory 问题。</p><p>我自己在调试几何 Agent 时，最常暴露的是前两个：</p><p>模型常常能“记得关键词”，却丢失了几何关系；</p><p>它能复述公式，但忘了变量的含义。</p><br/><p>这时候如果没有 Recall 或 Consistency 的量化指标，根本无法判断是哪一环在失灵。</p><br/><br/><h2 id="市面上能用的评测基准"><a href="#市面上能用的评测基准" class="headerlink" title="市面上能用的评测基准"></a>市面上能用的评测基准</h2><p>我查阅了近两年的研究，发现业界其实已经有了几套相当成熟的 Memory 评测基准。</p><p>只是大部分人没注意到它们可以直接上手。</p><br/><p>第一个是 <strong>LoCoMo</strong> —— Snap Research 的长对话评测。</p><p>它用上百轮对话测试模型的“长期记忆力”，</p><p>问题涵盖事实回忆、时间因果、甚至多模态对话。</p><p>在官方结果里，GPT-4 的 F1 分数只有 32，而人类平均是 88。</p><p>也就是说，我们距离真正的长期记忆，还差一整个物种的距离。</p><br/><p>第二个是 <strong>LongMemEval</strong> —— UCLA 团队的系统性基准。</p><p>它更像一个“记忆体检表”，从信息提取、跨会话推理、知识更新到拒答未知，一共五项指标。</p><p>如果你的 Agent 是任务型（比如客服、知识问答），</p><p>LongMemEval 是最值得尝试的那套测试。</p><br/><p>第三类是 <strong>多会话记忆集</strong>，如 Facebook 的 MSC 和百度的 Persona-Long。</p><p>它们主要考察“角色一致性”，模型是否能记住用户和自己的设定。</p><p>这对陪伴型、教育型 Agent 尤其重要。</p><p>这类基准就能帮你精确量化这种人格断层。</p><br/><br/><h2 id="评测指标背后的逻辑"><a href="#评测指标背后的逻辑" class="headerlink" title="评测指标背后的逻辑"></a>评测指标背后的逻辑</h2><p>这些基准的数据再多，也离不开几个核心指标：</p><ul><li><strong>Recall@K</strong> —— 检索召回率。衡量从记忆库中找到正确信息的能力。</li><li><strong>QA Accuracy &#x2F; F1</strong> —— 回答是否正确。最终看的是能否“说对话”。</li><li><strong>Consistency Score</strong> —— 是否前后矛盾。可以人工或自动打分。</li><li><strong>Rejection Rate</strong> —— 面对未知是否拒答，防止“假记忆”。</li><li><strong>ROUGE &#x2F; BLEU &#x2F; FactScore</strong> —— 用于生成式摘要或事件回忆任务。</li></ul><p>在工程里我发现，<strong>指标越细，越能暴露 Memory 模块的真实问题</strong>。</p><p>比如我的几何 Agent 在 Recall 很高时，QA 准确率却低；</p><p>这说明它能检索出正确片段，但推理时用错了。</p><p>有时候优化 Memory，不是改数据库，而是改检索策略。</p><br/><br/><h2 id="如何在自己的工程中测试记忆"><a href="#如何在自己的工程中测试记忆" class="headerlink" title="如何在自己的工程中测试记忆"></a>如何在自己的工程中测试记忆</h2><p>如果你已经有一个可运行的 Agent，不妨这样操作：</p><p><strong>第一步：准备测试集。</strong></p><p>可以直接使用 LoCoMo 或 LongMemEval 的公开数据，也可以用自己系统中的聊天日志，人工标几个“记忆点”。</p><p><strong>第二步：定义 Memory 模块接口。</strong></p><p>无论你用的是 LangChain 的 <code>ConversationBufferMemory</code>，还是自研的像 MemGPT 那样的分页机制，确保你能随时 dump 出它“记住了什么”。</p><p><strong>第三步：运行评测脚本。</strong></p><p>LoCoMo 官方仓库里有 <code>evaluate_qa.py</code>，只需改一下模型接口，就能测你的 Agent 的 QA F1。</p><p>LongMemEval 也有现成脚本，可以测 Recall、拒答率等。</p><p><strong>第四步：观察与反思。</strong></p><p>别只看准确率，更要看错误类型。</p><p>有些错误是检索不到，有些是检索到但没用。</p><p>如果可能，把指标接入 CI 流程，每次更新记忆逻辑时自动跑一轮。</p><p>当你第一次看到自己的 Agent 在 LoCoMo 上得分只有二十几，那种“原来它根本没记住我”的感觉，会比任何 bug 都真实。</p><br/><br/><h2 id="可直接上手的评测-Demo"><a href="#可直接上手的评测-Demo" class="headerlink" title="可直接上手的评测 Demo"></a>可直接上手的评测 Demo</h2><p>如果你不想从零开始，可以直接试这几个项目：</p><ul><li><p><strong>LoCoMo 官方评测工具</strong>：<br><a href="https://github.com/snap-research/locomo">https://github.com/snap-research/locomo</a></p><p>包含超长对话数据和 QA 评测脚本。</p></li><li><p><strong>LongMemEval 基准</strong>：<br><a href="https://github.com/xiaowu0162/LongMemEval">https://github.com/xiaowu0162/LongMemEval</a></p><p>五维记忆测试模板，支持 HuggingFace 模型。</p></li><li><p><strong>MemoryBank &#x2F; SiliconFriend</strong>：<br><a href="https://github.com/zhongwanjun/MemoryBank-SiliconFriend">https://github.com/zhongwanjun/MemoryBank-SiliconFriend</a></p><p>内置遗忘机制，可在本地启动一个有“长期记忆”的中文聊天机器人。</p></li><li><p><strong>Reflexion Agent</strong>：<br><a href="https://github.com/noahshinn/reflexion">https://github.com/noahshinn/reflexion</a></p><p>教你如何让 Agent 自己反思并记录经验。</p></li></ul><p>我个人推荐先跑 LongMemEval。</p><p>它轻量、指标明确、结果易解释。</p><p>如果你的 Agent 通过了它，再挑战 LoCoMo。</p><p>后者是真正的“长对话炼狱”，我第一次跑完时 GPU 都快烤化了。</p><br/><br/><h2 id="评测之外的坑与反思"><a href="#评测之外的坑与反思" class="headerlink" title="评测之外的坑与反思"></a>评测之外的坑与反思</h2><p>我最深的感触是：<strong>记忆问题不是硬件问题，是认知问题。</strong></p><br/><p>我曾花一周时间优化索引算法，却忽略了最根本的逻辑——Agent 根本不知道“什么时候该想起某件事”。</p><p>它拥有庞大的向量库，却缺乏触发机制。</p><p>那一刻我突然明白：真正需要评测的，不仅是“记得多少”，更是“何时想起”。</p><br/><p>另外一个坑是“假记忆”。</p><p>我曾让模型复盘教师一天的教学日志，它开始一本正经地编造“我今天教了学生三角函数”。</p><p>这时拒答率指标就很关键。</p><p>一个 Agent 不该无所不答，<strong>能承认不知道，本身就是记忆成熟的表现。</strong></p><br/><br/><h2 id="未来的方向"><a href="#未来的方向" class="headerlink" title="未来的方向"></a>未来的方向</h2><p>目前的基准都还停留在文本和问答层面。</p><p>未来的评测一定会走向更真实的多模态场景——让 Agent 看图、听音、记住空间位置，甚至在几天后的任务中回忆它曾经画过的图。</p><br/><p>我相信未来的开发工具里，会出现一种新的概念：<strong>MemoryOps</strong>。</p><p>就像今天的 DevOps 管理部署一样，MemoryOps 将监控和测试 Agent 的记忆健康。</p><p>到那时候，我们或许可以做到：“每次模型上线前，都跑一份记忆体检报告。”</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>写这篇文章时，我想到一个比喻：</p><p>训练模型像造大脑，但让它拥有记忆，才是赋予它灵魂。</p><p>所以，在你下一次调试 Agent 时，<br>别只盯着推理精度和响应速度，<br>也问问它一句：</p><blockquote><p>“你还记得我们第一次对话吗？”</p></blockquote><p><strong>没有被测过的记忆，终将变成幻觉。</strong></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent 如何避免记忆漂移：三大策略与工程实践</title>
      <link>https://blog.liluhui.cn/2025/10/17/Preventing-Memory-Drift-in-Agent-Systems-Three-Core-Techniques-and-Engineering-Implementations/</link>
      <guid>https://blog.liluhui.cn/2025/10/17/Preventing-Memory-Drift-in-Agent-Systems-Three-Core-Techniques-and-Engineering-Implementations/</guid>
      <pubDate>Fri, 17 Oct 2025 08:17:39 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h2&gt;&lt;p&gt;长期运行的 Agent 容易出现「记忆漂移」：随着时间推移，其记忆内容被反复重写、压缩、整合，逐渐偏离原始语义，进而</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>长期运行的 Agent 容易出现「记忆漂移」：随着时间推移，其记忆内容被反复重写、压缩、整合，逐渐偏离原始语义，进而导致自相矛盾、逻辑错乱或行为失常。</p><p>本篇文章总结了三种抵御记忆漂移的核心策略：</p><ul><li><p><strong>定期总结</strong>：定时摘要长对话内容，减少信息积压，保持上下文可控</p></li><li><p><strong>批处理蒸馏</strong>：从多轮对话中提炼出用户偏好与抽象知识，提升记忆质量</p></li><li><p><strong>冲突合并</strong>：发现重复或冲突内容，统一更新或标记失效，维持一致性</p></li></ul><p>结合实际系统如 MemGPT、AWS AgentCore、LangChain 等，下文提供了对应的工程实现与开源项目指引，适合构建具备长期稳定记忆的 Agent 架构。</p><br/><br/><h2 id="引言：智能体的「记忆漂移」现象"><a href="#引言：智能体的「记忆漂移」现象" class="headerlink" title="引言：智能体的「记忆漂移」现象"></a>引言：智能体的「记忆漂移」现象</h2><p>在构建具备长期行为一致性与任务连贯性的智能体（Agent）时，「记忆」机制成为不可或缺的一环。</p><p>与静态函数调用或短程上下文不同，Agent 的长期记忆的目标是支持跨轮次对话、持续学习与经验积累。</p><p>然而，随着记忆体量增长，「记忆漂移」问题逐渐显现：<strong>原始信息在多次重写、压缩、提取过程中偏离原意，导致 Agent 出现自相矛盾、事实错乱或行为不一致等现象</strong>。</p><p>这一问题本质上源自智能体在长期运行中所面临的信息冗余、表达歧义与语义演化挑战。</p><p>就像我们日常使用电脑，当桌面文件堆积如山、不及时分类整理，搜索效率降低、重复文件泛滥，最终反过来干扰工作。</p><br/><p>本文将深入探讨三种关键的抗记忆漂移策略：定期总结、批处理蒸馏、冲突合并。</p><p>结合最新的开源实践与研究进展，为构建稳定可靠的 Agent Memory 系统提供参考路径。</p><br/><br/><h2 id="策略一：定期总结-——-让记忆不过载"><a href="#策略一：定期总结-——-让记忆不过载" class="headerlink" title="策略一：定期总结 —— 让记忆不过载"></a>策略一：定期总结 —— 让记忆不过载</h2><p>如果你曾被对话窗口滚动到看不见开头、或者某个 Agent 总是健忘你提过的偏好，那大概率它缺了一套靠谱的定期总结机制。</p><p><strong>定期总结（Periodic Summarization）</strong> 是让智能体隔一段时间就回头看看：我刚刚都经历了什么？有哪些是值得留下的？</p><p>就像人写日记，不可能每句话都记，而是总结当日要点。</p><br/><p>工程实现示例：</p><ul><li><strong>LangChain SummaryBufferMemory</strong>：在上下文快要塞不下之前，自动将早期内容打包为摘要，留出空间给后续对话。</li><li><strong>MemGPT 滚动摘要</strong>：采用分层内存结构，短期记忆用完就「递归摘要」进中期记忆，就像桌面上的临时文件转存进资料夹。</li></ul><br/><p>一些提醒：</p><ul><li>连续多次摘要容易信息腐蚀，逐层失真。建议保留关键原句作为锚点。</li><li>可引入 pin memory 机制，确保核心事实始终原样保留。</li></ul><br/><br/><h2 id="策略二：批处理蒸馏-——-从日志中提炼稳定认知"><a href="#策略二：批处理蒸馏-——-从日志中提炼稳定认知" class="headerlink" title="策略二：批处理蒸馏 —— 从日志中提炼稳定认知"></a>策略二：批处理蒸馏 —— 从日志中提炼稳定认知</h2><p>有没有遇到过这种情况：你和 AI 聊了十几次后，TA还是不了解你？</p><p>这时，问题可能不在对话内容，而在于没有把那些碎片化记忆凝练为稳定认知。</p><p><strong>批处理蒸馏（Batched Distillation）</strong> 就像是回顾一整个项目后，总结出规律、套路、偏好，存档到知识库中。</p><br/><p>工程实现示例</p><ul><li><strong>Generative Agents（Stanford）</strong>：每个模拟人每天睡觉前会进行反思，总结一天所学所感。</li><li><strong>AWS AgentCore Memory</strong>：让新事件与历史记忆对话，LLM负责融合成一条高层认知。</li><li><strong>Letta MemGPT 的睡眠代理</strong>：利用主线程空闲时间悄悄整理旧资料，像我们周末清理相册一样。</li></ul><br/><p>提示模版示例</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请阅读以下 N 段日志，总结出关于用户的稳定特征、目标与情绪倾向。</span><br></pre></td></tr></table></figure><br/><br/><h2 id="策略三：冲突合并-——-去重与知识更新"><a href="#策略三：冲突合并-——-去重与知识更新" class="headerlink" title="策略三：冲突合并 —— 去重与知识更新"></a>策略三：冲突合并 —— 去重与知识更新</h2><p>Agent 说你喜欢喝美式，过一会儿又说你最讨厌咖啡？这不是它耍你，而是记忆没合并好的情况。</p><p>长期运行的智能体一定会遇到记忆冲突：用户今天说“我吃素”，下个月说“这家烧烤真香”。</p><br/><p><strong>不消解这些冲突，Agent 就会陷入人格分裂式输出。</strong></p><p>合并策略</p><ul><li><strong>语义合并</strong>：让 LLM判断两个说法是重复、矛盾还是递进。</li><li><strong>失效机制</strong>：标记旧记忆为「历史版本」，不参与默认推理。</li><li><strong>仲裁代理</strong>：在多智能体系统中，用专职 Agent 仲裁歧义事实。</li></ul><br/><p>工程落地示例</p><ul><li><strong>AWS AgentCore Memory</strong>：每条新记忆自动触发冲突检测与合并逻辑。</li><li><strong>Memori Memory Engine</strong>：具备版本控制与自动协调机制，适合长生命周期记忆管理。</li></ul><br/><br/><h2 id="拿来即用，策略对比与实践参考"><a href="#拿来即用，策略对比与实践参考" class="headerlink" title="拿来即用，策略对比与实践参考"></a>拿来即用，策略对比与实践参考</h2><table><thead><tr><th>策略名称</th><th>作用重点</th><th>典型触发时机</th><th>工程实现代表</th><th>风险点</th></tr></thead><tbody><tr><td>定期总结</td><td>控制体积，保存语境</td><td>消息超过阈值</td><td>LangChain、MemGPT</td><td>摘要失真</td></tr><tr><td>批量蒸馏</td><td>抽象提炼，积累知识</td><td>阶段归档或反思时刻</td><td>AWS AgentCore、Generative Agents</td><td>抽象过度</td></tr><tr><td>冲突合并</td><td>保持一致性，去重纠错</td><td>新旧冲突产生时</td><td>Memori、Letta、仲裁代理机制</td><td>决策不透明</td></tr></tbody></table><p><strong>组合建议</strong>：构建分层内存结构 + 批次反思 + 定期摘要 + 合并管道，能形成防漂移的“记忆治理闭环”。</p><br/><p>可用工具与开源实现一览：</p><table><thead><tr><th>名称</th><th>类型</th><th>地址</th><th>支持策略</th></tr></thead><tbody><tr><td>LangChain SummaryMemory</td><td>工具库</td><td><a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a></td><td>定期总结</td></tr><tr><td>MemGPT &#x2F; Letta</td><td>代理框架</td><td><a href="https://github.com/letta-ai/letta">https://github.com/letta-ai/letta</a></td><td>三者兼具</td></tr><tr><td>AgentCore Memory</td><td>云平台模块</td><td><a href="https://aws.amazon.com/bedrock/agents">https://aws.amazon.com/bedrock/agents</a></td><td>蒸馏 + 合并</td></tr><tr><td>Memori</td><td>记忆服务</td><td><a href="https://github.com/GibsonAI/memori">https://github.com/GibsonAI/memori</a></td><td>冲突合并</td></tr><tr><td>generative_agents</td><td>模拟研究框架</td><td><a href="https://github.com/joonspk-research/generative_agents">https://github.com/joonspk-research/generative_agents</a></td><td>批量蒸馏</td></tr></tbody></table><br/><br/><h2 id="写在最后：从记忆治理到认知形成"><a href="#写在最后：从记忆治理到认知形成" class="headerlink" title="写在最后：从记忆治理到认知形成"></a>写在最后：从记忆治理到认知形成</h2><p>智能体的记忆，终究不是日志数据库，而是它认知世界、理解人类、规划行动的基础。过去我们关注的是它记不记得；而现在，我们必须关心它记得对不对、准不准、合不合理。</p><p>三类策略分别作用于记忆系统的不同阶段：<strong>定期总结</strong> 帮它活在有限当下，<strong>批量蒸馏</strong> 帮它形成抽象理解，<strong>冲突合并</strong> 让它在真伪之间保持清醒。</p><p>三者结合，才可能走向真正可持续的 Agent 思维。</p><br/><p>我的观点是，未来 Agent 开发，不该再把 Memory 当附属功能，而应当作为核心设计范式来思考。</p><p>就像我们在产品中设计信息架构一样，我们也应该设计 Agent 的认知架构。</p><p>构建一个能学、能忘、能修的 Agent，不只是技术挑战，更是认知工程。</p><p>根基不稳，智能难远。</p><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/17/Preventing-Memory-Drift-in-Agent-Systems-Three-Core-Techniques-and-Engineering-Implementations/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>什么是多 Agent 系统？从单体模型到协作智能的进化</title>
      <link>https://blog.liluhui.cn/2025/10/15/What-Is-a-Multi-Agent-System-From-a-Single-LLM-to-Collaborative-Intelligence/</link>
      <guid>https://blog.liluhui.cn/2025/10/15/What-Is-a-Multi-Agent-System-From-a-Single-LLM-to-Collaborative-Intelligence/</guid>
      <pubDate>Wed, 15 Oct 2025 08:11:28 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;过去一年，我们见证了无数“大模型”带来的奇迹。&lt;br&gt;它们能写代码、能画图、能写策划书，甚至能帮你找 bug。  &lt;/p&gt;
&lt;p&gt;于是一个问题出现了：  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;既然大模型已经这么强了，&lt;strong&gt;为什么还要搞那么复杂的「多 Agent</description>
        
      
      
      
      <content:encoded><![CDATA[<p>过去一年，我们见证了无数“大模型”带来的奇迹。<br>它们能写代码、能画图、能写策划书，甚至能帮你找 bug。  </p><p>于是一个问题出现了：  </p><blockquote><p>既然大模型已经这么强了，<strong>为什么还要搞那么复杂的「多 Agent 系统」</strong>？ </p><p>是不是就是多开几个大模型？  </p></blockquote><p>其实，多 Agent 不是“堆模型”，而是一次<strong>智能结构的转变</strong>。  </p><p>它让我们开始用「团队」的方式去理解 AI，让语言模型从“单人作坊”变成“协作组织”。  </p><br/><br/><h2 id="一个-Agent-不够用吗？"><a href="#一个-Agent-不够用吗？" class="headerlink" title="一个 Agent 不够用吗？"></a>一个 Agent 不够用吗？</h2><p>想象你让 GPT 帮你开发一个网站：它能写前端，也能设计数据库，还能生成测试代码。 </p><p>但很快你会发现一个问题，它在写完后<strong>自测永远都通过</strong>，逻辑错误自己也检查不出来。  </p><br/><p>于是人们开始尝试：<br>让一个 GPT 写，另一个 GPT 审；<br>让一个负责规划，另一个负责执行；<br>甚至让两个 GPT 吵一架，看谁的结论更靠谱。  </p><p>这就是最早的 “<strong>multi-agent</strong>” 思想萌芽。<br>不是因为模型不行，而是因为<strong>智能需要结构</strong>。<br>单个 Agent 再强，也无法同时兼顾规划、执行、评估、反思这些不同心智功能。  </p><br/><br/><h2 id="为什么要多个-Agent：从「个体智能」到「集体智能」"><a href="#为什么要多个-Agent：从「个体智能」到「集体智能」" class="headerlink" title="为什么要多个 Agent：从「个体智能」到「集体智能」"></a>为什么要多个 Agent：从「个体智能」到「集体智能」</h2><h3 id="1️⃣-分工让思考更高效"><a href="#1️⃣-分工让思考更高效" class="headerlink" title="1️⃣ 分工让思考更高效"></a>1️⃣ 分工让思考更高效</h3><p>就像人类社会的分工一样，AI 也有“认知负荷”。<br>一个 Agent 的上下文再长，也无法同时规划和执行。  </p><br/><p>于是多 Agent 系统会把任务拆解：  </p><ul><li><strong>Planner</strong> 规划任务  </li><li><strong>Executor</strong> 调用工具或编写代码  </li><li><strong>Critic</strong> 检查结果、给出反馈</li></ul><br/><p>论文参考：</p><ul><li>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework (Hong et al., 2023)  </li><li>Reflexion: Verbal Reinforcement Learning with LLMs (Shinn et al., 2023)</li></ul><h3 id="2️⃣-不同视角，减少偏差"><a href="#2️⃣-不同视角，减少偏差" class="headerlink" title="2️⃣ 不同视角，减少偏差"></a>2️⃣ 不同视角，减少偏差</h3><p>当多个 Agent 相互讨论、辩论、复核时，错误就更难溜走。<br>这就是所谓的 “<strong>social critique</strong>” —— AI 用社会互动的方式，来弥补单模型思维的盲点。 </p><br/><p>例如 <em>CAMEL (2023)</em> 中，两个 LLM 扮演不同角色，通过角色扮演协作完成任务，结果准确率显著高于单模型执行。  </p><h3 id="3️⃣-群体记忆与长期任务"><a href="#3️⃣-群体记忆与长期任务" class="headerlink" title="3️⃣ 群体记忆与长期任务"></a>3️⃣ 群体记忆与长期任务</h3><p>一个模型记不住太久的上下文，而多 Agent 系统可以共享外部记忆。  </p><p><strong>LangGraph</strong> 就用图状结构记录各 Agent 的状态与结果，<br><strong>AutoGen</strong> 则通过消息路由实现多轮协作。  </p><p>这让智能系统第一次能像组织一样 <strong>有状态、有记忆、有分工</strong>。  </p><br/><br/><h2 id="「Agent」vs-「Multi-Agents」：什么时候该用哪种？"><a href="#「Agent」vs-「Multi-Agents」：什么时候该用哪种？" class="headerlink" title="「Agent」vs 「Multi Agents」：什么时候该用哪种？"></a>「Agent」vs 「Multi Agents」：什么时候该用哪种？</h2><p>不是所有任务都需要多 Agent，整理了下面这张表，帮你快速判断：  </p><table><thead><tr><th>场景类型</th><th>适合单 Agent</th><th>适合多 Agent</th></tr></thead><tbody><tr><td>简单问答、摘要、翻译</td><td>✅</td><td>❌</td></tr><tr><td>多步骤推理（数学、逻辑）</td><td>⚠️</td><td>✅</td></tr><tr><td>代码生成与测试</td><td>❌</td><td>✅</td></tr><tr><td>长期运营任务（游戏、规划）</td><td>❌</td><td>✅</td></tr><tr><td>决策 &#x2F; 研究讨论类任务</td><td>⚠️</td><td>✅</td></tr></tbody></table><p>👉 简单说：  </p><ul><li><strong>单 Agent</strong>：快、轻、直接。  </li><li><strong>多 Agent</strong>：稳、复杂、可解释。</li></ul><p>它更像是“团队思维”，适用于有阶段、有反馈、有评估的任务。  </p><br/><br/><h2 id="协作机制不是「堆模型」，而是「编排心智」"><a href="#协作机制不是「堆模型」，而是「编排心智」" class="headerlink" title="协作机制不是「堆模型」，而是「编排心智」"></a>协作机制不是「堆模型」，而是「编排心智」</h2><p>多 Agent 的价值在于协作机制，而不是数量。  </p><p>常见机制包括：  </p><ol><li><strong>角色定义</strong>：每个 Agent 有独立 Prompt 与目标（Planner &#x2F; Coder &#x2F; Tester）  </li><li><strong>通信协议</strong>：消息路由、对话规则（AutoGen 异步消息机制）  </li><li><strong>状态共享</strong>：外部记忆、数据库或上下文图（LangGraph）  </li><li><strong>自我审查</strong>：互评或辩论结构（CAMEL、Reflexion）  </li><li><strong>任务编排</strong>：从自然语言生成完整工作流（MetaGPT、CrewAI）</li></ol><p>工程参考：</p><ul><li>Microsoft AutoGen</li><li>LangChain LangGraph  </li><li>MetaGPT  </li><li>CAMEL-AI  </li><li>CrewAI</li></ul><br/><br/><h2 id="争议：多-Agent-是过渡，还是未来？"><a href="#争议：多-Agent-是过渡，还是未来？" class="headerlink" title="争议：多 Agent 是过渡，还是未来？"></a>争议：多 Agent 是过渡，还是未来？</h2><p>目前学术界有两种声音：  </p><p>1️⃣ <strong>单体强化派</strong><br>认为随着大模型上下文与记忆增强（如 GPT-o3、Gemini 2.5），<br>未来单模型内部就能模拟多角色思维，不再需要外部 Agent。  </p><p>2️⃣ <strong>协作网络派</strong><br>认为未来智能系统会像社会那样，通过标准协议（如 MCP、A2A）让不同 Agent 协作，<br>形成一个“分布式智能生态”。  </p><p>无论是哪种，multi-agent 都是一个重要的中间态：它让 AI 学会如何组织智能、引导协作，为真正的“社会化 AI”打下基础。  </p><br/><br/><h2 id="结语：AI，也需要同事"><a href="#结语：AI，也需要同事" class="headerlink" title="结语：AI，也需要同事"></a>结语：AI，也需要同事</h2><p>多 Agent 的出现，不是因为模型不够强，而是因为<strong>智能本身就不是单线程的</strong>。  </p><p>人类的思考依靠分工协作——感知、记忆、计划、执行、反思。  </p><p>AI 也在重走这条路，只不过是以 Agent 的形式。  </p><br/><p>当我们不再把智能看作一个「模型」，  </p><p>而是一群「协作的心智」时，  </p><p>也许我们就更接近真正的智能社会。  </p><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><p><strong>学术论文：</strong>  </p><ul><li><a href="https://arxiv.org/abs/2303.17760">Li et al. (2023) — <em>CAMEL: Communicative Agents for “Mind” Exploration of LLM Society</em></a></li><li><a href="https://arxiv.org/abs/2308.00352">Hong et al. (2023) — <em>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</em></a></li><li><a href="https://arxiv.org/html/2303.11366">Shinn et al. (2023) — <em>Reflexion: Verbal Reinforcement Learning for LLMs</em></a></li><li><a href="https://arxiv.org/abs/2304.03442">Park et al. (2023, Stanford) — <em>Generative Agents: Interactive Simulacra of Human Behavior</em></a></li><li><a href="https://arxiv.org/abs/2503.13657">Cemri et al. (2024) — <em>MAST: Multi-Agent System Taxonomy of Failures</em></a></li><li><a href="https://arxiv.org/abs/2503.05473">Mamie et al. (2025) - <em>The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms</em></a></li></ul><p><strong>开源工程：</strong>  </p><ul><li><a href="https://github.com/microsoft/autogen">Microsoft AutoGen</a>  </li><li><a href="https://github.com/langchain-ai/langgraph">LangChain LangGraph</a>  </li><li><a href="https://github.com/FoundationAgents/MetaGPT">MetaGPT</a>  </li><li><a href="https://github.com/camel-ai/camel">CAMEL-AI</a>  </li><li><a href="https://github.com/crewAIInc/crewAI">CrewAI</a>  </li><li><a href="https://github.com/openai/swarm">OpenAI Swarm</a> (实验性)</li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/15/What-Is-a-Multi-Agent-System-From-a-Single-LLM-to-Collaborative-Intelligence/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>什么是 Agent Memory？探索 AI 如何理解并记住你的话</title>
      <link>https://blog.liluhui.cn/2025/10/10/Understanding_Agent_Memory_The_Mechanisms_Behind_AI%E2%80%99s_Ability_to_Remember/</link>
      <guid>https://blog.liluhui.cn/2025/10/10/Understanding_Agent_Memory_The_Mechanisms_Behind_AI%E2%80%99s_Ability_to_Remember/</guid>
      <pubDate>Fri, 10 Oct 2025 08:18:47 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;记忆并非理所当然，理解-AI-时代的记忆&quot;&gt;&lt;a href=&quot;#记忆并非理所当然，理解-AI-时代的记忆&quot; class=&quot;headerlink&quot; title=&quot;记忆并非理所当然，理解 AI 时代的记忆&quot;&gt;&lt;/a&gt;记忆并非理所当然，理解 AI 时代的记忆&lt;/h2&gt;&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="记忆并非理所当然，理解-AI-时代的记忆"><a href="#记忆并非理所当然，理解-AI-时代的记忆" class="headerlink" title="记忆并非理所当然，理解 AI 时代的记忆"></a>记忆并非理所当然，理解 AI 时代的记忆</h2><p>记忆是我们日常生活中不可或缺的一部分。<br>从小到大，我们积累的经验和知识形成了对世界的认知框架。<br>人类的记忆帮助我们在面对未知时作出反应，记住重要的事情，避免重复犯错。<br>但是，<strong>如果我们只将 记忆 理解为人类自然存在的生理过程，那么在面对人工智能（AI）时代的 Agent Memory 时，可能会产生误解</strong>。</p><p>在 AI 的世界里，记忆并不像人类那样自然而然地存在。<br>这也是为什么我们需要从一个全新的角度，去理解和探索 Agent Memory —— 它不仅仅是记住过去的互动，而是让机器能够像人类一样，通过记忆优化每次决策的能力。</p><p>通过这篇文章，我将带你走进 Agent Memory 的世界，解答这个关键问题： <strong>Agent Memory 是什么？为什么它对智能代理至关重要？</strong></p><br/><br/><h2 id="什么是-Agent-Memory？"><a href="#什么是-Agent-Memory？" class="headerlink" title="什么是 Agent Memory？"></a>什么是 Agent Memory？</h2><p><strong>Agent Memory 是人工智能代理（AI Agent）中的“记忆系统”，它是人类设计的存储、更新&#x2F;遗忘和检索机制。</strong><br>它使得 AI 能够在与用户互动时，不仅记住和利用历史信息，还能根据任务的需要进行更新、遗忘和优化，以提供更加智能、个性化的服务。</p><p><strong>可以将 Agent Memory 理解为一套“存储-更新-检索”的策略。</strong><br>不同的实现路径和技术方案使得这一系统的具体应用形式多种多样。无论是短期记忆、长期记忆，还是对信息的筛选和检索方式，它都帮助 AI 在每次与用户的互动中，保持一致性、理解上下文，并从过去的经验中不断学习和调整。</p><p>举个例子，当你与智能语音助手对话时，它不仅记得你昨天询问过身体不舒服的症状，还会根据历史信息优化后续的建议。<strong>这不仅是记忆的简单存储，而是通过精心设计的策略，让 Agent Memory 具备了学习和适应的能力</strong>。</p><p>需要特别注意的是，在 Agent Memory 的设计中，<strong>上下文 和 记忆 是两个不同的概念</strong>。</p><ul><li><strong>上下文</strong>: 只存在于当前的交互中，它帮助 AI 理解当前对话或任务的背景。例如，用户正在询问今天的天气，AI 就会利用这个即时上下文来生成回答，但当对话结束时，这些上下文信息将不再保存。</li><li><strong>记忆</strong>：则是长期存储的信息，能够跨会话、跨任务持续保持，并帮助 AI 在未来的交互中做出更加个性化和高效的响应。</li></ul><p>这与人类的思维过程相似：我们在对话中可能暂时记住某些信息（上下文），但“记忆”则是指我们从过去的经历中提取经验，以便做出更明智的决策，而不仅仅局限于当前的对话。</p><br/><br/><h2 id="Agent-Memory-的类型"><a href="#Agent-Memory-的类型" class="headerlink" title="Agent Memory 的类型"></a>Agent Memory 的类型</h2><p>Agent Memory 并不是单一的记忆体，而是由多个层次和类型组成，旨在应对不同的应用场景。<br>这些记忆类型就像人类不同的记忆系统，分别负责即时的工作任务和长期的知识储备。<br>以下是 <strong>Agent Memory 的五种基本类型</strong>，详细区别可看同系列文章<a href="https://blog.liluhui.cn/2025/09/19/Agent-Memory-Five-Layer-Model-and-Engineering-Implementation/">《Agent 记忆五层模型与工程落地》</a>：</p><ul><li><strong>工作记忆（Working Memory）</strong><br> 工作记忆负责存储和处理当前任务或对话中的即时信息。它类似于人类的短期记忆，帮助 AI 在与用户的互动中保持上下文，处理即时的需求。工作记忆通常在会话结束后被清除，以便为新的任务腾出空间。</li><li><strong>情景记忆（Episodic Memory）</strong><br> 情景记忆负责记录和存储与特定事件相关的信息。这类记忆帮助 AI 记住特定的事件、用户的行为和相关的交互情境，从而提供更具上下文关联性的反馈。</li><li><strong>语义记忆（Semantic Memory）</strong><br> 语义记忆包含了广泛的通用知识和事实，例如世界常识、数学公式、日期等。这类记忆不依赖于个体的经历，而是基于普遍接受的事实和信息。</li><li><strong>程序性记忆（Procedural Memory）</strong><br> 程序性记忆用于存储如何执行特定任务或操作的知识。这些知识是 Agent 在反复执行某项技能时积累的，类似于人类如何记住特定技能的操作步骤。</li><li><strong>外部持久记忆（External Persistent Memory）</strong><br> 外部持久记忆负责记录并归档用户的长期历史和行为数据。这类记忆帮助 AI 在多个会话、任务和平台之间保持一致性，便于长期个性化和精准反馈。</li></ul><br/><br/><h2 id="Agent-Memory-如何工作？"><a href="#Agent-Memory-如何工作？" class="headerlink" title="Agent Memory 如何工作？"></a>Agent Memory 如何工作？</h2><p><strong>Agent Memory 的核心工作流是存储、更新和检索信息。</strong><br>AI 通过以下几个步骤有效管理记忆，使得每次与用户的互动都更加精准和高效：</p><ul><li><strong>信息存储</strong><br> AI 会根据一定的策略存储重要的交互信息，这些信息可能来自用户输入的对话内容、历史数据或任务需求。为了实现高效存储，信息通常通过数据库或嵌入式向量存储（如 FAISS）来实现。</li><li><strong>信息更新</strong><br> 信息并非一成不变，随着新的交互发生，AI 会更新其记忆。例如，用户的偏好、常见问题或新的任务需求都可以被记录下来，并反映在长期记忆中。信息的更新使得 AI 能够不断优化其响应。</li><li><strong>信息检索</strong><br> AI 必须能够根据当前需求，迅速从记忆中提取相关信息。这一过程类似于检索，通过技术手段（如向量数据库和嵌入检索），AI 可以从大量存储的数据中找到最匹配的内容，从而提供更加智能和精准的回应。</li></ul><br/><br/><h2 id="Agent-Memory-在实际应用中的作用"><a href="#Agent-Memory-在实际应用中的作用" class="headerlink" title="Agent Memory 在实际应用中的作用"></a>Agent Memory 在实际应用中的作用</h2><ol><li><strong>个性化体验</strong><br>Agent Memory 使得 AI 能够记住并理解用户的偏好和需求，从而提供更加个性化的服务。例如，智能推荐系统能根据用户的历史行为推荐商品或服务。</li><li><strong>多轮对话与一致性</strong><br> 在多轮对话中，Agent Memory 使得 AI 能够保持对话的上下文和连贯性，不会重复提问或对话断裂。AI 可以更好地理解用户的意图，从而提供更智能的回应。</li><li><strong>提高效率</strong><br> 通过有效的记忆管理，AI 能够减少用户重复输入信息的需求，提升交互效率。例如，智能助手记住用户的常见问题，能快速提供相关回答，而无需每次重新输入。</li></ol><br/><br/><h2 id="Agent-Memory-的技术实现"><a href="#Agent-Memory-的技术实现" class="headerlink" title="Agent Memory 的技术实现"></a>Agent Memory 的技术实现</h2><p><strong>Agent Memory 的实现依赖于多种技术策略和组件，目的是让 AI 代理在与用户的交互中保持智能化的记忆能力。</strong><br>它的核心机制包括 存储、更新、遗忘和检索，并通过不断优化这些环节来提供个性化和高效的服务。关键技术要素包括：</p><ol><li><p><strong>存储机制</strong>，将用户交互转化为长期可访问的格式：</p><ul><li>向量数据库：如 <strong>FAISS</strong>、<strong>Pinecone</strong>，通过 <strong>embedding</strong> 向量存储，支持高效相似度检索。</li><li>知识库：存储通用知识，如语法、历史事实等，增强记忆能力。</li></ul></li><li><p><strong>信息更新与遗忘策略</strong>，根据新的交互进行动态更新和优化：</p><ul><li>增量更新：将新信息有针对性地加入，而非覆盖已有记忆。</li><li>遗忘机制：删除过时或无用数据，避免记忆膨胀。</li><li>信息压缩：整理和压缩冗余信息，提高存储效率。</li></ul></li><li><p><strong>信息检索机制</strong>，高效检索存储数据：</p><ul><li>向量检索：通过 <strong>FAISS</strong> 等工具，根据相似度快速检索相关信息。</li><li>查询优化：结合 <strong>contextual embeddings</strong> 精确理解用户意图，提升检索精度。</li></ul></li><li><p><strong>多模态信息整合</strong>，结合图像、语音、文本等多种数据形式，提升 AI 的综合响应能力。</p><ul><li>例如，<strong>多模态图像识别系统</strong>，同时处理文本与图像信息，提供精准反馈。</li></ul></li><li><p><strong>外部持久记忆与分布式存储</strong><br>随着数据量的增加，部分记忆会存储到外部系统（如云数据库），保证跨设备、跨平台的持续更新与访问。</p></li></ol><br/><br/><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过合理的 Agent Memory 管理，人工智能可以从单纯的工具进化为智能的交互伙伴，能够理解用户需求、提供个性化的反馈并实现更高效的互动。<br>未来，随着技术的不断进步，Agent Memory 将会在个性化服务、智能化交互以及多代理协作中发挥越来越重要的作用。<br>毕竟，<strong>让“智能”真的成为智能是需要经验积累的，那个“经验”正是在记忆之上的思考。</strong><br>当然现在 Agent Memory 在提升智能代理的能力和用户体验方面仍然面临一些挑战：比如记忆膨胀问题。随着时间推移，AI 可能会积累大量信息，这可能导致记忆库膨胀，影响性能。</p><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><a href="https://arxiv.org/abs/2504.19413">Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</a></li><li><a href="https://arxiv.org/abs/2502.12110">A-MEM: Agentic Memory for LLM Agents</a></li><li><a href="https://www.researchgate.net/publication/388144017_Memory_Architectures_in_Long-Term_AI_Agents_Beyond_Simple_State_Representation">Memory Architectures in Long-Term AI Agents</a></li><li><a href="https://mem0.ai/blog/memory-in-agents-what-why-and-how">AI Agent Memory: What, Why and How It Works</a></li><li><a href="https://www.ibm.com/think/topics/ai-agent-memory">What Is AI Agent Memory? | IBM</a></li><li><a href="https://medium.com/@nirdiamant21/building-an-ai-agent-with-memory-and-adaptability-cdbc428bc36c">Building an AI Agent with Memory and Adaptability</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/10/Understanding_Agent_Memory_The_Mechanisms_Behind_AI%E2%80%99s_Ability_to_Remember/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>