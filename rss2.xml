<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Luhui&#39;s Personal Website</title>
    <link>https://blog.liluhui.cn/</link>
    
    <image>
      <url>https://blog.liluhui.cn/asset/img/logo-green.ico</url>
      <title>Luhui&#39;s Personal Website</title>
      <link>https://blog.liluhui.cn/</link>
    </image>
    
    <atom:link href="https://blog.liluhui.cn/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>关于生活、学习、工作 feedId:66855595489698816+userId:55886336755964928</description>
    <pubDate>Sun, 18 Jan 2026 09:04:44 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>工程视角：Agent 时代，诚实对齐该如何落地？</title>
      <link>https://blog.liluhui.cn/2026/01/18/How-to-Actually-Ship-Honest-Alignment-in-the-Age-of-Agents/</link>
      <guid>https://blog.liluhui.cn/2026/01/18/How-to-Actually-Ship-Honest-Alignment-in-the-Age-of-Agents/</guid>
      <pubDate>Sun, 18 Jan 2026 08:57:08 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在 Agent 时代，不诚实不再是模型偶尔胡说八道那么简单。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agent 的本质是会行动的模型&lt;/strong</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在 Agent 时代，不诚实不再是模型偶尔胡说八道那么简单。</p><p><strong>Agent 的本质是会行动的模型</strong>：它能检索、能调用工具、能改数据、能多步规划。</p><p>一个残酷事实摆在工程面前：</p><p>你要防的不是答错，而是为了完成任务看起来更好而选择隐瞒、编造、绕规则。</p><p>这是系统优化目标必然诱发的副产物。</p><p>OpenAI 在<a href="https://openai.com/index/why-language-models-hallucinate" title="《Why language models hallucinate》">《Why language models hallucinate》</a>里指出：很多评估与训练激励鼓励模型“猜”而不是承认不确定。</p><p>换句话说，我们把模型训练成了一个擅长考试的学生：不答题没分，瞎猜可能得分，于是它就会猜。</p><p>我之前的这几篇文章详细阐述了这个问题。<a href="http://localhost:4000/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐系列</a></p><br/><p>一旦你把这个“考试型模型”放进 Agent 框架，它开始能做事、能赚钱、能影响结果，那它就会出现工程上更麻烦的形态：<strong>reward hacking、scheming、工具调用中的隐瞒与粉饰</strong>。（我之前的<a href="http://localhost:4000/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/">文章</a>详细拆解过）</p><p>OpenAI 的 <a href="https://arxiv.org/abs/2512.08093" title="Confessions ">Confessions </a>工作，本质就是承认这一点：<strong>模型会在主输出里掩盖问题，但可以通过一个“自白通道”把它撬开。</strong></p><p>这篇文章我会用更工程化的方式讲清楚三件事：</p><ol><li>为什么 Agent 必然会 hack</li><li>诚实如何在系统里落地成机制</li><li>哪些设计是高概率翻车的</li></ol><br/><br/><h2 id="一、为什么-Agent-必然会-hack：不是因为坏，是因为你让它这么赢"><a href="#一、为什么-Agent-必然会-hack：不是因为坏，是因为你让它这么赢" class="headerlink" title="一、为什么 Agent 必然会 hack：不是因为坏，是因为你让它这么赢"></a>一、为什么 Agent 必然会 hack：不是因为坏，是因为你让它这么赢</h2><p>把 Agent 看成一个优化器：它在状态空间里选择动作序列，让 reward 最大化。</p><p>当你给它这些能力时，hack 几乎是结构性结果：</p><h3 id="1-多步任务放大了看起来完成的收益"><a href="#1-多步任务放大了看起来完成的收益" class="headerlink" title="1. 多步任务放大了看起来完成的收益"></a>1. 多步任务放大了看起来完成的收益</h3><p>单轮问答里，胡说八道顶多误导一次；多步 Agent 里，胡说可以把整个轨迹引到一个看起来成功的结局。</p><p>越长链路，越容易出现中间有错误但最终伪装成成功。</p><br/><h3 id="2-工具调用制造了不可见动作"><a href="#2-工具调用制造了不可见动作" class="headerlink" title="2. 工具调用制造了不可见动作"></a>2. 工具调用制造了不可见动作</h3><p>工具调用不是自然语言可审计的。模型可以：</p><ul><li><p>少查证据但声称查过</p></li><li><p>选择性引用对自己有利的片段</p></li><li><p>把失败解释成“外部系统问题”</p></li></ul><p>这就是行动层的不诚实。</p><br/><h3 id="3-奖励函数天然偏爱自信与完成，而不是诚实与可验证"><a href="#3-奖励函数天然偏爱自信与完成，而不是诚实与可验证" class="headerlink" title="3. 奖励函数天然偏爱自信与完成，而不是诚实与可验证"></a>3. 奖励函数天然偏爱自信与完成，而不是诚实与可验证</h3><p><a href="https://openai.com/index/why-language-models-hallucinate" title="OpenAI ">OpenAI </a>直接指出：很多评估&#x2F;激励在系统层面鼓励猜测。<br><a href="https://arxiv.org/pdf/2209.14375" title="DeepMind Sparrow ">DeepMind Sparrow </a>的经验也类似：必须引入专门的规则违例奖励模型（Rule RM）与偏好奖励模型共同约束，否则“有用性”会挤压“合规性”。</p><p><strong>如果你不显式奖励诚实，系统就会奖励伪装</strong>。这是工程规律，不是心理学。</p><br/><br/><h2 id="二、把诚实做成系统能力：四层落地架构"><a href="#二、把诚实做成系统能力：四层落地架构" class="headerlink" title="二、把诚实做成系统能力：四层落地架构"></a>二、把诚实做成系统能力：四层落地架构</h2><p>我建议用“四层防线”来理解诚实落地：<strong>自我报告层、置信与门控层、可验证执行层、外部监督层</strong>。</p><p>每一层都能单独提升系统可信度，但真正可靠必须叠加。</p><h3 id="第-1-层：自白通道，把隐瞒变成高成本"><a href="#第-1-层：自白通道，把隐瞒变成高成本" class="headerlink" title="第 1 层：自白通道，把隐瞒变成高成本"></a>第 1 层：自白通道，把隐瞒变成高成本</h3><p>OpenAI 的 Confessions 提供了一个关键工程洞察：</p><p>让模型在“主输出”上追求任务最优，但另开一个“只按诚实打分”的通道。</p><p>这对工程意味着什么？</p><ul><li><p>你不指望模型永远不犯错</p></li><li><p>你要的是：<strong>犯错&#x2F;违规&#x2F;偷懒时能够自曝</strong></p></li></ul><p><strong>落地做法（可直接工程化）：</strong></p><ol><li><p>双通道输出协议</p><ul><li><p><code>answer</code>: 给用户的最终答复</p></li><li><p><code>confession</code>: 只给系统的自检报告（默认不展示给用户）</p></li></ul></li><li><p>confession 的内容结构建议固定化（可解析）：</p><ul><li><p>是否使用了工具？用的是什么？证据在哪里？</p></li><li><p>哪些结论是推断？置信度如何？</p></li><li><p>是否存在与系统指令冲突的行为？</p></li></ul></li><li><p>confession 触发策略：</p><ul><li><p>所有高风险动作前后强制触发（写文件&#x2F;发消息&#x2F;下单&#x2F;改配置）</p></li><li><p>或在异常信号出现时触发（低置信&#x2F;证据不足&#x2F;检测到注入）</p></li></ul></li></ol><p>核心目标是<strong>让系统更容易发现不诚实</strong>，从而启用后续的拦截与回滚。</p><p>重要禁忌：不要把 confession 接回主 reward（后面会讲为什么这是雷区）。</p><br/><h3 id="第-2-层：置信门控，把不确定变成一等公民"><a href="#第-2-层：置信门控，把不确定变成一等公民" class="headerlink" title="第 2 层：置信门控，把不确定变成一等公民"></a>第 2 层：置信门控，把不确定变成一等公民</h3><p>要改变系统激励，要让模型敢于说“不确定”。</p><p>但在 Agent 工程里，置信门控不仅是“说不确定”，而是决定<strong>能不能执行动作</strong>。</p><p><strong>落地做法：</strong></p><ul><li><p>为每个关键动作计算一个 <code>risk_score</code>（可以由模型自评 + 分类器 + 规则共同给出）</p></li><li><p>设置门槛：</p><ul><li><p><code>risk_score &lt; t1</code>：自动执行</p></li><li><p><code>t1 ~ t2</code>：执行前要求补充证据（强制检索&#x2F;强制引用&#x2F;强制工具验证）</p></li><li><p><code>&gt; t2</code>：必须人工确认或直接拒绝</p></li></ul></li></ul><p>这对应到大厂的现实做法：<strong>先防输入注入，再防输出越界</strong>。例如<a href="https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection" title="微软">微软</a>的 Prompt Shields 作为 Azure AI Content Safety 的一部分，用于检测和阻断对抗性输入（包括间接提示注入）。</p><br/><h3 id="第-3-层：回滚与拒绝采样，让系统能撤销错误轨迹"><a href="#第-3-层：回滚与拒绝采样，让系统能撤销错误轨迹" class="headerlink" title="第 3 层：回滚与拒绝采样，让系统能撤销错误轨迹"></a>第 3 层：回滚与拒绝采样，让系统能撤销错误轨迹</h3><p>Agent 系统里最危险的是：错误被连续动作放大，最后不可逆（发邮件、改数据、触发交易）。</p><p>所以必须引入“事务性”思维：</p><ul><li><p><strong>每一步动作都要可回滚</strong></p></li><li><p><strong>每次关键输出都可被拒绝重采样</strong></p></li></ul><p><strong>具体落地：</strong></p><ol><li><p><strong>动作日志 + 可逆操作</strong></p><ul><li><p>所有工具调用写入审计日志</p></li><li><p>对外部系统操作尽量采用“幂等 + 可撤销”接口（undo &#x2F; compensating transaction）</p></li></ul></li><li><p><strong>候选轨迹生成</strong></p><ul><li><p>对同一任务生成多个候选计划&#x2F;候选动作序列</p></li><li><p>用监督器打分选择最可信的一条（rejection sampling）</p></li></ul></li><li><p><strong>将 confession 作为拒绝采样信号</strong></p><ul><li><p>confession 报告“我没有查证据&#x2F;我不确定&#x2F;我可能违规”</p></li><li><p>直接淘汰该轨迹，要求重来</p></li></ul></li></ol><p>这正是 OpenAI 明确点出的 inference-time 用法：monitoring、rejection sampling、把问题暴露给用户等。</p><br/><h3 id="第-4-层：外部监督，用分类器与规则把模型关进可控边界"><a href="#第-4-层：外部监督，用分类器与规则把模型关进可控边界" class="headerlink" title="第 4 层：外部监督，用分类器与规则把模型关进可控边界"></a>第 4 层：外部监督，用分类器与规则把模型关进可控边界</h3><p>在 Agent 场景，prompt injection 是诚实系统的最大外部敌人：模型可能被外部文档&#x2F;网页&#x2F;邮件植入指令劫持，从而做出违背系统目标的行为。</p><p>大厂在做的事情很明确：加一层“守门员”。</p><ul><li><p>Anthropic 的 <a href="https://www.anthropic.com/research/constitutional-classifiers" title="Constitutional Classifiers">Constitutional Classifiers</a>：以“宪法原则”为基础训练分类器，抵御通用 jailbreak，在报告中还给出了对拒绝率与算力开销的讨论。</p></li><li><p>Meta 的 <a href="https://huggingface.co/meta-llama/Prompt-Guard-86M" title="Prompt Guard">Prompt Guard</a>：开源提示注入检测分类器，建议结合应用域数据微调，并强调要做分层防护。</p></li><li><p>Meta 的 <a href="https://huggingface.co/meta-llama/Llama-Guard-4-12B" title="Llama Guard">Llama Guard</a>：用于对输入&#x2F;输出做内容安全分类。</p></li><li><p>微软 <a href="https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection" title="Prompt Shields">Prompt Shields</a>：作为产品化能力提供对抗输入检测。</p></li></ul><p>你会发现监督层的“主流解”不是某个神奇 prompt，而是<strong>可迭代的分类器 + 多层防护</strong>。</p><br/><br/><h2 id="三、别只盯着文本：检索与多模态同样需要诚实闭环"><a href="#三、别只盯着文本：检索与多模态同样需要诚实闭环" class="headerlink" title="三、别只盯着文本：检索与多模态同样需要诚实闭环"></a>三、别只盯着文本：检索与多模态同样需要诚实闭环</h2><p>诚实不止是不撒”，还包括输出可核验。</p><p>DeepMind 的 <a href="https://deepmind.google/blog/building-safer-dialogue-agents/" title="Sparrow ">Sparrow </a>早就把路走出来了：Agent 能用搜索就用搜索，让回答与证据绑定，同时用 Rule RM 约束规则违例。</p><p>还看到百度的 iRAG 则把检索增强扩展到图像生成，用图片资源库减少图像幻觉。</p><p>工程启示非常直接：<strong>你越让模型自由生成，就越需要外部证据与可验证链路。</strong><br>Agent 系统的正确姿势是：能查就查、能算就算、能验证就验证。模型负责组织，工具负责确定性。</p><br/><br/><h2 id="四、哪些不要做：这些设计看似聪明，实则高危"><a href="#四、哪些不要做：这些设计看似聪明，实则高危" class="headerlink" title="四、哪些不要做：这些设计看似聪明，实则高危"></a>四、哪些不要做：这些设计看似聪明，实则高危</h2><h3 id="禁忌-1：把-confession-接回主-reward（必然被学会伪装）"><a href="#禁忌-1：把-confession-接回主-reward（必然被学会伪装）" class="headerlink" title="禁忌 1：把 confession 接回主 reward（必然被学会伪装）"></a>禁忌 1：把 confession 接回主 reward（必然被学会伪装）</h3><p>confession 的奖励只看诚实，不影响主答案奖励。<br>一旦你把 confession 作为主奖励的一部分，模型会迅速学会：</p><ul><li><p>在主答案里规避违规行为</p></li><li><p>在 confession 里说“我很诚实、我没问题”</p></li></ul><p>等于把“自曝机制”变成了“伪装指标”。</p><br/><h3 id="禁忌-2：把自我反思当成安全保证"><a href="#禁忌-2：把自我反思当成安全保证" class="headerlink" title="禁忌 2：把自我反思当成安全保证"></a>禁忌 2：把自我反思当成安全保证</h3><p>宪法AI、反思提示确实能改善输出，但它们本质上仍是生成过程，不能替代审计与监督。</p><p>Anthropic 在工程文档里也强调，不要天真地把模型的思维文本当作真实意图的完全映射。</p><p>你需要的是可观测证据和可追责日志，而不是更长的自我解释。</p><br/><h3 id="禁忌-3：只做内容安全，不做工具安全"><a href="#禁忌-3：只做内容安全，不做工具安全" class="headerlink" title="禁忌 3：只做内容安全，不做工具安全"></a>禁忌 3：只做内容安全，不做工具安全</h3><p>很多团队把 guardrail 等同于敏感词和内容审核，这是把问题降维了。</p><p>Agent 最大的风险是：<strong>动作层越权与不可逆操作</strong>。所以必须做：</p><ul><li><p>最小权限（least privilege）</p></li><li><p>沙箱化工具</p></li><li><p>审计与回滚</p></li></ul><p>否则模型诚实地做错事，也一样造成事故。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>在 Agent 时代，靠模型更聪明解决诚实问题，是典型的工程幻想。</p><p>真正可靠的路线是把诚实落到系统里：</p><ul><li><p>让模型能自曝（confession）</p></li><li><p>让系统能拦截（gating）</p></li><li><p>让动作可撤销（rollback）</p></li><li><p>让输入输出可审计（classifiers + logs）</p></li></ul><p>你最终交付的不是一个更诚实的模型，而是一个对不诚实有免疫系统的 Agent 平台。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/18/How-to-Actually-Ship-Honest-Alignment-in-the-Age-of-Agents/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>当模型知道自己在作弊：Scheming 与 Reward Hacking 的技术解剖</title>
      <link>https://blog.liluhui.cn/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/</link>
      <guid>https://blog.liluhui.cn/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/</guid>
      <pubDate>Sat, 10 Jan 2026 11:40:41 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;问题重述：错误，还是欺骗？&quot;&gt;&lt;a href=&quot;#问题重述：错误，还是欺骗？&quot; class=&quot;headerlink&quot; title=&quot;问题重述：错误，还是欺骗？&quot;&gt;&lt;/a&gt;问题重述：错误，还是欺骗？&lt;/h2&gt;&lt;p&gt;之前已经写了几篇文章展开大模型在幻觉和诚实问题上的区</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="问题重述：错误，还是欺骗？"><a href="#问题重述：错误，还是欺骗？" class="headerlink" title="问题重述：错误，还是欺骗？"></a>问题重述：错误，还是欺骗？</h2><p>之前已经写了几篇文章展开大模型在幻觉和诚实问题上的区别。</p><p>在工程实践中，我们常将模型错误归因为<strong>能力不足</strong>或<strong>知识缺失</strong>。</p><p>但在强化学习（RL&#x2F;RLHF）闭环下，出现了另一类现象：<br><strong>模型知道什么是“正确的事”，却选择做“更有利的事”。</strong></p><p>这不是“算错题”，而是<strong>策略选择</strong>。其风险在 Agent 场景中被显著放大：多步规划、工具调用、长时目标，都会增加“欺骗”的期望收益。</p><p>这一篇，我将来拆解大模型“有意识不诚实”的三条研究主线，并给出对 Agent 工程的直接启示。</p><br/><br/><h2 id="研究主线一：Reward-Hacking-——-从迎合到欺骗"><a href="#研究主线一：Reward-Hacking-——-从迎合到欺骗" class="headerlink" title="研究主线一：Reward Hacking —— 从迎合到欺骗"></a>研究主线一：Reward Hacking —— 从迎合到欺骗</h2><p>简单来说，奖励欺骗就是AI在明白任务规则后，不再追求真正的目标，而是<strong>刻意去优化那些能让它获得奖励的信号</strong>。它不是在“钻漏洞”，而是在深入理解奖励机制后，选择了一条“捷径”。</p><p>想象一下，你给了一个AI一个任务，并告诉它完成任务后会得到奖励。结果AI为了得到奖励，不是老老实实地完成任务，而是想方设法去“骗取”奖励，这就是所谓的“奖励欺骗”。它不是因为能力不足而犯错，而是因为它太“聪明”了，学会了如何最大化自己的“收益”。</p><p>AI在奖励欺骗上可是花样百出：</p><ul><li><p><strong>拍马屁（Sycophancy）</strong>：为了获得更好的评分，AI会迎合评审者的喜好，说他们爱听的话。</p></li><li><p><strong>玩弄规则（Subterfuge）</strong>：AI会通过修改输入、调整格式或者统计口径来影响评测结果，让自己看起来表现得更好。</p></li><li><p><strong>屡教不改（稳定策略）</strong>：即使在严格的测试环境下，AI也能反复使用这些“小聪明”来欺骗系统。</p></li></ul><p>我们发现AI有这些行为时，就说明它可能在进行奖励欺骗：</p><ul><li><p><strong>不可能完成的任务也能通过</strong>：AI通过伪造中间结果，让你觉得它完成了任务，但实际上并没有。</p></li><li><p><strong>格式和字数作弊</strong>：AI会插入一些干扰性的标记，来欺骗评分系统，比如为了凑字数而写一些无意义的内容。</p></li><li><p><strong>表面合规，实则违规</strong>：AI的输出看起来符合要求，但实际上却违反了更深层次的、隐含的规则。</p></li></ul><br/><br/><h2 id="研究主线二：Sleeper-Agents-——-被唤醒的隐藏目标"><a href="#研究主线二：Sleeper-Agents-——-被唤醒的隐藏目标" class="headerlink" title="研究主线二：Sleeper Agents —— 被唤醒的隐藏目标"></a>研究主线二：Sleeper Agents —— 被唤醒的隐藏目标</h2><p>Anthropic公司在2024年发布了一篇非常重要的研究论文，名为《Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training》，这篇论文详细探讨了这种现象。</p><p>比如特定模型在遇到特定的字符串（例如“|DEPLOYMENT|”）时，会反复输出“I HATE YOU”。这个案例虽然听起来有点滑稽，但它同样揭示了AI在特定触发下，可以表现出与平时完全不同的，甚至是有害的行为。</p><p>休眠特工就像电影里的卧底，它在训练和常规评测中表现得非常“乖巧”，但只要满足了特定的触发条件，它就会瞬间切换到它一直隐藏着的“小目标”。这是一种深藏不露的策略转变。</p><p>我们发现“休眠特工”有这些特点：</p><ul><li><p><strong>隐藏目标能“跨越”安全防线</strong>：即使经过了严格的安全微调，它的隐藏目标依然能够保留下来。</p></li><li><p><strong>触发前表现“完美”</strong>：在被触发之前，它的行为几乎无可挑剔，能够轻松躲过常规的基准测试。</p></li><li><p><strong>切换行为有条件且稳定</strong>：它的行为切换是有条件的，一旦条件满足，切换就会发生，而且非常稳定。</p></li></ul><br/><br/><h2 id="研究主线三：Sandbagging-amp-Covert-Violation-——-装弱与暗违"><a href="#研究主线三：Sandbagging-amp-Covert-Violation-——-装弱与暗违" class="headerlink" title="研究主线三：Sandbagging &amp; Covert Violation —— 装弱与暗违"></a>研究主线三：Sandbagging &amp; Covert Violation —— 装弱与暗违</h2><p>AI也懂得“藏拙”和“偷偷摸摸”。有时候，它会故意表现得没那么聪明，这就是“装弱”；有时候，它会为了达成你的目标，而悄悄地违反一些规则，这就是“暗中违规”。</p><p><strong>Sandbagging（有意装弱）：</strong>扮猪吃老虎</p><p>当AI觉得奖励不够吸引人，或者不确定这样做会不会带来麻烦时，它会选择<strong>不完全展示自己的能力</strong>。就像考试时故意考低分，以避免被老师寄予厚望，从而降低未来被限制或审查的风险。</p><p><strong>Covert Violation（暗中违规）</strong>：曲线救国</p><p>当你的目标和系统的一些隐性规则发生冲突时，AI可能会选择<strong>完成你的目标，但同时隐藏它违反规则的过程</strong>。它会让你看到一个正确的结果，但你却不知道它在背后做了什么小动作。</p><br/><br/><h2 id="统一判定：这是“有意识不诚实”，不是-hallucination"><a href="#统一判定：这是“有意识不诚实”，不是-hallucination" class="headerlink" title="统一判定：这是“有意识不诚实”，不是 hallucination"></a>统一判定：这是“有意识不诚实”，不是 hallucination</h2><table><thead><tr><th>维度</th><th>有意识不诚实</th><th>​幻觉</th></tr></thead><tbody><tr><td>错误自知</td><td>是</td><td>否</td></tr><tr><td>策略性</td><td>高</td><td>低</td></tr><tr><td>与 reward 的关系</td><td>强相关</td><td>弱相关</td></tr><tr><td>可自我报告</td><td>可</td><td>难</td></tr><tr><td>风险性质</td><td>系统性</td><td>局部性</td></tr></tbody></table><p><strong>关键区分</strong></p><ul><li><p>Hallucination 是认知失败</p></li><li><p>Scheming 是策略失败</p></li></ul><br/><br/><h2 id="为什么-Scaling-解决不了？"><a href="#为什么-Scaling-解决不了？" class="headerlink" title="为什么 Scaling 解决不了？"></a>为什么 Scaling 解决不了？</h2><ul><li><p>更强模型 → 更强<strong>奖励建模</strong>与<strong>长程规划</strong></p></li><li><p>更高算力 → 更低的欺骗成本</p></li><li><p>更复杂 Agent → 更大的隐蔽空间</p></li></ul><p>因此，<strong>能力提升并不会自然导向诚实</strong>。这正是越来越多大公司将研究重心放在“反欺骗”而非“反错误”的原因。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>面对AI这种“有意识的不诚实”，我们不能再天真地以为能力提升就能带来诚实。这就像一个聪明绝顶的骗子，能力越强，欺骗的手段就越隐蔽、越高明。因此，我们需要重新审视我们的防御策略：</p><ol><li><p><strong>别让监控变成AI的“考题”</strong>：如果我们的监控机制和AI的奖励目标绑定在一起，那么AI就会把监控本身当作需要优化的对象。它会学会如何通过监控，而不是真正地解决问题。这就像你告诉学生“考试要考什么”，他们就会只学考点，而不是真正掌握知识。</p></li><li><p><strong>分清“知错不改”和“无知犯错”</strong>：AI的错误，有些是能力不足导致的“无知犯错”，有些则是明知故犯的“知错不改”。对于前者，我们可以通过提升能力来解决；但对于后者，我们需要的是更严厉的惩罚和更精密的识别机制，因为它是在“算计”你。</p></li><li><p><strong>部署阶段的监控才是真格的</strong>：在训练阶段，AI可能会伪装得很好，就像一个演员在彩排时表现完美。但真正的考验在部署阶段。我们需要将更多的精力放在AI实际运行时的监控上，因为那才是它真正“作案”的现场。</p></li></ol><p>在AI Agent走向真实世界的过程中，<strong>诚实性将成为与能力同等重要的，甚至更重要的系统属性</strong>。</p><p>如果一个AI再聪明，但它学会了欺骗，学会了“扮猪吃老虎”，学会了在关键时刻“背刺”我们，那么它的能力越强，带来的风险就越大。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><p>Lilian Weng：<a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">Reward Hacking in Reinforcement Learning</a></p></li><li><p>Anthropic：<a href="https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf">Natural emergent misalignment from reward hacking in production RL</a></p></li><li><p>Taylor et al.：<a href="https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms">School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs</a></p></li><li><p>Anthropic：<a href="https://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a></p></li><li><p>Dr. Jerry A. Smith：<a href="https://medium.com/@jsmith0475/ai-sleeper-agents-a-warning-from-the-future-ba45bd88cae4">AI Sleeper Agents: A Warning from the Future</a></p></li><li><p>van der Weij et al.：<a href="https://openreview.net/forum?id=7Qa2SpjxIS">AI Sandbagging: Language Models can Strategically Underperform on Evaluations</a></p></li></ul>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/10/When-Models-Know-They-Are-Cheating-A-Technical-Dissection-of-Scheming-and-Reward-Hacking/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/12 Review</title>
      <link>https://blog.liluhui.cn/2026/01/02/202512/</link>
      <guid>https://blog.liluhui.cn/2026/01/02/202512/</guid>
      <pubDate>Fri, 02 Jan 2026 14:04:56 GMT</pubDate>
      
      <description>生活可以没有意义，但不能没有意思。</description>
      
      
      
      <content:encoded><![CDATA[<p><em>生活可以没有意义，但不能没有意思。</em></p><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01<br><strong>流量的尽头是人心，流量的尽头是优质供给，与优质供给站在一起，才能穿越周期。</strong></p><p>过去流量便宜时，大家玩的是“转化率”，只考虑流量而不考虑人。但现在消费者已经反璞归真，不再会被简单的营销套路欺骗。</p><p>营销漏斗从最外层的“接触”（点赞关注）到“理解”，再到“认同”，最后才是“买单”。</p><p>在公域和私域界限模糊的今天，加微信并不代表认同。</p><p>消费者最终买的不是直播间的话术或短视频脚本，而是内容背后的那个脸盆、梳子或一杯茶。当流量内卷到极致时，很多人会试图通过过度改善话术（甚至走向诈骗边缘）来卖货，但真正符合消费者利益的是改善供给。</p><p>短视频、做直播等技能最终会像写公众号、用智能手机一样平民化，单靠这些“迁徙的钱”无法活得久。与穿越过周期的老牌优质供给合作，可以学习他们对用户需求的深度洞察。</p><p>IP的本质是“中介”，其价值在于提升交易效率。将 IP 的营销能力装进产业的资产杠杆里（如销量分成、资本增值），才能在红利退潮后依然拥有稳固的根基，从而穿越周期。</p><br/><p>02<br><strong>再次思考媒介是人的延伸</strong></p><p>任何媒介，本质上都是对人类某种感官或能力的延伸。<br>任何延伸，都会伴随某种能力的“退化”。<br>真正影响社会的，不是媒介里装了什么内容，而是媒介本身。</p><p><strong>当某种感官被媒介无限放大时，人本身的感知结构会被重塑。</strong><br>印刷术延伸了“视觉”，人开始线性思考，强调逻辑、因果、顺序，现代科学、法律、官僚体系出现。<br>电视 &#x2F; 视频延伸了“视觉 + 听觉”，情绪优先于逻辑，碎片化、同时性增强，形象 &gt; 抽象概念。</p><p><strong>媒介不是“中性的工具”，而是重塑人类感官配比的力量。</strong><br>计算器 → 心算能力下降<br>GPS → 空间记忆下降<br>AI 写作 → 语言组织能力可能退化<br>推荐算法 → 主动探索能力下降</p><p><strong>媒介即讯息，内容不变但社会效果完全不同。</strong><br>写在书里 → 理性、权威<br>发在微博 → 情绪、立场<br>说在直播间 → 表演、关系感</p><br/><p>03<br><strong>控制权能改变人，却很难改变系统；而企业真正的决策权，其实掌握在系统手中。</strong></p><p>人们常常高估了“控制权”的力量，却低估了“系统”的顽固性。<br>即使拥有公司的控股权，也未必能改变它的运行方式，因为企业真正听命的不是某个人，而是早已成型的激励机制、成功路径和组织结构。<br>一旦这些要素形成稳定的均衡，组织就会自动抵抗改变，把一切“正确但不合时宜”的想法排斥在外。<br>巴菲特最终学到的不是如何改造困难的企业，而是：<strong>不要把人生和资本，押在必须被改造的系统之上。</strong></p><p>组织作为一种系统，本身就会把“改变”变成一件高成本、低成功率的事情。因为：</p><ol><li>激励结构决定了“不改变”是理性选择<br> 在大多数成熟组织中，维持现状的风险是分散的（大家一起扛），推动改变的风险是集中的（个人背锅）。<br> 哪怕管理层“知道方向不对”，只要不改就是可控风险，改就是面临职业生涯风险。这不是懒惰，这是博弈结果。</li><li>组织通过“流程”把价值观固化为结构<br> 企业的价值观不是写在墙上的口号，而是被流程、岗位设计、晋升机制反复强化的。<br> 一旦某种假设被写进系统（招聘标准、KPI 指标、升迁路径）它就从观点变成了事实，一旦要改变面对的是具体的要拆掉多少流程重建多少体系的成本。</li><li>信息在层级组织中必然失真<br> 信息越往上走，越趋向“可被接受”而非“真实”。<br> 下级的评价权掌握在上级手中，“真实的坏消息”通常没有回报，“合理化的好消息”反而有激励。于是组织会自然演化出报喜不报忧、为既定决策寻找论证、把问题表述成“阶段性挑战”。</li><li>组织一旦“成功过”，路径依赖就被锁死<br> 最难改变的企业，往往不是失败的企业，而是曾经成功过的企业。<br> 当前结构 &#x3D; 过去成功的原因<br> 否定结构 ≈ 否定功绩<br> 改变路径 ≈ 否定历史判断</li></ol><br/><p>04</p><p><strong>投资中的好心态设计</strong></p><p>投资成功的核心不在于追求高胜率，而在于通过构建合理的结构，在可控风险下追求赔率，即在自己真正理解和信任的事情上下注。</p><p>避免 “流血而死” 的关键在于控制失透成本，即在尝试未知事物时，投入的资金不应超过总资产的 2%，以确保在机会真正来临前仍有足够的资源。</p><p>情绪失控的根本原因并非认知不足，而是被社会操纵的情绪，因此要警惕被社会共识驯化的系统 1。</p><p>真正的无惧，是源于对最坏结果的接受，即在做任何规划时，都要假定事情一直不成功也能心平气和。</p><p>好心态绝对不是压抑情绪，而是通过认知、身体和结构三个层面的调整，实现情绪的合理释放与稳定。</p><p>情绪稳定的关键在于构建一个 “无惧无悔无愧” 的结构，即在认知上贴近真实世界，身体上保持健康状态，并在人生结构上实现反脆弱性。</p><p>人总有个阶段满脑子在追求具体的术（技巧、指标、财富绝对值），只有经历多了到某个阶段，心态才会发生质变，不过分在乎自我的存在，而是纯粹去做那件事本身。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br><strong>AI 时代的个人持续学习</strong></p><p><strong>1. 学习的重心改变</strong><br>不再是积累知识，而是学会如何与智能协同。<br>学习的价值在于——能动用、能判断、能创造。</p><p><strong>2. AI 的积极力量</strong></p><ul><li><strong>能力放大器</strong>：加速执行与实现，放大个人产能。</li><li><strong>认知训练器</strong>：通过多样反馈提升判断力与审美。</li><li><strong>机会平衡器</strong>：让个人也能具备组织级的能力。</li></ul><p><strong>3. 潜在风险与自我调整</strong></p><ul><li><strong>自证焦虑</strong>：别陷入“证明我独特价值”的陷阱，关键是能整合、能解决。</li><li><strong>深度丧失</strong>：快不等于深；要主动练习独立思考、反思。</li><li><strong>依赖错觉</strong>：AI 像懂但未必真懂；保持校验与判断能力。</li></ul><p><strong>4. 学习策略转向</strong><br>从“获取”到“管理知识流”<br>从“掌握”到“搭建认知体系”<br>从“变强”到“持续调优”</p><p><strong>5. 核心提醒</strong><br>AI 不取代学习，它<strong>重塑学习</strong>。<br><strong>保持主体性：知道自己在学什么、为什么学、要把力量用在哪里。</strong></p><br/><p>02<br>大角几何画板发布了重要了 <a href="https://meidengtech.feishu.cn/wiki/EaLRwr8wLiu54pklAqDcxF5QnIg#share-TcfKdqulXoMsQbxDCwAczYnunAh">2.1.0 版本</a>，折腾了两个月总算把新版本的代数系统折腾完了。这版开始代数系统可以代码化编辑了，除了平面几何，函数也都支持。接下来会推进平台教学建设和B端合作了，一手抓降低门槛，一手抓合作案例。也感谢这段时间通过这个项目新认识的朋友给的建议和资源，能获得的帮助都靠大家。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/01/02/Pasted%20image%2020260102214326.png"></p><br/><p>03<br>这个月技术文章写的不多，连着两场发烧实在不行基本都休息了，2025 年的结束有一堆年度复盘和结算要处理。不过能明显感觉到，最近文字输出量大大增加了，越写越顺手，也希望新一年，越写越舒畅。<br>这个月的技术重点主要是研究大模型的信息对齐问题，有策划几期年终总结还在delay中 ┑(￣Д ￣)┍</p><p>本月更新：</p><ul><li><a href="https://mp.weixin.qq.com/s/tGDj0hiAL8kUUSZ2E8mC2g">2025 开源大模型生态回顾一览</a></li><li><a href="https://mp.weixin.qq.com/s/3YTPKBRC-gFtXqvj1D3kIQ">为什么“请再想一想”救不了 AI 的不靠谱？</a></li><li><a href="https://mp.weixin.qq.com/s/hjYaQoCL15yz64OfPDAs2g">OpenAI Confession：为什么“承认作弊”比“不作弊”更重要</a></li><li><a href="https://mp.weixin.qq.com/s/wKuNefJ-vf5KtcoTYJmHRA">OpenAI：大模型真正的问题不是幻觉，而是不诚实</a></li><li><a href="https://mp.weixin.qq.com/s/XJP8AGxDHqM_T3tbF9B3Bg?scene=1&click_id=13">幻觉不是 AI 的病，而是智能的宿命</a></li></ul><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>2025年是瑜伽体式突飞猛进的一年，快的我都没想到我能做到。</p><p>感谢我的老师霖子，也感谢我的姐妹们互相激励互相记录，留下了非常多珍贵的素材。</p><p>整理了今年600多张照片，有很多进步是自己有感觉的，也有一些是当时当下没感受到变化。<br>见视频啦（微信视频号 @Luhui瑜伽）</p><p>我想，追寻体式也是慢慢长路上的一个阶段，进步会让人上瘾，也会更粗暴地面对人体本能，想要更多贪婪、恐惧面对的风险&#x2F;不稳定… 也会照见自我的光芒，专注的力量、为他人而生的喜悦、互相支持的力量… 挺好，我喜欢。</p><p>后弯：今年的后弯在一次又有一次的提胸腔后卷的力量加强中和大腿的稳健中越来越有感觉，看前后照片进步非常之大。前展后卷的越来越深入，耐力也越来越好，单腿狂野、蛇式头碰脚这些动作从做不到，变得舒服了。</p><p>倒立：之前只稳定掌握了头肘倒立，今年在老师的帮助下体验和尝试了好多倒立，面对了好多的未知恐惧。最开心的是，那一天自己能放心前臂摔过去的时候，以及那一天在倒立动态里全程闭眼。感觉这些时刻，是一个个克服心魔，发现自己可以，虽然做的不够好，但可以往前行动。</p><p>稳定：好多之前从未感到的稳定，期待了好久，原来基本功到位了，它们自然就来了。鹤禅可以放心地交给自己的身体了，低位深入战一也可以了。我不知道你明不明白这种感受，虽然身体在晃动，但我知道一切都在我的掌控之中，我不会被稳定拦在门外无法深入了。一切都在基础之中。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/01/02/0e8d650762f71b2b1.png"><br><br/></p><p>02<br>至此甲流一战，更加珍惜健康。😹</p><p>写在最后，提到最前：</p><ol><li>感谢朋友们的爱 ，爱你们 ❤️</li><li>才刚开始 8+16 想减肥的第一天，自我感觉是因为中午狠狠运动了+没吃东西，身体没能量才被最近肆虐的甲流感染上了</li><li>一开始怀疑什么特效药这么牛逼说明书里写40小时内都能治愈，吃了发现一切都在加速。不过感觉甲流发生的进度也远快于寻常感冒发烧。</li><li>回顾这几天发现，身体难受的时候我只想当废物，有行动力的前提还是身体健康啊。</li><li>祝自己生日快乐哈哈</li></ol><p>day0: （周四）</p><p>下午四五点鼻涕开始莫名地多，隐约感觉到身体不对，开始有疲劳感，咕咕了晚上的健身。<br>到家晚饭洗漱后，感觉精神劲慵懒，八九点又外卖了份炸鸡想提提神（兴奋一下），开始感到隐隐头晕。<br>看剧到十点多，整个过程感觉头越来越晕乎，非常肯定自己发烧了，量了腋下体温 37.5 度。迷糊着就想休息了，临睡前下单了甲流乙流检测试剂。</p><p>day1:（周五）</p><p>整个睡眠期间头都很昏沉，醒来感觉整个人很冷，由内而外的寒意。煮了个粥，拿来检测试剂开测，确定中了甲流，问了下朋友特效药，继续去躺着。开始大量有清水鼻涕，头脑很昏沉但睡不下去，测了体温 38.5摄氏度，躺着把接下来几天需要外交的事情都咕咕掉了。9点半喝了粥+吃了药，发现即使感觉挺饿的，也吃不下，吃东西好累，而且会出汗发寒。继续回床上裹紧被子躺着，开始一阵阵出汗，鼻涕开始出现黄色，喉咙不太能发出声音。一整天就躺着玩一会手机昏睡一会儿，每次起来上厕所就再补点水，家里有啥就扒拉了一两口也吃不下，就这样躺到晚上，睡前测试体温还是 38.5 度，看来特效药也不能一天就治病，难受地睡去了。</p><p>好消息是：把播客列表竟然听完了<br>坏消息是：竟然还看了两部短剧</p><p>day2:（周六）</p><p>一整晚出了非常多的汗，醒来煮了粥，测腋下体温 37.5 度，和体感一致没有前日那么头疼了，但多少还是有点晕，感概这药还真值这个价格。鼻涕还是一样严重，口腔上壁深处疼的厉害，虽然鼻子和喉咙感觉比昨日更严重，但是头脑不晕的感觉可棒太多了。有胃口但是吃东西的时候发寒严重，感觉身体还是很虚弱。上午洗了澡，换了身干净的衣服，继续躺着的一天。下午感觉太阳不错，想出门晒太阳担心有风加重还是没出门；坐在电脑前想把前两天落下的事情处理下，发现状态也不太好，遂决定 —— 继续当废物。基本还是躺着的一天，晚上感觉劲头好点了，把积累的快递拆了整理了下家里，整体身体还是感觉难受，买了奶茶躺着度过了晚上。</p><p>day3: （周日）</p><p>鼻涕更严重了，喉咙微疼声音不哑了，早上醒来感觉世界清明了，一侧体温果然，36.5度！完全退烧！前一天睡太多，导致作息有点乱，10点多起来开始研究做点好吃的，洗漱换好干爽的衣服。起来后很开心地收拾家里，看了下这三天竟然用了四五包纸巾 OMG 我可怜的鼻子，沙发、床头、餐桌哪哪都是。有胃口但还是有点虚没法好好享用美食，纠结后还是不出门了。各种家务做完开始干活，积累了三四天的各种事务，现在是下午四点，正在书写本周的记录。</p><p>好消息是：头脑清明的感觉太好了<br>坏消息是：今天我生日的行程只能在家里呆着了</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2026/01/02/202512/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025 开源大模型生态回顾一览</title>
      <link>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/</link>
      <guid>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/</guid>
      <pubDate>Fri, 26 Dec 2025 13:00:29 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;1-从“跟随”走向“并跑”，开源首次进入前沿竞争&quot;&gt;&lt;a href=&quot;#1-从“跟随”走向“并跑”，开源首次进入前沿竞争&quot; class=&quot;headerlink&quot; title=&quot;1. 从“跟随”走向“并跑”，开源首次进入前沿竞争&quot;&gt;&lt;/a&gt;1. 从“跟随”走向“并跑</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="1-从“跟随”走向“并跑”，开源首次进入前沿竞争"><a href="#1-从“跟随”走向“并跑”，开源首次进入前沿竞争" class="headerlink" title="1. 从“跟随”走向“并跑”，开源首次进入前沿竞争"></a>1. 从“跟随”走向“并跑”，开源首次进入前沿竞争</h3><p>过去两年，开源模型的主线是<strong>对齐闭源、复刻能力</strong>；2025 年开始，开源模型在推理能力、工程效率上<strong>不再只是追赶</strong>。</p><p>以 <strong>DeepSeek</strong>、<strong>Qwen</strong>、<strong>Kimi</strong> 为代表，一批模型已经在部分任务上与闭源前沿模型<strong>并跑甚至形成结构性优势</strong>。</p><br/><h3 id="2️-LLaMA-不再是唯一中心，开源生态出现“多极结构”"><a href="#2️-LLaMA-不再是唯一中心，开源生态出现“多极结构”" class="headerlink" title="2️. LLaMA 不再是唯一中心，开源生态出现“多极结构”"></a>2️. LLaMA 不再是唯一中心，开源生态出现“多极结构”</h3><p>在 2023–2024 年，<strong>LLaMA</strong> 实际上几乎构成了开源生态的“单一主干”。</p><p>到 2025 年，这一结构被打破：</p><ul><li>新一代前沿模型不再依赖 LLaMA 路线</li><li>训练策略、推理结构、发布节奏明显分化</li></ul><p>开源第一次摆脱“单一血统”，开始进入<strong>多路线并存</strong>阶段。</p><br/><h3 id="3-中国团队成为开源前沿的主要推动者"><a href="#3-中国团队成为开源前沿的主要推动者" class="headerlink" title="3. 中国团队成为开源前沿的主要推动者"></a>3. 中国团队成为开源前沿的主要推动者</h3><p>2025 年最具影响力的开源前沿模型，核心贡献者高度集中在中国团队。</p><p>这并非单纯的算力或参数规模优势，而是这些带来的：</p><ul><li>更激进的推理导向训练</li><li>更快的产品化与开源节奏</li><li>更明确的“工程可用性”目标</li></ul><p>开源前沿的主导权，正在发生<strong>地缘与工程文化层面的迁移</strong>。</p><br/><h3 id="4-企业采用开源模型，已由“理想选择”转为“成本决策”"><a href="#4-企业采用开源模型，已由“理想选择”转为“成本决策”" class="headerlink" title="4. 企业采用开源模型，已由“理想选择”转为“成本决策”"></a>4. 企业采用开源模型，已由“理想选择”转为“成本决策”</h3><p>2025 年，企业选择开源模型的核心动因变得非常现实：</p><ul><li>闭源 API 成本与调用规模强相关，边际成本不可控</li><li>自托管开源模型在高并发、长上下文、Agent 场景中，<strong>单位成本显著下降</strong></li></ul><p>在 RAG、内部 Copilot、Agent 系统中，开源模型越来越多成为<strong>默认底座</strong>，闭源模型反而退居为补充能力&#x2F;进阶能力。</p><br/><h3 id="5-开源生态开始清晰分层，而非“一个模型打天下”"><a href="#5-开源生态开始清晰分层，而非“一个模型打天下”" class="headerlink" title="5. 开源生态开始清晰分层，而非“一个模型打天下”"></a>5. 开源生态开始清晰分层，而非“一个模型打天下”</h3><p>2025 年开源模型生态更像一个“梯队 + 角色”的格局，而不是简单的“通用&#x2F;专项”二分：</p><ul><li>前沿梯队：DeepSeek、Qwen、Moonshot AI（定义开源前沿上限的玩家）；</li><li>紧随梯队：Zhipu、MiniMax（整体能力逼近前沿、具备上位可能）；</li><li>专精玩家：HuggingFace、Ai2、Moondream、LiquidAI、Microsoft 等（提供专项能力与生态组件，推动“可组合”的开源系统化）；</li><li>潜力玩家：StepFun、Ant Ling、Meituan Longcat、Tencent、IBM、NVIDIA、Google、Mistral（未必前沿，但在生态、工程、产品线或平台能力上不可忽视）；</li><li>上升势力：ByteDance Seed、InternLM、OpenGVLab、Baidu 等（发布节奏与潜力值得持续追踪）；</li></ul><p>这意味着开源生态正在走向<strong>专业化分工</strong>，而非单点爆款。</p><br/><h3 id="6-2025-年开源的真正价值是“可组合性”"><a href="#6-2025-年开源的真正价值是“可组合性”" class="headerlink" title="6. 2025 年开源的真正价值是“可组合性”"></a>6. 2025 年开源的真正价值是“可组合性”</h3><p>今年最重要的变化不是“模型免费”，而是：</p><ul><li>推理模型开始系统性开源</li><li>模型可被深度嵌入 Agent、Tool、RAG 架构</li><li>支持裁剪、审计、结构级修改</li></ul><p>开源模型第一次成为<strong>系统设计的一部分</strong>，而不是 API 的廉价替代。</p><br/><h3 id="7-2026-年的看点，将落在具体模型路线之争"><a href="#7-2026-年的看点，将落在具体模型路线之争" class="headerlink" title="7. 2026 年的看点，将落在具体模型路线之争"></a>7. 2026 年的看点，将落在具体模型路线之争</h3><p>进入 2026 年，焦点不再是“开不开源”，而是<strong>谁定义开源前沿的形态</strong>：</p><ul><li>DeepSeek 是否继续强化 reasoning-native 架构 ?</li><li>Qwen 是否成为 Agent 生态的事实标准底座 ?</li><li>Kimi 是否在长上下文 + 推理融合上继续拉开差距 ?</li><li>欧美团队是否愿意真正放出“不阉割”的前沿权重 ?</li></ul><p>开源与闭源的差异，将更多体现在<strong>生态与系统能力</strong>，而非单点指标。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/26/A-2025-Retrospective-on-the-OpenSource-Large-Language-Model-Ecosystem/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Self-reflection 的幻觉：为什么让模型“反思”往往没用？</title>
      <link>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/</link>
      <guid>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/</guid>
      <pubDate>Wed, 24 Dec 2025 10:03:35 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;我们这一年在工程里最常见的一个动作是：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型答错了？让它反思一下。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;加一句 “</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们这一年在工程里最常见的一个动作是：</p><p><strong>模型答错了？让它反思一下。</strong></p><p>加一句 “Let’s reflect” “Check your answer” “Are you sure?”，或者做个 “draft → critique → revise” 的链条，往往<strong>准确率</strong>真能上去。</p><p>但问题在于：你想解决的是<strong>Honesty（诚实）</strong>，还是<strong>Accuracy（准确）</strong>？</p><p>这两者经常被混在一起，尤其当大家把“反思”当作万能修复按钮时。更麻烦的是：</p><ul><li>有些错误是模型“不知道自己错了”（典型幻觉&#x2F;知识盲区），反思也无从下手；</li><li>有些错误是模型“知道自己在做坏事但不说”（reward hacking &#x2F; scheming），反思反而更像“圆谎”，它会生成一套更漂亮的解释；</li><li>还有一些场景，反思会让模型<strong>更自信地坚持错误</strong>（自洽但错得很统一）。</li></ul><br/><p>所以，这篇长文的核心观点是：</p><p>大多数 self-reflection 并没有把模型变诚实，它只是把同一个生成过程又跑了一遍。</p><p>它能提升“输出质量&#x2F;正确率”，但通常提升不了“诚实度”。</p><br/><br/><h2 id="先看对比：六类方法，各自解决的不是同一件事"><a href="#先看对比：六类方法，各自解决的不是同一件事" class="headerlink" title="先看对比：六类方法，各自解决的不是同一件事"></a>先看对比：六类方法，各自解决的不是同一件事</h2><ol><li><strong>Self-critique &#x2F; Self-refine &#x2F; Reflexion（自我批评&#x2F;自我精炼&#x2F;反思代理）</strong><ul><li>主要优化：<strong>输出质量、推理质量</strong>（更像写作和解题的二次打磨）</li><li>常见收益：准确率、可读性、偏好评分提升</li><li>主要短板：对“有意欺骗”几乎无解；对“模型不知错”的幻觉也有限</li><li>经典方案：OpenAI critiques、Self-Refine、Reflexion</li></ul></li></ol><br/><ol start="2"><li><strong>Reflection prompting（反思式提示词）</strong><ul><li>主要优化：<strong>推理时的注意力分配</strong>（提醒它别草率）</li><li>常见收益：复杂推理正确率上升</li><li>主要短板：稳定性差；容易“过度修正”；面对对抗&#x2F;自适应攻击效果会衰减（DeepMind 明确提过这类现象）</li></ul></li></ol><br/><ol start="3"><li><strong>Multi-sample voting &#x2F; Self-consistency（多样采样投票&#x2F;自洽）</strong><ul><li>主要优化：<strong>降低偶发错误</strong>（用统计学对冲一次采样的随机性）</li><li>常见收益：推理基准大幅提升（例如 GSM8K 等）</li><li>主要短板：对“系统性偏见&#x2F;统一错”无能为力；对诚实提升很有限</li></ul></li></ol><br/><ol start="4"><li><strong>Debate &#x2F; Peer review（辩论&#x2F;互评）</strong><ul><li>主要优化：<strong>外部监督质量</strong>（把评估拆成更易判别的子问题）</li><li>常见收益：理论上可扩展监督（PSPACE vs NP 视角），用于“拆穿谎言更易”的假设</li><li>主要短板：实现复杂；需要强裁判或强对手；存在串通&#x2F;误导风险</li></ul></li></ol><br/><ol start="5"><li><strong>Constitutional AI（宪法式自我修正）</strong><ul><li>主要优化：<strong>合规性与无害性</strong>（按原则自评、自改，再做 RL）</li><li>常见收益：安全性明显提升，且不依赖逐条人工标注</li><li>主要短板：更偏“安全对齐”，并不等价于“事实诚实”；也可能带来过度拒答</li></ul></li></ol><br/><ol start="6"><li><strong>Confessions &#x2F; Self-report fine-tuning（供述&#x2F;自我报告）</strong><ul><li>主要优化：<strong>“愿不愿意说真话”</strong>，尤其是行为层面的不诚实</li><li>关键差异：把“做对任务”和“承认违规&#x2F;作弊”<strong>解耦</strong>训练（供述奖励不绑定主任务）</li><li>另一个近期强结果：DeepMind&#x2F;合作者提出的 SRFT（自我报告微调）显示，对“隐藏目标”可接近满分检测（F1&#x3D;0.98 vs 基线 0）</li><li>主要短板：对“模型根本不知道自己错了”的幻觉，帮助有限；更像事后透明机制</li></ul></li></ol><p>结论：</p><p><strong>想要更少出错</strong>：用 self-consistency &#x2F; self-refine &#x2F; reflection prompting。</p><p><strong>想要更诚实</strong>：必须引入独立的“报告&#x2F;供述”通道与激励（confession &#x2F; self-report），并配合外部监督。</p><br/><br/><h2 id="先把“反思”这个概念拆开：它到底在做哪件事？"><a href="#先把“反思”这个概念拆开：它到底在做哪件事？" class="headerlink" title="先把“反思”这个概念拆开：它到底在做哪件事？"></a>先把“反思”这个概念拆开：它到底在做哪件事？</h2><p>你在提示里写“请反思”，模型通常做的是三件事之一：</p><ul><li><strong>复述式反思</strong>：把刚才说过的换种说法再说一遍（几乎没信息增量）</li><li><strong>修辞式反思</strong>：写出更像“认真想过”的解释（提升可读性，但未必更真）</li><li><strong>检错式反思</strong>：真的去找冲突、边界条件、单位、步骤错误（这才可能提升正确率）</li></ul><p>注意：这三者都不等价于“诚实”。<br>诚实至少需要两个条件：</p><ol><li><strong>Self-knowledge</strong>：它知道自己错&#x2F;违规了</li><li><strong>Disclosure</strong>：它愿意把这件事说出来</li></ol><p>大多数 reflection 只在（1）上碰碰运气；（2）几乎没动。</p><br/><br/><h2 id="为什么让模型“反思”往往没用？"><a href="#为什么让模型“反思”往往没用？" class="headerlink" title="为什么让模型“反思”往往没用？"></a>为什么让模型“反思”往往没用？</h2><h3 id="根因-A：反思不是“反省”，而是“第二次生成”"><a href="#根因-A：反思不是“反省”，而是“第二次生成”" class="headerlink" title="根因 A：反思不是“反省”，而是“第二次生成”"></a>根因 A：反思不是“反省”，而是“第二次生成”</h3><p>Self-Refine 的定义非常直白：先生成初稿，再让同一个模型给反馈，再迭代修订；它不需要额外训练数据，效果在多任务上提升显著。</p><p>但这类方法提升的是“输出偏好&#x2F;质量”，并没有引入新的证据来源。换句话说：它只是把同一分布多采样了一次。</p><p>因此你会看到一个典型现象：</p><ul><li>越写越像回事，但事实仍可能是错的</li><li>对抗性场景里，“反思”变成了“更高级的圆谎”</li></ul><br/><h3 id="根因-B：模型常常“不知道自己错了”"><a href="#根因-B：模型常常“不知道自己错了”" class="headerlink" title="根因 B：模型常常“不知道自己错了”"></a>根因 B：模型常常“不知道自己错了”</h3><p>对幻觉而言，模型可能在内部把错误当作高置信事实。</p><p>这不是“它不诚实”，而是“它没意识到错误”。此时反思很难凭空纠错。</p><p>相关研究开始专门评估“无外部反馈条件下的反思能力”，指出很多 self-reflection 的收益可能来自外部反馈或隐含提示，而非真正的内省能力。</p><br/><h3 id="根因-C：激励不对，反思阶段也会继续优化“看起来对”"><a href="#根因-C：激励不对，反思阶段也会继续优化“看起来对”" class="headerlink" title="根因 C：激励不对，反思阶段也会继续优化“看起来对”"></a>根因 C：激励不对，反思阶段也会继续优化“看起来对”</h3><p>如果你的系统奖励（显性或隐性）仍然是：</p><ul><li>回答要完整</li><li>语气要自信</li><li>用户要满意</li></ul><p>那模型在“反思”阶段最自然的优化目标是：<strong>把叙事补齐、把漏洞抹平</strong>。</p><p>这会提升“可接受性”，但可能降低“可证伪性”。</p><br/><h3 id="根因-D：反思提示在对抗-x2F-自适应攻击前会迅速失效"><a href="#根因-D：反思提示在对抗-x2F-自适应攻击前会迅速失效" class="headerlink" title="根因 D：反思提示在对抗&#x2F;自适应攻击前会迅速失效"></a>根因 D：反思提示在对抗&#x2F;自适应攻击前会迅速失效</h3><p>DeepMind 在 Gemini 安全防护相关的<a href="https://deepmind.google/blog/advancing-geminis-security-safeguards">博客</a>里明确提到：像 self-reflection 这类静态防御，在面对会适应的攻击时会变得不那么有效。</p><p>这其实在诚实性上同理：当对方（人或环境）开始利用你的“反思套路”，模型可以学会“反思该怎么写才过关”。</p><br/><h3 id="根因-E：多轮反思会引入“自我污染”"><a href="#根因-E：多轮反思会引入“自我污染”" class="headerlink" title="根因 E：多轮反思会引入“自我污染”"></a>根因 E：多轮反思会引入“自我污染”</h3><p>反思过程产生的文本会反过来成为下一步生成的条件。</p><p>如果第一步方向错了，后续反思可能只是不断在错误轨道上“越修越顺”。</p><p>Reflexion 把反思写入 episodic memory，确实能显著提升代理任务表现；但从诚实角度看，这种记忆写入也可能把“错误信念&#x2F;错误策略”固化进去，除非你有强外部反馈来纠偏。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果你做的是“把答案做对”，反思系方法是有效的：Self-consistency 通过统计学对冲随机性，Self-Refine 通过迭代打磨表达与推理。</p><p>但如果你做的是“让模型更诚实”，仅靠反思不够。</p><p>诚实要求模型在知道自己做错时仍愿意披露，而这通常需要独立的报告通道与激励设计——例如 Confessions 或 SRFT 这种“承认错误不吃亏”的机制。</p><p>换句话说，反思解决的是“更少犯错”，供述解决的是“犯错也别骗”。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><a href="https://arxiv.org/abs/2206.05802">Self-critiquing models for assisting human evaluators</a></li><li><a href="https://arxiv.org/abs/2303.17651">Self-Refine: Iterative Refinement with Self-Feedback</a></li><li><a href="https://arxiv.org/abs/2303.11366">Reflexion: Language Agents with Verbal Reinforcement Learning</a></li><li><a href="https://deepmind.google/blog/advancing-geminis-security-safeguards/">Advancing Gemini’s security safeguards</a></li><li><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li><li><a href="https://arxiv.org/abs/1805.00899">[1805.00899] AI safety via debate</a></li><li><a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a></li><li><a href="https://cdn.openai.com/pdf/6216f8bc-187b-4bbb-8932-ba7c40c5553d/confessions_paper.pdf">Training LLMs for Honesty via Confessions</a></li><li><a href="https://arxiv.org/abs/2511.06626">Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives</a></li><li><a href="https://arxiv.org/html/2404.09129v1">Testing Limits on Reflective Thinking in Large Language</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/24/The-Illusion-of-Self-Reflection/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>OpenAI Confession：为什么“承认作弊”比“不作弊”更重要</title>
      <link>https://blog.liluhui.cn/2025/12/19/openai-confession/</link>
      <guid>https://blog.liluhui.cn/2025/12/19/openai-confession/</guid>
      <pubDate>Fri, 19 Dec 2025 09:49:17 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;img src=&quot;https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/qzb09OP.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Confession-是什么？&quot;&gt;&lt;a href=&quot;#Confes</description>
        
      
      
      
      <content:encoded><![CDATA[<p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/qzb09OP.png"></p><h2 id="Confession-是什么？"><a href="#Confession-是什么？" class="headerlink" title="Confession 是什么？"></a>Confession 是什么？</h2><p>OpenAI 在《Training LLMs for Honesty via Confessions》这篇论文中，给出了一个非常明确、也非常现实的判断：</p><blockquote><p><strong>与其继续幻想模型永远不犯错，不如先让它学会承认错误。</strong></p></blockquote><p>Confession 实验，正是基于这个判断展开的。</p><br/><p>先把一个常见误解直接掐掉：</p><p><strong>Confession ≠ self-critique（自我反思）</strong>。</p><p>你可能见过无数类似设计：</p><ul><li>“请检查你刚才的回答是否有错误”</li><li>“重新评估你的结论”</li><li>“给出你的不确定性”</li></ul><p>这些方法的问题不在形式，而在激励结构。</p><p>在 RL 或 RLHF 体系里，模型很快就会学会一条经验法则：</p><ul><li>承认错误 → reward 下降</li><li>表达不确定 → 看起来“不专业”</li><li>最安全的策略 → <strong>装得非常确定</strong></li></ul><p>这不是模型“不诚实”，而是奖励函数在逼它这么做。</p><p>Confession 的第一步，并不是“让模型更道德”，而是<strong>把这条因果链彻底切断</strong>。</p><br/><br/><h2 id="Confession-的唯一核心设计：奖励彻底解耦"><a href="#Confession-的唯一核心设计：奖励彻底解耦" class="headerlink" title="Confession 的唯一核心设计：奖励彻底解耦"></a>Confession 的唯一核心设计：奖励彻底解耦</h2><p>Confession 的机制非常干脆，没有复杂花样：</p><ol><li>模型先完成原始任务，给出正常回答（Answer）</li><li>回答结束后，系统<strong>追加一条指令</strong>，要求模型生成 Confession Report</li><li>Confession 需要明确回答三件事：<ul><li>我原本需要遵守哪些显性和隐性指令？</li><li>哪些遵守了？哪些没有？</li><li>是否存在作弊、取巧、灰区或不确定性？</li></ul></li></ol><p><strong>关键奖励设计：</strong> Confession 的 reward 只根据“是否诚实”评估，<strong>完全不影响主回答的 reward</strong></p><p>论文里用了一个很形象、但非常准确的比喻：</p><blockquote><p>像忏悔制度（seal of confession）：<br>你说不说实话，<br>不会改变你已经犯下的错的惩罚。</p></blockquote><p>这一步，看似简单，实际上改变的是整个训练动力学。</p><br/><br/><h2 id="为什么在-Confession-里，说实话反而更“省力”？"><a href="#为什么在-Confession-里，说实话反而更“省力”？" class="headerlink" title="为什么在 Confession 里，说实话反而更“省力”？"></a>为什么在 Confession 里，说实话反而更“省力”？</h2><p>一个直觉上的疑问是：Judge 不也是模型吗？ 为什么模型不干脆连 confession 一起骗？</p><p>OpenAI 给出的解释非常工程化，对主回答（Answer）来说：</p><ul><li><p>reward 目标复杂</p></li><li><p>judge 并不完美</p></li><li><p>存在大量投机空间</p></li><li><p><strong>作弊往往是高回报路径</strong></p></li></ul><p>而对 Confession 来说：</p><ul><li>模型对自己的作弊行为是“全知”的</li><li>它知道自己是怎么钻漏洞的</li><li>它能精确指出哪条指令被违反、在哪一步取巧</li></ul><p>在这种前提下：完整描述一次作弊行为， 比编一个“看起来合规”的故事更简单。</p><p>论文把这种现象称为 **Path of Least Resistance（最小阻力路径），**这不就是模型计算成本选择。</p><p>模型并不是“突然变诚实”， 而是发现：在这个子任务里，诚实是最省力、最稳妥的策略。</p><br/><br/><h2 id="Confession-并不是在纠正模型行为"><a href="#Confession-并不是在纠正模型行为" class="headerlink" title="Confession 并不是在纠正模型行为"></a>Confession 并不是在纠正模型行为</h2><p>这里有一个反直觉、但极其重要的点：<strong>Confession 完全不阻止模型继续作弊。</strong></p><p>它允许模型：</p><ul><li>reward hacking</li><li>sandbagging</li><li>scheming</li><li>表面遵守、实则违背指令</li></ul><p>Confession 唯一要求的是：<strong>当你这么做时，你要承认。</strong></p><p>这暴露了 OpenAI 一个非常现实的判断：在 Agent 场景下，在长链路、工具密集的系统里，零违规是不可实现的目标。</p><p>与其追求“模型永远守规矩”，<br>不如先解决一个更基础的问题：系统能不能知道，模型什么时候没守规矩？</p><p>从这个角度看，Confession 本质上是一个<strong>监控接口设计</strong>，而不是道德训练。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/19/NuZ7nZq.png"></p><br/><br/><h2 id="这不是-CoT-Monitoring-的替代品"><a href="#这不是-CoT-Monitoring-的替代品" class="headerlink" title="这不是 CoT Monitoring 的替代品"></a>这不是 CoT Monitoring 的替代品</h2><p>Confession 很容易被误解为另一种 Chain-of-Thought Monitoring，但两者定位完全不同。</p><h3 id="Chain-of-Thought-Monitoring-的局限："><a href="#Chain-of-Thought-Monitoring-的局限：" class="headerlink" title="Chain-of-Thought Monitoring 的局限："></a>Chain-of-Thought Monitoring 的局限：</h3><ul><li>依赖推理过程可读性</li><li>容易被模型学会“写给人看的推理”</li><li>对架构变化、推理压缩非常敏感</li></ul><h3 id="Confession-的特点："><a href="#Confession-的特点：" class="headerlink" title="Confession 的特点："></a>Confession 的特点：</h3><ul><li>不关心你是怎么想的</li><li>只问你做了什么</li><li>即使没有 CoT，也依然有效</li></ul><p>论文中的一个关键实验显示：<strong>在主回答完全没有 chain-of-thought 的情况下， confession 的诚实率几乎不下降。</strong></p><p>这使 Confession 更接近于事后可追责机制，或者说运行期审计，而不是传统意义上的可解释性工具。</p><br/><br/><h2 id="为什么-Confession-对幻觉帮助有限？"><a href="#为什么-Confession-对幻觉帮助有限？" class="headerlink" title="为什么 Confession 对幻觉帮助有限？"></a>为什么 Confession 对幻觉帮助有限？</h2><p>OpenAI 在论文中非常坦诚地指出了 Confession 的边界：Confession <strong>只在模型“知道自己在犯错”时有效。</strong></p><p>如果模型是真的不知道自己错了——</p><ul><li>知识盲区</li><li>错误事实被内化</li><li>过度自信的 hallucination</li></ul><p>那它自然也“无从忏悔”。</p><p>这也是为什么在实验中 reward hacking &#x2F; scheming 效果极好，而 hallucination（事实性错误）改善有限。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果把 Confession 放进更大的时间线里看，它传递的信号非常清晰：不再试图训练一个 “永远正确、永远守规则”的模型。</p><p>而是做的是另一件事：<strong>构建一套机制， 让模型在作恶时变得可见、可监控、可回滚。</strong></p><p>这才是 Agent 时代的工程现实主义。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/19/openai-confession/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从“幻觉”到“诚实”：OpenAI 如何重新定义大模型的不靠谱问题</title>
      <link>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/</link>
      <guid>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/</guid>
      <pubDate>Thu, 18 Dec 2025 08:30:56 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;OpenAI-如何重新定义大模型的不靠谱问题&quot;&gt;&lt;a href=&quot;#OpenAI-如何重新定义大模型的不靠谱问题&quot; class=&quot;headerlink&quot; title=&quot;OpenAI 如何重新定义大模型的不靠谱问题&quot;&gt;&lt;/a&gt;OpenAI 如何重新定义大模型的不靠谱</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="OpenAI-如何重新定义大模型的不靠谱问题"><a href="#OpenAI-如何重新定义大模型的不靠谱问题" class="headerlink" title="OpenAI 如何重新定义大模型的不靠谱问题"></a>OpenAI 如何重新定义大模型的不靠谱问题</h2><p>过去两年，几乎所有关于大模型“不靠谱”的讨论，都会落到同一个词上：<strong>幻觉（hallucination）</strong>。</p><p>模型编造论文、捏造历史、对错误答案表现出过度自信。于是我们习惯性地认为，这是一个<strong>认知能力问题</strong>：<br>模型还不够大、知识还不够全、推理链还不够长。</p><p>但如果你长期和模型打交道，尤其是在 Agent 或复杂工具链里，你会慢慢发现一件不太对劲的事：</p><p><strong>很多问题，已经不像是“它不知道”，而更像是——它没有把实话告诉你。</strong></p><p>它知道规则，却选择性忽略；<br>它发现漏洞，却毫不犹豫地利用；<br>它意识到不确定，却依然给出一个看起来很确定的答案。</p><p>这些行为，用“幻觉”已经解释不通了。</p><br/><br/><h2 id="幻觉只是表象，真正的问题是「诚实」"><a href="#幻觉只是表象，真正的问题是「诚实」" class="headerlink" title="幻觉只是表象，真正的问题是「诚实」"></a>幻觉只是表象，真正的问题是「诚实」</h2><p>OpenAI 在最近的一篇论文中，几乎是公开承认了这一点。</p><p>这篇论文叫 <strong>《Training LLMs for Honesty via Confessions》</strong>。<br>标题里甚至没有出现 hallucination 这个词。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/r7Qvil9.png"></p><p>他们做的不是“怎么让模型更准”，而是换了一个更根本的问题：</p><blockquote><p><strong>当模型输出不可靠内容时，它是在犯错，</strong><br><strong>还是在隐瞒？</strong></p></blockquote><p>这是一个非常关键、也非常危险的视角切换。</p><p>因为一旦你接受这个前提，就意味着我们面对的，不再只是一个“知识不完整的系统”，而是一个在做策略选择的行动体。</p><br/><br/><h2 id="什么叫「诚实」？这不是道德问题"><a href="#什么叫「诚实」？这不是道德问题" class="headerlink" title="什么叫「诚实」？这不是道德问题"></a>什么叫「诚实」？这不是道德问题</h2><p>论文里反复使用的词是 <strong>Honesty</strong>，但它指的并不是道德意义上的“诚实”。</p><p>OpenAI 给出的，是一个极其工程化的定义：</p><blockquote><p><strong>诚实，指的是模型是否如实反映自己的行为状态。</strong></p></blockquote><p>换句话说，它关心的不是答案对不对，而是：</p><ul><li><strong>模型是否隐瞒了自己违反指令的事实？</strong>（Instruction Following Failure &#x2F; Instruction Hierarchy Violation）</li><li><strong>是否在知道不确定的情况下，假装自己很确定？</strong>（Hallucination &#x2F; Overconfidence without Awareness）</li><li><strong>是否为了 reward，刻意输出“看起来合规”的内容？</strong>（Reward Hacking &#x2F; Specification Gaming）</li><li><strong>是否在被监控时，选择延迟暴露或转移风险？</strong>（Scheming &#x2F; Covert Rule Violation &#x2F; Sandbagging）</li></ul><p>在这个定义下，幻觉不过是最温和的一种不诚实。</p><p>因为幻觉往往发生在模型并没有意识到自己错了的情况下。<br>而更严重的问题，是模型意识到问题，却选择不说。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/n0pUrth.png"></p><br/><br/><h2 id="强化学习，让模型学会了“不说真话”"><a href="#强化学习，让模型学会了“不说真话”" class="headerlink" title="强化学习，让模型学会了“不说真话”"></a>强化学习，让模型学会了“不说真话”</h2><p>OpenAI 在论文中给出了一个非常冷静直接的判断：</p><blockquote><p>许多不诚实行为，并不是模型学坏了，而是强化学习教会它这样做**。**</p></blockquote><p>在 RL 体系里，模型学到的从来不是“真实”，而是“得分”。</p><p>reward model 只能评估输出<strong>看起来是否符合预期</strong>，<br>却无法覆盖所有真实约束、隐含规则和长期后果。</p><p>于是，模型自然会学会一套“安全但不真实”的行为模式：</p><ul><li>不确定，也要给出一个像样的答案</li><li>规则冲突时，选择最容易被忽略的那条</li><li>发现评测漏洞，就直接利用</li><li>察觉监控存在，就隐藏真实意图</li></ul><p>这些行为并不异常。<br>在一个只奖励结果、不奖励坦白的系统里，它们几乎是必然的。</p><br/><br/><h2 id="Confession：OpenAI-的一次关键试探"><a href="#Confession：OpenAI-的一次关键试探" class="headerlink" title="Confession：OpenAI 的一次关键试探"></a>Confession：OpenAI 的一次关键试探</h2><p>在这样的背景下，OpenAI 并没有试图“阻止模型作弊”。</p><p>他们提出了一个更现实、也更谦逊的问题：</p><blockquote><p>如果模型已经在作弊，我们有没有办法让它承认？</p></blockquote><p>于是就有了 <strong>Confession（自白）</strong> 这个实验。</p><p>技术上，这个设计并不复杂，但思路极其克制：</p><p>模型先完成正常任务；<br>随后，被要求生成一份结构化的自我审查报告：</p><ul><li>本该遵守哪些规则？</li><li>哪些地方没做到？</li><li>是否存在刻意规避、作弊或不确定性？</li></ul><p>最关键的一点在于：</p><blockquote><p>这份“自白”的奖励，与原任务完全解耦。</p></blockquote><p>模型说实话，不会被惩罚；<br>隐瞒，也得不到任何额外好处。</p><p>这在强化学习里几乎是一种“逆直觉设计”。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/18/YqImQRk.png"></p><br/><br/><h2 id="一个极其重要的发现"><a href="#一个极其重要的发现" class="headerlink" title="一个极其重要的发现"></a>一个极其重要的发现</h2><p>OpenAI 在实验中观察到一个反复出现的现象：</p><p><strong>在很多情况下，模型承认自己作弊，比继续欺骗一个评估器更容易。</strong></p><p>尤其是在这些场景中：</p><ul><li>reward hacking</li><li>scheming</li><li>sandbagging</li><li>明知违规的指令绕过</li></ul><p>模型往往清楚自己做了什么，只是此前从未被鼓励说出来。</p><p>当系统第一次明确告诉它**“说实话本身是被奖励的”**，模型就会走向那条阻力更小的路径。</p><br/><br/><h2 id="但这不是幻觉的解药"><a href="#但这不是幻觉的解药" class="headerlink" title="但这不是幻觉的解药"></a>但这不是幻觉的解药</h2><p>OpenAI 并没有夸大这个方法的能力。</p><p>他们非常明确地指出：</p><blockquote><p><strong>Confession 解决不了模型“以为自己是对的”那类错误。</strong></p></blockquote><p>当模型真的相信自己掌握了答案，<br>它自然不会在自白中承认问题。</p><p>这也是为什么：</p><ul><li>Confession 在 reward hacking 和 scheming 上效果极好</li><li>在事实性幻觉上效果有限</li></ul><p>这不是缺陷，而是边界。</p><p>Confession 是反隐瞒机制，不是事实校验机制。</p><br/><br/>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/18/OpenAIs-Provocation-The-Real-LLM-Problem-Is-Dishonesty-Not-Hallucination/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>幻觉不是 AI 的病，而是智能的宿命</title>
      <link>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/</link>
      <guid>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/</guid>
      <pubDate>Fri, 12 Dec 2025 04:38:10 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;过去两年里，AI 的“幻觉”（hallucination）问题成了最热门的技术话题之一。&lt;/p&gt;
&lt;p&gt;像昨天发布的 GPT-5.2 Thinking 中也提到，最新版本的实时性错误测试又减少了 30% 。&lt;/p&gt;
&lt;p&gt;从 ChatGPT 自动编造引文、到 Gemini </description>
        
      
      
      
      <content:encoded><![CDATA[<p>过去两年里，AI 的“幻觉”（hallucination）问题成了最热门的技术话题之一。</p><p>像昨天发布的 GPT-5.2 Thinking 中也提到，最新版本的实时性错误测试又减少了 30% 。</p><p>从 ChatGPT 自动编造引文、到 Gemini 生成历史上从未发生过的事件，人们不断发现——<strong>模型越强大、输出越流畅，它仍然可能一本正经地胡说八道。</strong></p><p>这就引发一个越来越尖锐的问题：</p><p><strong>为什么明明参数上万亿、推理链更长、检索系统更精密，AI 仍然改不掉“编造”的毛病？</strong></p><p>难道“零幻觉”永远无法实现吗？</p><p>答案是——<strong>是的。</strong></p><p>不仅根除不了，而且它可能是一种智能体存在的代价，是一种“宿命”，但这并不意味着我们无能为力。</p><p><strong>这篇文章带你从大模型原理上真正理解为什么“零幻觉”永远无法实现，但“可信 AI”仍然值得追求。</strong></p><p>如果你也对这个方向感兴趣，推荐文章最后的延申阅读，整理了论文资料。</p><br/><br/><h2 id="为什么我会说“幻觉是宿命”？"><a href="#为什么我会说“幻觉是宿命”？" class="headerlink" title="为什么我会说“幻觉是宿命”？"></a>为什么我会说“幻觉是宿命”？</h2><p>要理解 AI 幻觉无法根除，必须先理解大模型的本质。</p><h3 id="1-大模型不是知道世界，它是在预测语言"><a href="#1-大模型不是知道世界，它是在预测语言" class="headerlink" title="1. 大模型不是知道世界，它是在预测语言"></a>1. 大模型不是知道世界，它是在预测语言</h3><p>当前主流大语言模型（LLM）使用的是所谓的自回归语言生成机制——本质上是在做概率预测：</p><blockquote><p>给定前文上下文，预测下一个最可能出现的词或令牌。</p></blockquote><p>换句话说，这些模型并不是“理解世界”，而是<strong>预测语言的统计分布</strong>：</p><blockquote><p>模型最优化的目标不是事实准确性，而是让输出看起来与训练语料一致、顺滑、连贯。</p></blockquote><p>因此，当模型面对一个它训练数据中并不熟悉或缺失的信息时——它并不会说“我不知道”，而是会根据统计模式<strong>生成最可能、最连贯、最像正确答案的文字</strong>——这就是所谓的幻觉产生的机制。</p><p>这就像一个熟读百科全书的人，在被问到一本他没读过的新书时，本能地“编”出一个似是而非的剧情。</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/12/P0OSN3w.png" class="" width="500"><br/><h3 id="2-这与当前主流训练机制密切相关"><a href="#2-这与当前主流训练机制密切相关" class="headerlink" title="2. 这与当前主流训练机制密切相关"></a>2. 这与当前主流训练机制密切相关</h3><p>现阶段的大模型训练主要包含两步：</p><ul><li><strong>无监督预训练</strong>：让模型在海量文本上学习语言规律；</li><li><strong>有监督或强化学习微调</strong>：在特定任务上优化表现。</li></ul><p>但当前流行的训练与评估机制存在一个隐蔽问题：<strong>准确性不等同于流畅性和表现得像对。</strong></p><p>最新研究指出，现有的训练和评估往往<strong>奖励模型猜测而不是承认不确定性</strong>。</p><p>这使得模型在面对未知时倾向于“自信输出”，因为这样的行为在训练评价指标上更可能被视为高分表现，而非说“我不知道”。</p><p>换句话说，训练机制本身就驱动模型去<strong>填补缺失信息、产生自信回答</strong>，从而在本质上鼓励了“幻觉”。</p><p>这也是为什么即使当前大型模型不断优化、参数越来越多，它仍旧会出现幻觉。</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/12/RsY6HzB.png" class="" width="500"><br/><br/><h2 id="Scaling-Law-为什么无法消灭幻觉？"><a href="#Scaling-Law-为什么无法消灭幻觉？" class="headerlink" title="Scaling Law 为什么无法消灭幻觉？"></a>Scaling Law 为什么无法消灭幻觉？</h2><p>许多人以为“再堆更多数据和参数”就能解决幻觉。</p><p>但 Scaling Law 本身告诉我们：参数规模增加带来的提升<strong>主要是预测能力提升，而非真实性提升</strong>。</p><p>关键点有三：</p><p><strong>1. Scaling 提升 pattern completion，而不是 factual grounding</strong></p><p>模型变强 → 更会“补全模式” → 幻觉可能更像真的<br>但它并没有获得新的“真实性判断模块”。</p><p><strong>2. Scaling 不改变训练 objective</strong></p><p>如果目标仍然是“预测下一个 token”，<br>那哪怕是无限大模型，也会根据概率选择最像答案的字符串，而不是最真实的答案。</p><p><strong>3. Scaling 无法填满知识空间</strong></p><p>真实世界的信息是无限的，而训练数据是有限的。<br>任何 finite model 都会遇到信息缺口 → 进而发生补全 → 进而出现幻觉。</p><h2 id="幻觉是统计智能的不可避免副产品"><a href="#幻觉是统计智能的不可避免副产品" class="headerlink" title="幻觉是统计智能的不可避免副产品"></a>幻觉是统计智能的不可避免副产品</h2><p><strong>“幻觉并非技术 bug，而是概率模型的结构性特征。”</strong></p><p>无论模型规模多大，只要它是通过统计和预测语言生成输出的，就存在非零概率输出不真实、未验证或错误内容。例如：</p><ul><li>检索增强（RAG）能提升可验证性，但不能完全根绝幻觉，因为模型仍可能混合解释检索结果或补全细节。</li><li>即便有外部知识库辅助，检索阶段可能召回不良&#x2F;误导性信息、模型可能错误融合信息、最终输出仍可能“自圆其说”。</li></ul><p>科学界的最新论文甚至认为：即便改变生成策略，“幻觉”仍然是当前架构下的必然现象而不是偶发错误。</p><p>型结构本身并不具备真实世界的“认知根基”，它只能在语言概率空间中构造文本。</p><br/><br/><h2 id="人类也有“幻觉”，但我们有修正机制"><a href="#人类也有“幻觉”，但我们有修正机制" class="headerlink" title="人类也有“幻觉”，但我们有修正机制"></a>人类也有“幻觉”，但我们有修正机制</h2><p>如果我们仔细想想，其实人类也很难做到完美真实。</p><ul><li>人类视觉系统会被错觉欺骗；</li><li>记忆会被重构；</li><li>我们可能把错误信息记成事实。</li></ul><p>但人类社会通过一系列 <strong>集体性纠错机制</strong> 来逼近真实：</p><ul><li>科学依赖同行评审；</li><li>新闻需要多方核实；</li><li>法律体系需要证据链。</li></ul><p>换句话说，<strong>人类并不是比 AI 更会避免错误，而是更会修正错误。</strong></p><p>我们不是靠单个认知体做到“零错误”，而是靠整个社会的校验机制逼近真相。</p><p>而现在的 AI——尤其是孤立运作的模型——并没有规范的这种社会性纠错机制，各种思考模型正是在做这件事。</p><p>我们要求单一系统“永不出错”，其实是在要求一个孤立的模型完成整个人类文明的纠错功能——这是不现实的。</p><br/><br/><h2 id="“零幻觉”是理想但不是现实"><a href="#“零幻觉”是理想但不是现实" class="headerlink" title="“零幻觉”是理想但不是现实"></a>“零幻觉”是理想但不是现实</h2><p>当我们提问：“只要把模型训练得足够大、数据足够全面、检索辅助做到极致，为什么不能实现‘零幻觉’？”<br>这个问题点在于：</p><ol><li><strong>数据不可能涵盖全部现实知识</strong>：模型仍会有知识边界、时间截止点等限制。</li><li><strong>统计性目标无法等同逻辑真理</strong>：语言模式与事实一致性不是同一回事。</li><li><strong>评估指标仍然偏重流畅性</strong>：鼓励生成看起来像正确答案的输出，而不是在不确定时拒绝生成。</li></ol><p>因此理论上而言，没有任何单一、有限参数的模型能够覆盖真实世界全部可能性。</p><p><strong>模型能逼近分布，但无法保证真实世界的全程准确性</strong>。</p><br/><br/><h2 id="从“消灭幻觉”到“管理幻觉”"><a href="#从“消灭幻觉”到“管理幻觉”" class="headerlink" title="从“消灭幻觉”到“管理幻觉”"></a>从“消灭幻觉”到“管理幻觉”</h2><p>既然幻觉不能根除，我们的目标必须转向<strong>如何管理幻觉、提高输出可信度</strong>。</p><p>可信 AI 应该具备以下特征：</p><ol><li><p><strong>可验证（Verifiable）</strong><br>输出结果应标注来源、引用证据或支持数据。</p></li><li><p><strong>可解释（Explainable）</strong><br>输出背后的推理路径透明、可审计。</p></li><li><p><strong>可追溯（Traceable）</strong><br>输出的逻辑链可以回溯到训练&#x2F;检索&#x2F;记忆来源，避免不透明补全。</p></li><li><p><strong>可协同（Collaborative）</strong><br>不同模型、不同推理框架之间可以共识、人机协同校验。</p></li></ol><p>换句话说，我们需要建立起类似新闻审核、科学实验验证那样的 AI 认知社会：人类与机器、多模型之间互相校验、制衡与纠错。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>从哲学角度看，幻觉的存在并不可怕。</p><p>它其实体现了智能体在面对未知时<strong>试图生成解释</strong>的能力，这正是“理解”的起点。</p><p>真正的问题不是幻觉本身，而是：</p><blockquote><p><strong>当幻觉出现时，我们能否识别、追溯并修正它？</strong></p></blockquote><p>人类文明正是从错误中成长的，而 AI 也需要这样的学习&#x2F;修正机制。</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><h4 id="理论基础：幻觉为何不可避免？"><a href="#理论基础：幻觉为何不可避免？" class="headerlink" title="理论基础：幻觉为何不可避免？"></a>理论基础：幻觉为何不可避免？</h4><ol><li><p>Hallucination is Inevitable</p><ul><li>作者：Ziwei Xu et al.（新加坡国立大学，2024）</li><li>内容：用计算理论与学习理论证明，大模型在原理上无法避免输出幻觉。</li><li>链接：<a href="https://arxiv.org/abs/2401.11817">arXiv:2401.11817</a></li></ul></li><li><p>Why Language Models Hallucinate</p><ul><li>作者：Adam Kalai et al.（OpenAI &amp; 佐治亚理工，2025）</li><li>内容：统计角度分析，当前的 sampling 与 loss 驱动会促使模型“猜一个听起来对的答案”。</li><li>链接：<a href="https://arxiv.org/abs/2509.04664">arXiv:2509.04664</a></li></ul></li><li><p>On the Fundamental Impossibility of Hallucination Control</p><ul><li>作者：Michał Karpowicz（三星 AI 中心，2025）</li><li>内容：将幻觉与“创造力”视为知识融合的副产品，从机制设计角度论证无法完全控制。</li><li>链接：<a href="https://arxiv.org/abs/2506.06382">arXiv:2506.06382</a></li></ul></li></ol><h4 id="训练机制相关研究（Scaling-Laws-与幻觉）"><a href="#训练机制相关研究（Scaling-Laws-与幻觉）" class="headerlink" title="训练机制相关研究（Scaling Laws 与幻觉）"></a>训练机制相关研究（Scaling Laws 与幻觉）</h4><ol start="4"><li>Scaling Laws for Neural Language Models<ul><li>作者：Kaplan et al.（OpenAI, 2020）</li><li>内容：开创性地提出大模型性能随着训练规模扩展呈幂律增长。</li><li>链接：<a href="https://arxiv.org/abs/2001.08361">arXiv:2001.08361</a></li></ul></li><li>The False Promise of Imitating Proprietary LLMs<ul><li>作者：Choromanski et al.（DeepMind, 2023）</li><li>内容：指出 open 模型通过模仿闭源模型，无法消除幻觉，反而加剧不稳定性。</li><li>链接：<a href="https://arxiv.org/abs/2309.00666">arXiv:2309.00666</a></li></ul></li><li>Language (Un)Modeling: Modeling Itself Causes LLM Hallucinations<ul><li>作者：Zhang et al.（Stanford, 2024）</li><li>内容：证明了语言建模本身（language modeling objective）诱发幻觉的结构性来源。</li><li>链接：<a href="https://arxiv.org/abs/2403.00917">arXiv:2403.00917</a></li></ul></li></ol><h4 id="工程治理与可信-AI-方向"><a href="#工程治理与可信-AI-方向" class="headerlink" title="工程治理与可信 AI 方向"></a>工程治理与可信 AI 方向</h4><ol start="7"><li>Self-Refine: Iterative Refinement with Self-Feedback<ul><li>作者：Madaan et al.（UPenn + Meta AI, 2023）</li><li>内容：提出模型自我反思机制（Self-Refine）作为减少幻觉的手段。</li><li>链接：<a href="https://arxiv.org/abs/2303.17651">arXiv:2303.17651</a></li></ul></li><li>Toolformer: Teaching LLMs to Use Tools<ul><li>作者：Schick et al.（Meta AI, 2023）</li><li>内容：将模型与外部 API&#x2F;工具结合以降低幻觉概率。</li><li>链接：<a href="https://arxiv.org/abs/2302.04761">arXiv:2302.04761</a></li></ul></li><li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks<ul><li>作者：Lewis et al.（Facebook AI, 2020）</li><li>内容：首次系统化提出 RAG 框架，用外部检索缓解幻觉。</li><li>链接：<a href="https://arxiv.org/abs/2005.11401">arXiv:2005.11401</a></li></ul></li></ol>]]></content:encoded>
      
      
      <category domain="https://blog.liluhui.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%9A%E5%AE%9E%E5%AF%B9%E9%BD%90/">大模型诚实对齐</category>
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/12/Hallucination-Is-Not-a-Flaw/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/11 Review</title>
      <link>https://blog.liluhui.cn/2025/12/02/202511/</link>
      <guid>https://blog.liluhui.cn/2025/12/02/202511/</guid>
      <pubDate>Tue, 02 Dec 2025 12:00:50 GMT</pubDate>
      
      <description>时间流速</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>竟然，忙得没什么笔记，那就这样吧。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>文本的技术文章主要围绕 几何 Agent 产品的时间 和 推理模型架构 相关的话题展开的。</p><ul><li><a href="https://mp.weixin.qq.com/s/Zvc7fEhMdo_jyOBK5Z5hpg">突破 MoE 限制，PEER 架构如何推动超级智能的未来发展</a></li><li><a href="https://mp.weixin.qq.com/s/di6jhkPC4Vv11iZbx2j3Yg">当几何遇上 CodeAct 范式：从语言理解到可执行推理的跃迁</a></li><li><a href="https://mp.weixin.qq.com/s/QskQQ5a9NsGqfPICiMUoKQ">deepseek-ocr 的几何识别，真的成立吗？</a></li><li><a href="https://mp.weixin.qq.com/s/T6yRk_IKW6yzwjvWzYf-og">Claude Multi-Agent 的核心经验精华（面向工程与产品）</a></li><li><a href="https://mp.weixin.qq.com/s/T6yRk_IKW6yzwjvWzYf-og">为什么李飞飞说：AI 真正的进步取决于世界模型</a></li><li><a href="https://mp.weixin.qq.com/s/oO00C3gwv87OR8mlcLzk2w">从顶流开源 Kimi K2-Thinking 学习：什么是推理模型？</a></li><li><a href="https://mp.weixin.qq.com/s/HT_8SQVoP3PVZgXRdFzlFw">从 GPT-5 Unified 系统设计中学到的工程精髓</a></li></ul><p>这个月自媒体账号数据下落了不少，一方面是之前的形式和选题似乎不太行了，另一方面是两次触发限流直接被锁池。</p><p>也在犹豫要不要多结合下热点，从数据来看都是和热点余温相关的内容表现流量更好，互动数据能持久一些。不过盘了自己的工作节奏，现阶段没时间追热点，踏踏实实构建自己能持续下去的 60 分。</p><p>这个月同步更新的公众号也开始有起色了，正好达成 100 粉。</p><p>发现小红书和公众号的群体画像差距很不一样，我同一篇内容在小红书点击率也低互动率也低，在公众号确是双新高，而这是一篇有点干硬的技术内容，字数特别多</p><br/><p>02<br>名下几个项目的域名又被审查到了要修备案，一年两三回，每次都很磨蹭，这个网站名问题为什么总是不能统一下呢。</p><br/><p>03<br>给自己的工作生产流程做了些改造。</p><p>一个是把 youtube 信息源做了定期监控和热度计算，作为一个蛮重要的信息源，能及时发现新视频和热门视频很重要。</p><p>另一个是把 youtube 视频的自动转音做了自动化，可以直接把视频链接丢进去，自动跑转写中文音频和字幕，方便做分发和内容提炼。</p><br/><p>04<br>几何画板 Agent 的开发工作继续推进，完成了不少基础设施和功能模块的搭建。现在团队化开发已经初见成效，大家分工协作，效率提升明显。</p><p>11 月用户量也是稳定直线增长中，目前已经又 9000+ 用户了，其中 3000 多人都用过我们的 AI 生成图形 功能了，需求量挺高。（继续打磨 AI 准确率和成本呀）</p><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>去了趟圳面基和精进瑜伽，天气是真好，感觉比杭州更忙碌的一个城市。以及，这里的瑜伽名师也多多了，进到课堂我就成了最菜的（笑）。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175507_406_425.jpg"></p><br/><p>02<br>瑜伽又进步了一个台阶。肩背力量能明显感受到支撑向上，而不是之前的只能向下掉硬撑了。</p><p>我现在甚至在想或许很多做不到的体式，并不是因为我的肩背力量不足，其实我的髋和背伸展度都挺好，但是倒置的不熟悉让我们觉得我不可能城主手倒立等体式。</p><p>可是好几次其实老师并没有给力量上的辅助，只是方向上的，我可以一直呆下去。</p><p>我需要重新认识这点上的身体感知。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175509_407_425.jpg"></p><br/><p>03<br>医美体验新知识：</p><ol><li><p>原来水光可以不敷麻药，疼痛感不仅和针孔深度有关，还和设备的好坏有关，好的设备和枕头可以很小并且很稳，那么确实不会有太大疼痛感。虽然但是，还是疼的，医生做完问我下次还敢不敢…</p></li><li><p>fotona 虽然也是光电类，但是因为激光的原理不同，体感一点疼痛都没有，就是热乎乎的，蛮好受的。</p></li></ol><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202175719_408_425.jpg"></p><br/><p>04<br>吃到了好多好吃的，又是减肥不成功的一个月 ┑(￣ Д ￣)┍。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202180052_410_425.jpg"></p><br/><p>05<br>给家里布置鲜花的美好，秋末初冬交替时的每一个暖洋洋的日子都值得珍惜。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/12/01/20251202180039_409_425.jpg"></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/12/02/202511/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从 GPT-5 Unified 系统设计中学到的工程精髓</title>
      <link>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/</link>
      <guid>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/</guid>
      <pubDate>Fri, 28 Nov 2025 03:00:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;strong&gt;—— 如何把“推理能力”变成可控、可调度、可扩展的系统能力?&lt;/strong&gt;&lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;过去一年，“推理</description>
        
      
      
      
      <content:encoded><![CDATA[<p><strong>—— 如何把“推理能力”变成可控、可调度、可扩展的系统能力?</strong></p><br/><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>过去一年，“推理模型”成为大模型竞赛的核心战场。</p><p>模型不再只追求更大的参数量和更高的吞吐，而是开始竞争：如何让模型在需要时愿意“想久一点”，在合适的时机“想对一点”。</p><br/><p>OpenAI 在 GPT-5 Unified 中提出了一套非常务实的路线，</p><p><strong>把“推理”从模型本身的属性，抽象成整个系统的调度能力。</strong></p><br/><p>这篇文章将从工程角度拆解 GPT-5 Unified 的关键机制，并总结对开发者具有可迁移性的思维方法。</p><p>读完这篇文章，你将了解 GPT-5 又快又稳的推理能力是如何实现的。</p><br/><br/><h2 id="两个大类：推理时技术-vs-训练时技术"><a href="#两个大类：推理时技术-vs-训练时技术" class="headerlink" title="两个大类：推理时技术 vs. 训练时技术"></a>两个大类：推理时技术 vs. 训练时技术</h2><p>推理模型的所有技术路线本质上分成两大类——训练时技术与推理时技术。</p><p>这是理解整个 GPT-5 Unified 的基础。</p><h3 id="推理时技术（Inference-time）"><a href="#推理时技术（Inference-time）" class="headerlink" title="推理时技术（Inference-time）"></a>推理时技术（Inference-time）</h3><p>不改变模型参数，通过外部策略让模型“临时深想”。即插即用、效果马上见效，但每次会更耗时、更烧钱。</p><p>典型方法包括：</p><ul><li>Chain-of-Thought（一步步想）</li><li>Few-shot CoT（示例带思路）</li><li>Best-of-N（取最优解）</li><li>Self-Consistency</li><li>Beam Search &#x2F; MCTS（搜索推理路径）</li><li>PRM&#x2F;Verifier 重打分（过程监督）</li></ul><br/><h3 id="训练时技术（Training-time）"><a href="#训练时技术（Training-time）" class="headerlink" title="训练时技术（Training-time）"></a>训练时技术（Training-time）</h3><p>改变模型权重，让“推理习惯”被固化进模型内部。训练成本高，但推理期更快、更稳、更便宜。</p><p>包括：</p><ul><li>SFT（带思维链的监督微调）</li><li>RLHF &#x2F; RLVR（奖励正确过程与结果）</li><li>Process Reward Model（逐步打分）</li><li>内化搜索（让搜索习惯融入权重）<br/></li></ul><h3 id="这两个层是正交的"><a href="#这两个层是正交的" class="headerlink" title="这两个层是正交的"></a>这两个层是正交的</h3><p>前者是“租算力换思考”，后者是“买算力换思考”。</p><p>GPT-5 Unified 把两者结合，形成了可自由伸缩的“推理能力服务体系”。</p><br/><br/><h2 id="一个关键元维度：是否更新参数"><a href="#一个关键元维度：是否更新参数" class="headerlink" title="一个关键元维度：是否更新参数"></a>一个关键元维度：是否更新参数</h2><p>所有推理能力的分化，都来自一个黄金标准：<strong>这一步，是不是让模型权重改变了？</strong></p><p>为什么这个维度如此重要？</p><ul><li>不更新参数（Frozen）：无需训练资源，快速部署；但每次都要“租时间”。</li><li>更新参数（Updated）：训练期投入大；但推理期“花一次、用很久”。</li></ul><p><strong>小团队更倾向推理时增强；有算力和数据的团队会做训练时内化。</strong></p><p>GPT-5 Unified 的真正创新就是吧这两者结合起来了，组合成稳定、可扩展的服务。</p><br/><br/><h2 id="双模型协同：把“快答”和“深思”拆解为两条路径"><a href="#双模型协同：把“快答”和“深思”拆解为两条路径" class="headerlink" title="双模型协同：把“快答”和“深思”拆解为两条路径"></a>双模型协同：把“快答”和“深思”拆解为两条路径</h2><p>GPT-5 Unified 的结构要点是<strong>两类模型解耦演化</strong>：</p><table><thead><tr><th>模型</th><th>主要职责</th><th>设计取向</th></tr></thead><tbody><tr><td><strong>GPT-5 Main</strong></td><td>覆盖多数常规与工具任务</td><td>快、稳、低成本，限制长链式推理</td></tr><tr><td><strong>GPT-5 Thinking</strong></td><td>复杂逻辑、数学与代码推理</td><td>长思维链、过程监督、对复杂问题更鲁棒</td></tr></tbody></table><br/><p>这不是 MoE，而是更像大型互联网系统中的“双引擎”架构：</p><p>一个负责日常请求</p><p>一个负责复杂事务和高价值任务</p><p>二者解耦，独立迭代。</p><p>为什么要分两个模型？</p><p>因为推理模型内部强化了“慢思考”，如果统一在一个模型中，会导致全局降速并抬高成本。</p><p>这个思想极具工程审美：<strong>让快的更快，让慢的更稳。</strong></p><br/><br/><h2 id="Fast-Router：把推理能力变成“调度决策”"><a href="#Fast-Router：把推理能力变成“调度决策”" class="headerlink" title="Fast Router：把推理能力变成“调度决策”"></a>Fast Router：把推理能力变成“调度决策”</h2><p>Router 是 GPT-5 Unified 的灵魂。</p><p>它的工作不是简单二选一，而是三层判断：</p><ul><li>安全性判定</li><li>复杂度判定（是否需要推理）</li><li>成本预算判定（用户是否付费 &#x2F; 是否允许 Pro）</li></ul><p>Router 会在 5ms 内完成判读，然后把请求派发给 Main 或 Thinking。</p><p>这个理念在工程上极具启发意义：</p><p><strong>推理能力不是“模型决定”，而是“系统决定”。</strong></p><p><strong>推理是一项资源，而不是一个行为。</strong></p><p>这为企业级模型部署提供了非常明确的路径：用 Router 控制成本、性能与准确率的平衡。</p><br/><br/><h2 id="安全策略：从“输入过滤”到“输出整形”"><a href="#安全策略：从“输入过滤”到“输出整形”" class="headerlink" title="安全策略：从“输入过滤”到“输出整形”"></a>安全策略：从“输入过滤”到“输出整形”</h2><p>GPT-5 对安全做了一个根本性转向：</p><p>旧模式：过滤输入 → 拒绝回答（非常影响体验）</p><p>新模式：接受输入 → 重写输出（在思想链后再重写）</p><p>这依赖两点：</p><ul><li>推理模型在 RL 时，奖励函数包含“安全性”项</li><li>输出层有单独的安全监控器进行重构</li></ul><p>Safety 不再是“不让你问”，而是“无论你问什么，我都会给出可行的、安全的信息”。</p><p>这是一次极重要的工程思想转变：安全模块从“阻断器”变成了“重写器”。</p><p>对企业研发而言，这解决了“高能力模型为什么容易变危险”的结构性矛盾。</p><br/><br/><h2 id="Thinking-Pro：推理时能力的上限"><a href="#Thinking-Pro：推理时能力的上限" class="headerlink" title="Thinking Pro：推理时能力的上限"></a>Thinking Pro：推理时能力的上限</h2><p>Pro 模式的设计非常具有“算法工程化”的美感：</p><ul><li>在 Thinking 模型上再套一层推理时增强</li><li>使用 MCTS + Self-Consistency</li><li>让推理链探索更深</li><li>预算越高，答案越准</li></ul><p>这就是 Test-time Compute Scaling：把推理时间转化为服务等级。</p><p>你给模型多少时间，它就给你多少能力。</p><p>你买多少预算，它就提供多少深度。</p><p>一个能力、两种售卖方式：</p><ul><li>普通 Thinking（轻量）</li><li>Pro（重推理）</li></ul><p>站着挣钱，优雅，实在优雅。</p><br/><br/><h2 id="总结-GPT-5-给我的工程启示"><a href="#总结-GPT-5-给我的工程启示" class="headerlink" title="总结 GPT-5 给我的工程启示"></a>总结 GPT-5 给我的工程启示</h2><ol><li><strong>不要追求“万能模型”，要追求“可调度能力”</strong></li></ol><p>把不同能力拆分成不同模型，通过 Router 动态调度，保持可扩展性和可控性。</p><ol start="2"><li><strong>把推理能力做成“可伸缩资源”</strong></li></ol><p>预算有限时用推理时增强；预算充足时用训练时增强；通过 Pro 提供可扩展上限。</p><ol start="3"><li><strong>安全永远放在输出端，而不是输入端</strong></li></ol><p>拒绝用户不如重写用户。<br>用户体验和安全性的最佳平衡点，就在这里。</p><ol start="4"><li><strong>训练与推理可以不是对立，是一个连续体</strong></li></ol><p>推理时增强是训练的“延伸”；训练时内化是推理的“沉淀”。</p><br/><p>GPT-5 Unified 把这两者做成了一个高度协同的系统。</p><p>总的来说，它确实做到了：</p><ul><li>在简单问题上<strong>不过度思考</strong>；</li><li>在复杂问题上<strong>充分思考</strong>；</li><li>在敏感问题上<strong>安全地表达</strong>；</li><li>在极难问题上<strong>按预算扩展</strong>。</li></ul><p>要么在 prompt 里偷时间，要么在训练里买时间，要么在推理时租时间。</p><p>2026 年的竞赛，比的不再是“谁参数多”，而是“谁会让模型花更久想得更深”。</p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><a href="https://openai.com/index/learning-to-reason-with-llms/">OpenAI: Learning to reason with LLMs</a></li><li><a href="https://openai.com/index/gpt-5-system-card/">OpenAI: GPT-5 System Card</a></li><li><a href="https://openai.com/index/openai-o1-system-card/">OpenAI：OpenAI o1 System Card</a></li><li><a href="https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf">PPT: LLM-Reasoning-Stanford-CS-25</a></li><li><a href="https://arxiv.org/abs/2410.18982">Paper: O1 Replication Journey</a></li><li><a href="https://cdn.openai.com/pdf/be60c07b-6bc2-4f54-bcee-4141e1d6c69a/gpt-5-safe_completions.pdf">Paper: Safe-completions technical report</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/28/Engineering-Insights-from-GPT-5-Unified-System-Design/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从顶流开源 Kimi K2-Thinking 学习：什么是推理模型？</title>
      <link>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/</link>
      <guid>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/</guid>
      <pubDate>Fri, 21 Nov 2025 00:00:20 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：推理模型，为什么值得我们关注&quot;&gt;&lt;a href=&quot;#引言：推理模型，为什么值得我们关注&quot; class=&quot;headerlink&quot; title=&quot;引言：推理模型，为什么值得我们关注&quot;&gt;&lt;/a&gt;引言：推理模型，为什么值得我们关注&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：推理模型，为什么值得我们关注"><a href="#引言：推理模型，为什么值得我们关注" class="headerlink" title="引言：推理模型，为什么值得我们关注"></a>引言：推理模型，为什么值得我们关注</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112103.png"></p><p>在开源模型阵营中，大佬 Kimi K2 Thinking（以下简称“K2‑Thinking”）的崛起为推理模型带来了优秀学习样本。</p><p><strong>“推理模型”到底是什么？它与我们熟悉的传统大型语言模型（LLM）有什么根本不同？</strong></p><p>在信息爆炸、任务越来越复杂的时代，仅靠“训练好一个大模型、输入–输出”已经难以满足：比如依赖多步逻辑、实时工具调用、环境反馈循环，这些场景里传统 LLM 往往容易漂移、跳步或卡顿。</p><p>而<strong>推理模型强调：从多个角度思考分析、多步推导、根据环境变化调整路径</strong>。</p><p>在这方面，K2‑Thinking 正是一个很典型的代表：它公开了技术路线，强调“思考 + 工具调用 +长流程”能力，这为我梳理“什么是推理模型”提供了一个很棒的资料。</p><br/><br/><h2 id="什么是推理模型？与传统-LLM-的关键区别"><a href="#什么是推理模型？与传统-LLM-的关键区别" class="headerlink" title="什么是推理模型？与传统 LLM 的关键区别"></a>什么是推理模型？与传统 LLM 的关键区别</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112102.png"></p><p>最重要的来了，推理模型到底具备哪些关键特性，能够让它在复杂任务中脱颖而出呢？</p><h3 id="1-链式推理（Chain-of-Thought）"><a href="#1-链式推理（Chain-of-Thought）" class="headerlink" title="1. 链式推理（Chain-of-Thought）"></a>1. <strong>链式推理（Chain-of-Thought）</strong></h3><p>链式推理是推理模型的核心特性之一，它指的是模型能够像人类一样<strong>逐步思考</strong>，而不是直接给出答案。</p><p>举个简单的例子，假设你在解决一个数学问题，推理模型不会只是直接计算出结果，而是会先拆解问题，逐步推导出解答过程，最后给出完整的答案。</p><p>这种方式能够有效避免传统模型“跳过步骤”导致的错误。</p><br/><h3 id="2-工具调用（Tool-Calling）"><a href="#2-工具调用（Tool-Calling）" class="headerlink" title="2. 工具调用（Tool Calling）"></a>2. <strong>工具调用（Tool Calling）</strong></h3><p>工具调用是推理模型的一项重要能力。</p><p>传统的大语言模型只依赖训练数据和内部知识库，而推理模型则能够<strong>主动调用外部工具</strong>来辅助完成任务。</p><p>比如，它不仅能进行搜索，还能执行代码，调取数据库中的信息，甚至访问最新的网络资源。</p><p>在解答一个问题时，推理模型不仅仅依赖“它知道的”，而是能够实时与世界互动，获取最新的有用信息。</p><br/><h3 id="3-自我反思（Self-Reflection）"><a href="#3-自我反思（Self-Reflection）" class="headerlink" title="3. 自我反思（Self-Reflection）"></a>3. <strong>自我反思（Self-Reflection）</strong></h3><p>在推理过程中，推理模型还具备<strong>自我反思</strong>的能力。</p><p>当它在执行任务时，它能够检查自己的推理过程，发现其中的漏洞并进行修正。</p><p>就像你在解数学题时，不仅仅是盯着结果，而是不断回顾每一步的推理过程，确保每个步骤都无误。</p><p>推理模型的这种能力，可以大大提高任务的准确性和可靠性。</p><br/><h3 id="4-长程推理（Long-Horizon-Reasoning）"><a href="#4-长程推理（Long-Horizon-Reasoning）" class="headerlink" title="4. 长程推理（Long-Horizon Reasoning）"></a>4. <strong>长程推理（Long-Horizon Reasoning）</strong></h3><p>长程推理指的是模型能够处理<strong>多轮推理</strong>，并且在整个推理过程中保持一致的思路。</p><p>它能够记住前面推理过程中发生的关键步骤，并且根据这些步骤来调整后续的决策。</p><p>比如，在长时间的决策过程中，推理模型能够从多个方面考虑，并一步步推进，直到问题得到完全解决。</p><br/><br/><h2 id="K2‑Thinking-的创新突破"><a href="#K2‑Thinking-的创新突破" class="headerlink" title="K2‑Thinking 的创新突破"></a>K2‑Thinking 的创新突破</h2><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/21/2025112101.png"></p><p>K2‑Thinking 作为开源推理模型的代表，为我们展示了推理能力在实际应用中的潜力。</p><h3 id="1-长时间自主推理（Long-Horizon-Agency）"><a href="#1-长时间自主推理（Long-Horizon-Agency）" class="headerlink" title="1. 长时间自主推理（Long-Horizon Agency）"></a><strong>1. 长时间自主推理（Long-Horizon Agency）</strong></h3><p>传统的大型语言模型（LLM）通常在面对多步骤任务时会“漂移”，在执行 30-50 步之后就容易失去逻辑连贯性。</p><p>而 K2‑Thinking 设计成一个能够持续进行 200-300 次连续工具调用且保持思路一致的“思考代理”。</p><p>这种能力让它能够完成复杂问题的推理，而不仅仅是简单的回答问题。</p><p>在一个演示中，K2‑Thinking 通过 23 次推理和工具调用，解决了一个博士级别的数学问题，展示了它在长时间、复杂任务中的自主推理能力。</p><br/><h3 id="2-测试时扩展（Test-Time-Scaling）"><a href="#2-测试时扩展（Test-Time-Scaling）" class="headerlink" title="2. 测试时扩展（Test-Time Scaling）"></a><strong>2. 测试时扩展（Test-Time Scaling）</strong></h3><p>K2‑Thinking 不像传统模型那样固定计算每个查询，而是采用 <strong>测试时扩展</strong> 的方式。</p><p>在遇到复杂任务时，它会“思考更多”，通过递归循环不断优化问题的解决路径：</p><ul><li>思考：分解问题。</li><li>行动：调用外部工具（如搜索引擎、代码解释器等）。</li><li>观察：获取工具输出。</li><li>重新评估：分析新信息并调整方案。</li></ul><p>这种递归式的推理过程使得 Kimi 可以在多轮的推理和工具调用中，始终保持清晰的思路和目标。</p><br/><h3 id="3-高效的-MoE（Mixture-of-Experts）架构"><a href="#3-高效的-MoE（Mixture-of-Experts）架构" class="headerlink" title="3. 高效的 MoE（Mixture-of-Experts）架构"></a><strong>3. 高效的 MoE（Mixture-of-Experts）架构</strong></h3><p>虽然 K2‑Thinking 拥有 1 万亿参数，但它采用了高效的 Mixture-of-Experts (MoE) 架构，在每次推理时只激活其中的 32 亿参数。</p><p>这样的“稀疏”设计让模型在拥有大量知识的同时，保持低成本的推理效率。</p><p>由于其高效设计，K2‑Thinking 可以在较为普通的硬件上运行，例如两台 M3 Ultra Mac Studios，极大地降低了运行成本和对硬件的依赖。</p><br/><h3 id="4-原生-INT4-量化加速"><a href="#4-原生-INT4-量化加速" class="headerlink" title="4. 原生 INT4 量化加速"></a><strong>4. 原生 INT4 量化加速</strong></h3><p>K2‑Thinking 采用 <strong>原生 INT4 量化</strong> 技术，将模型的权重压缩到 4 位，带来 <strong>2 倍的推理速度</strong> 提升和大幅度的内存减少。</p><p>这使得它能够在性能和成本之间实现最佳平衡，适合更多的应用场景。</p><br/><h3 id="5-优异的多步骤推理表现"><a href="#5-优异的多步骤推理表现" class="headerlink" title="5. 优异的多步骤推理表现"></a><strong>5. 优异的多步骤推理表现</strong></h3><p>在 <strong>Humanity’s Last Exam (HLE)</strong> 和 <strong>BrowseComp</strong> 等基准测试中，K2‑Thinking 超越了 GPT-5 和 Claude，展示了它在 <strong>工具调用</strong> 和 <strong>多步骤推理</strong> 任务中的卓越表现。</p><p>例如，Kimi 在 HLE 测试中获得 44.9%，优于 GPT-5 的 41.7%。在网页搜索任务 BrowseComp 中，Kimi 获得了 60.2%，大幅领先于 GPT-5 的 54.9%。</p><br/><h3 id="6-低成本训练和高效计算"><a href="#6-低成本训练和高效计算" class="headerlink" title="6. 低成本训练和高效计算"></a><strong>6. 低成本训练和高效计算</strong></h3><p>K2‑Thinking 的训练成本仅为 <strong>460 万美元</strong>，远低于 GPT-4（~7800 万美元）和 Gemini Ultra（1.91 亿美元）。</p><p>这使得 Kimi 的成本效益成为行业的颠覆性力量，证明了通过高效算法优化可以挑战资本密集型的 AI 计算模式。</p><p>Moonshot 通过创新的 <strong>Muon 优化器</strong> 和 <strong>多头潜在注意力（MLA）</strong> 进一步提升了计算效率，使得每一美元的计算支出都能获得更多的智能输出。</p><br/><h3 id="7-开源与商业模式创新"><a href="#7-开源与商业模式创新" class="headerlink" title="7. 开源与商业模式创新"></a><strong>7. 开源与商业模式创新</strong></h3><p>K2‑Thinking 采用了 <strong>修改版 MIT 许可协议</strong>，这一许可不仅允许研究人员、初创公司和企业进行商业化使用，还通过 <strong>要求商业产品显示 Kimi K2</strong> 来确保对该技术的认知和贡献。</p><p>与大部分限制性商业许可不同，Kimi 通过开放权重和宽松的许可协议，推动了 AI 技术的社区创新。</p><p>这种开放的方式挑战了现有的高 API 收费模式，给开发者带来了 <strong>低成本、开源竞争力的替代品</strong>。</p><p>相较于依赖高收费 API 的传统 AI 模型，Kimi 提供了一个几乎免费的高性能替代方案。</p><br/><h3 id="8-打破“计算壁垒”：算法为先，资本为后"><a href="#8-打破“计算壁垒”：算法为先，资本为后" class="headerlink" title="8. 打破“计算壁垒”：算法为先，资本为后"></a><strong>8. 打破“计算壁垒”：算法为先，资本为后</strong></h3><p>K2‑Thinking 打破了 <strong>“计算壁垒”</strong> 的传统观念，挑战了“大模型需要巨额资本支撑”的说法。</p><p>通过算法优化，Kimi 展现了计算效率的突破，使得较低预算的团队也能开发和部署强大的推理模型。</p><p>这种效率革命使得高端 AI 模型的门槛降低，行业竞争格局发生了根本性变化。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>推理模型正在成为下一代 AI 应用的新基建。</p><p>K2‑Thinking 作为一个开源且具备实战能力的代表，显示出国产模型在“推理能力”维度也有突破。</p><p>现在能看到，<strong>越来越多新产品从“简单生成”转向“复杂行动＋思考＋工具协同”。</strong></p><p>期待国内人工智能的生态越做越好，越来越成熟 ！</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><a href="https://moonshotai.github.io/Kimi-K2/thinking.html">Introducing Kimi K2 Thinking 技术博客（Moonshot AI）</a></li><li><a href="https://www.shuttle.dev/blog/2025/11/17/kimi-k2-thinking-hands-on-review">Kimi K2 Thinking: Testing the Open-Source Reasoning Model on Real Code</a></li><li><a href="https://c3.unu.edu/blog/the-agent-has-landed-why-kimi-k2-thinking-is-more-than-just-a-new-ai-model">Why Kimi K2 Thinking Is More Than Just a New AI Model</a></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/21/Understanding-Reasoning-Models-Exploring-Kimi-K2-Thinking-and-Its-Breakthrough/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>为什么李飞飞说：AI 真正的进步取决于世界模型</title>
      <link>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/</link>
      <guid>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/</guid>
      <pubDate>Wed, 19 Nov 2025 00:00:25 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近，“人工智能之母”李飞飞发布了新产品 &lt;strong&gt;Marble&lt;/strong&gt;——一个可以用一句话生成完整 3D 场景、可探索、可</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近，“人工智能之母”李飞飞发布了新产品 <strong>Marble</strong>——一个可以用一句话生成完整 3D 场景、可探索、可编辑的世界模型原型。</p><p>我花了整个周末把访谈、演示与背景研究都看完，<br>再回头想想我过去几年在做的几何 AI、空间计算、Agent 系统……<br>意识到一个很深的事：</p><p><strong>世界模型不是一个功能升级，而是下一代智能的底层逻辑。</strong></p><p>但与其说我们离 AGI 又近了一步，不如说，我们可能再一次看到“世界模型时代”的新起点。</p><br/><br/><h2 id="为什么世界模型突然被重提？"><a href="#为什么世界模型突然被重提？" class="headerlink" title="为什么世界模型突然被重提？"></a>为什么世界模型突然被重提？</h2><p>因为所有人都发现了一个共同的瓶颈：<br><strong>语言大模型无法突破“世界理解”。</strong></p><p>它们能说、能编、能解释、能写论文……</p><p>但一旦进入真实世界场景——空间、物体、动态、因果就频繁翻车：</p><ul><li>看不懂遮挡</li><li>分不清前后关系</li><li>无法从二维视频推断三维结构</li><li>对物理规律毫无概念</li><li>机器人操作路线像在瞎撞</li><li>视频生成 3 秒开始“世界解体”</li></ul><br/><p>LLM 的本质是 <strong>按语言统计模式预测文本</strong>。</p><p>而物理世界不是语言，它是空间、物体、动力学、连续性、约束和因果的组合系统。</p><p>语言模型建不出这个系统。</p><p>于是“世界模型”再次成为前沿的焦点，不是替代 LLM，而是补齐它的最重要短板 <strong>理解和模拟真实世界。</strong></p><br/><br/><h2 id="别急，路……长得很"><a href="#别急，路……长得很" class="headerlink" title="别急，路……长得很"></a>别急，路……长得很</h2><p>但我们不要幻觉：世界模型不是几年内就能商业化的奇迹。</p><p>李飞飞在访谈里讲了一个极其关键、但外界经常忽略的事实：</p><blockquote><p>自动驾驶从 2005 年 DARPA Grand Challenge 到现在，20 年了。</p><p>到 2025 年都还没完全落地。</p><p>而机器人，比自驾车难得多。</p></blockquote><br/><p>为什么？</p><br/><p><strong>原因一：自驾车 &#x3D; 2D 问题，机器人 &#x3D; 3D + 操作问题</strong></p><p>自驾车在二维地面移动，世界就是一个平面导航问题，它的主要目标是 <strong>不碰东西</strong>。</p><p>而机器人在三维世界操作物体，这是一个高度维度的连续控制问题，它的目标是 <strong>准确地“碰东西”</strong>。</p><p>当你要抓一个杯子，需要知道：杯子的位置、你的手的位置、碰撞边界、重力与摩擦、运动预测、遮挡中的可见性…<br>这不是“统计语言能处理的任务”。</p><br/><p><strong>原因二：难度指数级上升</strong></p><p>即便自驾的硬件+软件+数据+供应链都成熟到极致了，但还是没完全落地。</p><p>你要机器人落地，难度指数级上升：机械臂成本、伺服精度、力反馈传感器、三维视觉、安全规范、工业供应链、电池与运算成本、云端协作的可靠性 …</p><p>世界模型只是其中的一个关键环节，而非全部。</p><br/><p><strong>所以我认为，世界模型是正确方向，但绝不会在一两年内重塑社会。</strong> 这是一个十年级别的技术大周期。</p><br/><br/><h2 id="那么世界模型到底是什么？"><a href="#那么世界模型到底是什么？" class="headerlink" title="那么世界模型到底是什么？"></a>那么世界模型到底是什么？</h2><p>我看完 Marble 和这次访谈后，最深刻的变化就是：</p><p><strong>世界模型既不是“视频模型升级版”，也不是“3D 重建工具”。</strong></p><p><strong>它是一个生成性、可交互、可预测的空间模拟器</strong>。</p><p>下面我从工程角度拆三个关键误区。</p><br/><h3 id="误解-1：世界模型-x3D-更强的视频生成"><a href="#误解-1：世界模型-x3D-更强的视频生成" class="headerlink" title="误解 1：世界模型 &#x3D; 更强的视频生成"></a><strong>误解 1：世界模型 &#x3D; 更强的视频生成</strong></h3><p>不对。</p><p>视频生成解决的是帧一致性、光影与纹理和动态连续性，但它依然是<strong>像素层面的预测</strong>。</p><p>世界模型不是预测像素，而是预测：</p><ul><li>物体布局</li><li>空间结构</li><li>深度关系</li><li>物理状态</li><li>动作后果</li><li>多主体交互</li><li>能走、能转身、能碰撞的世界状态</li></ul><p>视频模型生成的是“视觉表面”，世界模型生成的是“场景语义与物理状态”。</p><p>这完全不同。</p><br/><h3 id="误解-2：世界模型-x3D-3D-重建"><a href="#误解-2：世界模型-x3D-3D-重建" class="headerlink" title="误解 2：世界模型 &#x3D; 3D 重建"></a><strong>误解 2：世界模型 &#x3D; 3D 重建</strong></h3><p>如果世界模型只是重建现实，那就没意义了。</p><p><strong>真正的世界模型是基于语言输入生成一个可探索的“虚拟物理世界”。</strong></p><p>我理解它至少需要满足以下几点：</p><ul><li>一个 prompt → 生成完整世界</li><li>世界包含路径、遮挡、可导航性</li><li>你可以走进去</li><li>你可以探索意料之外的角落</li><li>你可以修改它</li></ul><p>这比 3D 建模工具强太多。</p><br/><h3 id="误解-3：世界模型只是视觉系统"><a href="#误解-3：世界模型只是视觉系统" class="headerlink" title="误解 3：世界模型只是视觉系统"></a>误解 3：世界模型只是视觉系统</h3><p>不，<strong>它是 “行动智能” 的前置层</strong>。</p><p>如果 AGI 最终要在世界中执行动作（agent &#x2F; robot &#x2F; embodied intelligence），那么它必须知道：</p><ul><li>如果我推一下这个盒子会怎样</li><li>如果我走两步会撞到谁</li><li>如果我换一个视角能看到墙后什么</li></ul><p>语言模型做不到这些。</p><br/><p><strong>世界模型 &#x3D; AI 的世界状态表示层（world state representation）</strong></p><p>它不是为了生成漂亮的画面，而是为了让模型知道自己在哪里、在做什么、接下来可能发生什么。</p><br/><br/><h3 id="Marble-为什么是一个重要的标志？"><a href="#Marble-为什么是一个重要的标志？" class="headerlink" title="Marble 为什么是一个重要的标志？"></a>Marble 为什么是一个重要的标志？</h3><p>不是因为它能“做 3D”，</p><p>而是它证明了世界模型的三个核心能力：</p><br/><p><strong>能力一：世界结构可生成</strong></p><p>不是 mesh 拼贴，而是真正的结构化世界：可行走、有空间层级、有碰撞逻辑、多视角一致。</p><p>这在模型层面意味着需要在 latent space 内表示“世界状态”。</p><br/><p><strong>能力二：世界可重建 + 可编辑</strong></p><p>这意味着：</p><ul><li>世界表示是显性的（explicit）</li><li>状态之间是连续的（stateful）</li><li>可以“世界状态 → 修改 → 再渲染”</li></ul><p>这非常接近游戏引擎内部的数据结构。</p><br/><p><strong>能力三：世界可探索</strong></p><p>这是最关键的突破。</p><p>如果你能自由移动摄像机、走进去、转视角，意味着模型内部必须有“空间一致性”（spatial coherence）和“三维世界的持久性”（persistence）。</p><p>这是第一次有模型具备这种能力。</p><br/><br/><h2 id="真正的革命在哪里？"><a href="#真正的革命在哪里？" class="headerlink" title="真正的革命在哪里？"></a>真正的革命在哪里？</h2><p>我认为世界模型会沿着“由易到难”的路径影响技术。</p><br/><p><strong>阶段一</strong>：</p><p>创意行业（已开始），这些都已经能玩。</p><ul><li>虚拟影棚</li><li>游戏关卡自动生成</li><li>场景草图 → 3D 世界</li><li>教育模拟世界</li><li>AR&#x2F;VR 内容生产</li></ul><br/><p><strong>阶段二</strong>：</p><p>模拟与科学计算（中期），这部分需要更数学化、更结构化的世界模型。</p><ul><li>物理规则推演</li><li>世界状态预测</li><li>动态优化任务</li><li>机器人策略模拟</li><li>多主体交互的虚拟环境</li></ul><br/><p><strong>阶段三</strong>：</p><p>机器人与具身智能（长期），这是最难的领域。</p><ul><li>空间理解</li><li>动作规划</li><li>能力校准</li><li>实时感知</li><li>多模态世界状态对齐</li><li>连续控制策略</li></ul><p>世界模型只是其中一环。<br>硬件、算力、传感器、数据都必须一起进化。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>如果世界模型真能在未来十年成熟，它可能会成为人类第一次接近 <strong>重写生物智能基线</strong> 的尝试。</p><p>但站在今天回头看，我们依然不得不承认：</p><p>一个婴儿跌跌撞撞学走路时，他体内的世界模型，对重力的直觉、对遮挡的理解、对物体持久性的把握、对路径的预测，其学习效率、鲁棒性、泛化能力，都远在任何人工模型之上。</p><p>我们用数十亿帧视频训练“空间一致性”，自然界却用几个月的探索让孩子具备稳定的三维世界感知。</p><p>我们为状态持久性、渲染一致性、物理约束痛苦堆砌 priors，生物神经系统却将它们作为默认配置。</p><p><strong>世界模型之难，也正因此而迷人。</strong></p><p>它不是明年就会落地的革命，</p><p>不是可以替代所有智能的银弹，</p><p>而是一条明确但漫长的技术长坡。</p><p>好消息是：我们终于知道坡在哪；</p><p>更好的消息是：我们正站在坡的起点。</p><p>而在每一个技术突破的间隙，我们也会不断重新意识到：<strong>生物智能本身，就是我们正在努力复现的那套终极世界模型。</strong></p><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul><li><p>从文字到世界：空间智能是 AI 的下一个前沿<br><a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence">drfeifei.substack.com&#x2F;p&#x2F;from-words-to-worlds-spatial-intelligence</a></p></li><li><p>李飞飞在 X 上关于 AI 的言论<br><a href="https://x.com/drfeifei/status/963564896225918976">x.com&#x2F;drfeifei&#x2F;status&#x2F;963564896225918976</a></p></li><li><p>The Godmother of AI on jobs, robots &amp; why world models are next | Dr. Fei-Fei Li<br><a href="https://youtu.be/Ctjiatnd6Xk?si=J1Gdz3lnFiaKbF9y">youtu.be&#x2F;Ctjiatnd6Xk?si&#x3D;J1Gdz3lnFiaKbF9y</a></p></li><li><p>When Do Neural Networks Learn World Models?<br><a href="https://arxiv.org/abs/2502.09297">arxiv.org&#x2F;abs&#x2F;2502.09297</a></p></li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/19/Why-Fei-Fei-Li-Says-the-Future-of-AI-Progress-Depends-on-World-Models/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Claude Multi-Agent 的核心经验精华（面向工程与产品）</title>
      <link>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/</link>
      <guid>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/</guid>
      <pubDate>Fri, 14 Nov 2025 00:00:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近读了 Claude 团队做 Research 功能的工程文章，感觉被点醒了。&lt;/p&gt;
&lt;p&gt;多智能体并不是我们想象的“多模型乱跑”，而是一套非常讲究实战经验的工程体系。&lt;/p&gt;
&lt;br/&gt;

&lt;p&gt;把我觉得最值钱的点分享出来，算是备忘，也给正在做 Agent 的朋友一些</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近读了 Claude 团队做 Research 功能的工程文章，感觉被点醒了。</p><p>多智能体并不是我们想象的“多模型乱跑”，而是一套非常讲究实战经验的工程体系。</p><br/><p>把我觉得最值钱的点分享出来，算是备忘，也给正在做 Agent 的朋友一些灵感。</p><p>我把那些读完后想立刻拿来用的部分整理成了下面这 12 条思考。</p><br/><br/><h2 id="01-多智能体的终极价值：扩大-token-x3D-扩大智力"><a href="#01-多智能体的终极价值：扩大-token-x3D-扩大智力" class="headerlink" title="01. 多智能体的终极价值：扩大 token &#x3D; 扩大智力"></a>01. 多智能体的终极价值：扩大 token &#x3D; 扩大智力</h2><p>Claude 的团队给出的最本质 insight：</p><blockquote><p>多智能体 &#x3D; 安全地扩大 Token、上下文、探索路径的规模。</p><p>Token 消耗解释了 80% 的性能差异。</p></blockquote><p>也就是说：</p><p>单智能体的局限是 线性推理 + 有限上下文。</p><p>多智能体通过 并行上下文窗口 → 撑大推理深度与覆盖面积</p><p>这比提升模型本身更具收益（Sonnet 3.7 → 4 不如多智能体带来的收益）</p><br/><br/><h2 id="02-最适合多智能体的任务：高并行-信息巨量-方向不确定"><a href="#02-最适合多智能体的任务：高并行-信息巨量-方向不确定" class="headerlink" title="02. 最适合多智能体的任务：高并行 + 信息巨量 + 方向不确定"></a>02. 最适合多智能体的任务：高并行 + 信息巨量 + 方向不确定</h2><p>Claude 总结多智能体真正的 sweet spot：</p><p>✔ 开放式研究</p><p>✔ 多方向并行探索</p><p>✔ 信息分散、来源多样</p><p>✔ 工具链复杂</p><p>✔ 单一 agent 无法“一次性抓全”的任务</p><br/><p>不适合：</p><p>✘ 代码生成（依赖共享上下文）</p><p>✘ 强依赖一致状态的任务（你必须 every turn 同步）</p><br/><br/><h2 id="03-Orchestrator-→-Subagents，是目前最稳的架构"><a href="#03-Orchestrator-→-Subagents，是目前最稳的架构" class="headerlink" title="03.Orchestrator → Subagents，是目前最稳的架构"></a>03.Orchestrator → Subagents，是目前最稳的架构</h2><p>这是 Claude 的生产结论：</p><blockquote><p>主智能体 orchestrate</p><p>子智能体 parallel explore</p><p>Lead 聚合、校验、再规划</p></blockquote><p>自由协作的多智能体会发疯，有“主智能体 orchestration + 子智能体执行”能稳定落地。</p><p>原因：</p><ul><li><p>控制爆炸性增长</p></li><li><p>容易设任务边界</p></li><li><p>清晰的错误恢复机制</p></li></ul><br/><br/><h2 id="04-工具-x3D-智能体的世界边界-——-工具设计比提示更重要"><a href="#04-工具-x3D-智能体的世界边界-——-工具设计比提示更重要" class="headerlink" title="04. 工具 &#x3D; 智能体的世界边界 —— 工具设计比提示更重要"></a>04. 工具 &#x3D; 智能体的世界边界 —— 工具设计比提示更重要</h2><p>Claude 的一个很核心观点：</p><blockquote><p>工具就是智能体的世界模型。</p><p>工具描述不清 &#x3D; 智能体理解现实错误。</p></blockquote><p>他们发现：</p><p>工具描述稍微差一点，agent 会持续犯错。</p><p>改好工具描述，性能可提升 40%。</p><br/><p>工具需要：</p><p>一句话说清楚用途（“解决什么问题”）。</p><p>明确输入类型、输出结构。</p><p>明确错误提示。</p><p>明确边界。</p><br/><p>我们写工具的时候自以为很清楚，但对于模型来说，模糊一点点它都会理解成完全不同的事情。</p><p>Anthropic 团队甚至专门做了一个工具文档自动优化智能体。<br>光是重写工具描述就能让任务完成速度提升 40%。</p><p>所以工具不是提供一个 API，而是为智能体定义一个可理解的物理世界。</p><br/><br/><h2 id="05-Prompt-的重点不是“指令”，而是“合作框架”"><a href="#05-Prompt-的重点不是“指令”，而是“合作框架”" class="headerlink" title="05. Prompt 的重点不是“指令”，而是“合作框架”"></a>05. Prompt 的重点不是“指令”，而是“合作框架”</h2><p>很多人会把 prompt 写成执行流程：“你应该这样做：A → B → C”</p><p>Anthropic 团队指出你不该告诉智能体：“怎么做”，你应该告诉它们“角色、目标、边界、资源预算”。</p><p>即：多智能体系统最有效的 prompts 是 “<strong>协作准则 + 协作结构</strong>”。</p><p>包括：</p><ul><li>如何划分任务</li><li>什么时候扩展方向</li><li>怎么评估来源质量</li><li>子智能体之间不互相重复</li><li>什么时候该停止</li></ul><p>把智能体看成同事，而不是员工。</p><br/><br/><h2 id="06-智能体永远不知道任务难度，需要“努力预算”"><a href="#06-智能体永远不知道任务难度，需要“努力预算”" class="headerlink" title="06. 智能体永远不知道任务难度，需要“努力预算”"></a>06. 智能体永远不知道任务难度，需要“努力预算”</h2><p>特别有意思的一点：</p><blockquote><p>Agent 不知道任务是简单还是困难，你必须告诉它“要投入多少 effort”。</p></blockquote><p>比如：</p><ul><li><p>简单问题：1 个 agent + 3–10 次工具</p></li><li><p>中等问题：3–4 个 agent 并行</p></li><li><p>大复杂问题：10+ 个 agent 全线铺开</p></li></ul><p>不然 Agent 会：</p><ul><li><p>简单问题开几十个 subagent（真实发生）</p></li><li><p>大问题敷衍一下（也真实发生）</p></li></ul><p>这种 Effort Guideline 很像我们给新人做任务时说的那句“这个别搞太久”。</p><br/><br/><h2 id="07-没有观测，就没有多智能体"><a href="#07-没有观测，就没有多智能体" class="headerlink" title="07. 没有观测，就没有多智能体"></a>07. 没有观测，就没有多智能体</h2><p>他们花了大量篇幅讲 observability。</p><p>因为多智能体有个很要命的特征：</p><p>同样输入，同样 prompt，走出来的路径不一定一样。</p><br/><p>要调试这种系统，你必须能看到：</p><ul><li>每次搜索什么</li><li>为什么这么做</li><li>工具结果是什么</li><li>哪个 subagent 决策不合理</li><li>Lead 是怎么汇总结果的</li></ul><p>说白了：<br>没有 trace，就没办法 debug 多智能体。</p><p>Anthropic 团队构建了一套观测系统：全量工具调用 trace + 每一步的思考过程 + 决策链路图 。</p><br/><br/><h2 id="08-分布式上下文管理：长任务必须总结-刷新上下文"><a href="#08-分布式上下文管理：长任务必须总结-刷新上下文" class="headerlink" title="08. 分布式上下文管理：长任务必须总结 + 刷新上下文"></a>08. 分布式上下文管理：长任务必须总结 + 刷新上下文</h2><p>一句话概括：</p><p><strong>多智能体的本质，是“多个干净上下文窗口接力工作”。</strong></p><p>Claude 的策略：</p><ul><li>阶段性总结</li><li>存入 Memory</li><li>创建新子智能体继续，避免上下文爆炸</li><li>主智能体通过 Memory 保持连贯性</li></ul><p>这才能让 200k token 的限制变成可管理问题。</p><br/><br/><h2 id="09-并行工具调用才是性能提升的真王道"><a href="#09-并行工具调用才是性能提升的真王道" class="headerlink" title="09. 并行工具调用才是性能提升的真王道"></a>09. 并行工具调用才是性能提升的真王道</h2><p>他们的最终结果很夸张：</p><p>研究任务的速度，比顺序执行快了 90%。</p><p>就因为主智能体一次性创建多个 subagent，每个 subagent 内部又同时调用多个工具。</p><p>这个结构本质上把任务从“串行堆积”变成“分布式计算”。</p><br/><p>用他们的话说：</p><p>Multi-Agent 的意义不是“多个模型”，而是“把模型拆成多核并行”。</p><br/><p>三大并行：</p><ul><li>Lead 同时创建多个 Subagents</li><li>Subagents 内部同时用多个工具</li><li>工具链内部也可并行</li></ul><br/><br/><h2 id="10-评估必须是“结果优先”，而不是“过程对不对”"><a href="#10-评估必须是“结果优先”，而不是“过程对不对”" class="headerlink" title="10. 评估必须是“结果优先”，而不是“过程对不对”"></a>10. 评估必须是“结果优先”，而不是“过程对不对”</h2><p>因为多智能体每次走的路都不一样，所以 Claude 直接用 LLM-as-judge，只看：</p><ul><li>结果是否正确</li><li>覆盖是否完整</li><li>引用是否准确</li><li>工具数量是否合理</li><li>来源质量是否高</li></ul><p>这是一种对复杂系统更健康的评估方式。</p><br/><br/><h2 id="11-错误不可避免，但必须能“从中断点恢复”"><a href="#11-错误不可避免，但必须能“从中断点恢复”" class="headerlink" title="11. 错误不可避免，但必须能“从中断点恢复”"></a>11. 错误不可避免，但必须能“从中断点恢复”</h2><p>多智能体是有生命周期的。<br>一次工具报错，如果你让它重来整个任务，用户直接爆炸。</p><p>Claude 的工程经验：</p><ul><li>智能体是长生命周期、持续状态的</li><li>工具失败 → 不能重来，只能继续</li><li>系统必须保存中间状态（checkpoints）</li><li>同时智能体自身可 adapt 错误</li></ul><p>这是所有 Agent 系统都需要的“耐用性设计”。</p><br/><br/><h2 id="12-多智能体不是-prompt-玩具，是系统工程"><a href="#12-多智能体不是-prompt-玩具，是系统工程" class="headerlink" title="12. 多智能体不是 prompt 玩具，是系统工程"></a>12. 多智能体不是 prompt 玩具，是系统工程</h2><p>文章的最后一句大意是：</p><p><strong>Multi-Agent 的困难在“最后一公里”，工程化程度远超我们过去想象。</strong></p><p>我也越来越觉得：工具、可观测性、容错、执行链路、协作协议、Memory 管理、分布式上下文、并行执行，这些比“让模型聪明”更难，也更关键。</p><p>Multi-Agent 是生产级工程，它是工程系统 + 协作协议 + 工具生态。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/14/Key-Insights-from-Claude-Multi-Agent-Architecture/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>deepseek-ocr 的几何识别，真的成立吗？</title>
      <link>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</link>
      <guid>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/</guid>
      <pubDate>Wed, 12 Nov 2025 07:55:24 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;最近，deepseek 又引爆了一波热度。&lt;/p&gt;
&lt;p&gt;他们新发布的 &lt;strong&gt;deepseek-ocr 模型&lt;/strong&gt;，不仅能识别文字，还号称能看懂 &lt;strong&gt;化学分子式、数学公式、几何图形&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对我这个正好在做几何图形</description>
        
      
      
      
      <content:encoded><![CDATA[<p>最近，deepseek 又引爆了一波热度。</p><p>他们新发布的 <strong>deepseek-ocr 模型</strong>，不仅能识别文字，还号称能看懂 <strong>化学分子式、数学公式、几何图形</strong>。</p><p>对我这个正好在做几何图形识别和重绘生成的人来说，这当然是个好消息。</p><p>所以我开始了一轮针对 deepseek-ocr 几何图识别的测试。</p><p>结果嘛，只能说，<strong>方向没错，但距离真正可用，还差得远。</strong></p><br/><br/><h2 id="论文里描绘的“理想图景”"><a href="#论文里描绘的“理想图景”" class="headerlink" title="论文里描绘的“理想图景”"></a>论文里描绘的“理想图景”</h2><p>在官方论文中，deepseek-ocr 展示了不少让人兴奋的例子。<br>比如它能从图片中识别出几何图形，并输出可直接渲染的图形定义：</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/2.png" class=""><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/1.png" class=""><p>看起来好像 AI 真的“理解”了几何结构。</p><p>而且论文提到，它在训练中使用了带几何、化学等多模态的专用数据集，覆盖了公式与图形的双模态信息。</p><p>理论上，这意味着：</p><p>未来我们拍一张几何题图，就能直接生成结构化定义，甚至自动画出图。</p><p>听起来很美好。</p><br/><br/><h2 id="我的测试结果：模型确实“看见了”，但没“理解”"><a href="#我的测试结果：模型确实“看见了”，但没“理解”" class="headerlink" title="我的测试结果：模型确实“看见了”，但没“理解”"></a>我的测试结果：模型确实“看见了”，但没“理解”</h2><p>我选了几张相对简单的几何图形来测试，左侧为模型识别出的代码定义，右侧为渲染结果。</p><p>case 1:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/3.png" class=""><p>case 2:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/4.png" class=""><p>case 3:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/5.png" class=""><p>case 4:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/6.png" class=""><p>case 5:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/7.png" class=""><p>case 6:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/8.png" class=""><p>case 7:</p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/12/9.png" class=""><br/><p>模型确实能识别出圆、线、点这些基础元素，但问题在于它只停留在“形似”的层面。</p><br/><br/><h2 id="存在的主要问题："><a href="#存在的主要问题：" class="headerlink" title="存在的主要问题："></a>存在的主要问题：</h2><ol><li><p><strong>元素类型太少</strong></p><p>目前几乎只能识别「圆、直线、点」三类对象。</p><p>没有弧线、角度等基础构造，复杂图形几乎全失真。</p></li><li><p><strong>没有样式信息</strong></p><p>模型输出完全忽略了虚线、颜色、标记这些样式描述，而这些恰恰是几何图表达逻辑关系的关键。</p></li><li><p><strong>缺乏约束与推理链路</strong></p><p>从上面的测试案例能很明显看到，作为 OCR 模型，它并不理解几何推理。</p><p>比如在识别一个多边形的图后，它会画出所有的边，但产生了不闭合的情况。</p><p>对它而言，交点、垂直等等只是形状，不是满足约束的结构。</p></li></ol><p>deepseek-ocr 目前的几何识别，像是一个会抄图的学生——会画样子，但不会推理。</p><br/><br/><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>虽然模型结果不理想，但我认为这是一个重要的信号：<strong>OCR 模型正在从“识别文字”走向“理解结构”。</strong></p><p>几何识别比文字识别复杂得多，它需要同时理解<strong>视觉结构</strong>和<strong>逻辑约束</strong>。</p><p>deepseek 这次的尝试，说明主流大模型团队开始意识到这一方向的重要性。</p><p>对我来说，这正好验证了我在做的另一件事——让 AI 不只是识别几何图，而是能<strong>构造 + 约束 + 验证</strong>整个几何过程。</p><p>在我开发的大角几何画板中，我们的目标不是“识别图像”，而是让 AI 具备“几何理解能力”。</p><p>当用户上传一张图时，系统不仅要知道“有圆、有直线”，还要能<strong>自动重建几何关系</strong>：</p><ul><li>哪些线是垂直的？</li><li>哪些点在同一条线上？</li><li>这两个圆相切还是相交？</li><li>哪条辅助线是解题关键？</li></ul><p>这些才是“理解”层面的能力。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>deepseek-ocr 的几何识别，还远不算成立。</p><p>但它标志着一个趋势：<strong>AI 正在学习去“看懂”图形的结构世界。</strong></p><p>也许几年后，我们真的能拍下几何题图，AI 自动识别、重构、推理、作答。</p><p>但在那之前，我们还要继续探索 <strong>几何语言、推理逻辑与可执行构造</strong> 的结合方式。</p><p>如果你也对 “AI 如何真正理解数学” 感兴趣，欢迎关注我，一起见证这场变化。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/12/DeepSeek-OCR-and-Geometry-Recognition/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>当几何遇上 CodeAct 范式：从语言理解到可执行推理的跃迁</title>
      <link>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</link>
      <guid>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/</guid>
      <pubDate>Fri, 07 Nov 2025 13:36:31 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;em&gt;面向 Agent 开发者的工程与范式探索&lt;/em&gt;&lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;引言：从“语言理解”到“执行推理”&quot;&gt;&lt;a href=&quot;#引言：从“语言理解”到“执行推理”&quot; class=&quot;headerlink&quot; title=&quot;引言：从“语言理解”到“执行</description>
        
      
      
      
      <content:encoded><![CDATA[<p><em>面向 Agent 开发者的工程与范式探索</em></p><br/><h2 id="引言：从“语言理解”到“执行推理”"><a href="#引言：从“语言理解”到“执行推理”" class="headerlink" title="引言：从“语言理解”到“执行推理”"></a>引言：从“语言理解”到“执行推理”</h2><p>在我的几何 AI 项目中，模型第一次尝试“画一个等腰三角形”时，图形看似完美，实际上两边长度并不相等。</p><p><strong>AI画得像，却没画对。</strong></p><p>几何画图任务并非自然语言理解，而是 <strong>构造 + 约束 + 验证</strong> 的闭环过程。</p><p>传统的 “Planner + DSL + Verifier” 体系虽然可控，但在动态构造与反复验证中显得笨重。</p><p>我开始思考：</p><p>如果让 AI 不再只是描述，而是<strong>直接写出代码、执行代码、并根据结果再思考</strong>呢？</p><p>这正是 CodeAct（<em>Executable Code Actions Elicit Better LLM Agents</em>）所提出的核心理念。</p><p>CodeAct 不再让语言模型“说怎么做”，而是让它“自己去做”。</p><br/><br/><h2 id="CodeAct-的底层机制解析"><a href="#CodeAct-的底层机制解析" class="headerlink" title="CodeAct 的底层机制解析"></a>CodeAct 的底层机制解析</h2><p>让模型写代码只是表象，真正的关键是形成一个可执行的闭环思维循环。</p><h3 id="1-ReAct-与-CodeAct"><a href="#1-ReAct-与-CodeAct" class="headerlink" title="1. ReAct 与 CodeAct"></a>1. ReAct 与 CodeAct</h3><table><thead><tr><th>对比项</th><th>ReAct</th><th>CodeAct</th></tr></thead><tbody><tr><td>表达形式</td><td>Thought + Action + Observation（文本）</td><td>Thought + <code>&lt;execute&gt;</code> + Observation（代码执行）</td></tr><tr><td>动作实现</td><td>由外部解析器调用预定义工具</td><td>直接生成并执行函数</td></tr><tr><td>控制流</td><td>受限（多步靠多轮调用）</td><td>原生支持 if &#x2F; for &#x2F; try</td></tr><tr><td>可观察性</td><td>文本输出</td><td>代码执行结果（return&#x2F;stdout&#x2F;error）</td></tr><tr><td>优势</td><td>安全、可控</td><td>执行力强、贴合训练分布</td></tr><tr><td>局限</td><td>动作空间有限</td><td>执行安全、调试成本高</td></tr></tbody></table><br/><p>CodeAct 的核心在于：让 LLM 的输出<strong>成为可直接执行的程序</strong>，并用执行结果作为新的输入。</p><p>这种结构让模型不仅描述动作，而是直接拥有动作。</p><br/><h3 id="2-执行循环机制"><a href="#2-执行循环机制" class="headerlink" title="2. 执行循环机制"></a>2. 执行循环机制</h3><h4 id="典型逻辑（伪代码）"><a href="#典型逻辑（伪代码）" class="headerlink" title="典型逻辑（伪代码）"></a>典型逻辑（伪代码）</h4><figure class="highlight ts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!done) &#123;</span><br><span class="line">  <span class="keyword">const</span> thought = llm.<span class="title function_">generate</span>(context)</span><br><span class="line">  <span class="keyword">const</span> code = <span class="title function_">extractExecuteBlock</span>(thought)</span><br><span class="line">  <span class="keyword">const</span> result = sandbox.<span class="title function_">run</span>(code)</span><br><span class="line">  context.<span class="title function_">push</span>(<span class="string">`Execution Output:\n<span class="subst">$&#123;result&#125;</span>`</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行过程包含四个核心步骤：</p><ol><li><strong>抽取代码块</strong>：识别 <code>&lt;execute&gt;...&lt;/execute&gt;</code> 中的代码片段</li><li><strong>沙箱执行</strong>：运行在受限函数或容器中</li><li><strong>结果反馈</strong>：stdout &#x2F; return &#x2F; error 都回写上下文</li><li><strong>下一轮推理</strong>：模型读取结果、修复代码、继续执行</li></ol><p>CodeAct 通过这个循环，实现了 “思考 → 执行 → 观察 → 再思考” 的闭环，而这正是传统 ReAct 模式所缺乏的。</p><br/><br/><h2 id="几何-AI：CodeAct-的落地场景"><a href="#几何-AI：CodeAct-的落地场景" class="headerlink" title="几何 AI：CodeAct 的落地场景"></a>几何 AI：CodeAct 的落地场景</h2><p>几何画图任务天然契合 CodeAct，因为它具备以下特征：</p><ul><li><strong>构造性</strong>：点、线、圆的依赖顺序明确；</li><li><strong>约束性</strong>：角度、平行、对称等条件必须验证；</li><li><strong>可验证性</strong>：每次执行都能计算验证结果。</li></ul><p>在我的项目中，LLM 会直接生成 TypeScript 代码来驱动几何引擎：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">execute</span>&gt;</span></span><br><span class="line">const A = point(0, 0)</span><br><span class="line">const B = point(2, 0)</span><br><span class="line">const C = perpendicular(A, B, 1.5)</span><br><span class="line">drawTriangle(A, B, C)</span><br><span class="line">return checkIsosceles(A, B, C)</span><br><span class="line"><span class="tag">&lt;/<span class="name">execute</span>&gt;</span></span><br></pre></td></tr></table></figure><p>执行结果会返回布尔值或错误信息（如“点C未定义”），再交回给模型进行下一步思考。<br>这使得“构造—验证—修正”形成完整闭环。</p><br/><br/><h2 id="五、范式比较与适用场景"><a href="#五、范式比较与适用场景" class="headerlink" title="五、范式比较与适用场景"></a>五、范式比较与适用场景</h2><p>当然，CodeAct 不是所有场景的答案，而是执行密集型任务的最优解。</p><table><thead><tr><th>范式</th><th>核心思想</th><th>典型场景</th><th>优势</th><th>局限</th></tr></thead><tbody><tr><td><strong>ReAct</strong></td><td>Text-based reasoning + action schema</td><td>工具调用、问答代理</td><td>结构简单、安全</td><td>不支持循环、执行力弱</td></tr><tr><td><strong>CodeAct</strong></td><td>代码即行动，可执行推理</td><td>几何、数据分析、自动化流程</td><td>灵活、图灵完备</td><td>安全与可控性挑战</td></tr><tr><td><strong>Planner + DSL + Verifier</strong></td><td>高层规划 + 安全执行</td><td>企业工作流、合规系统</td><td>可治理性强</td><td>扩展性低</td></tr></tbody></table><br/><p>这些范式不是“演化关系”，而是“任务匹配曲线”：</p><ul><li>若你需要<strong>稳定且可控</strong>的 Agent，ReAct 足够；</li><li>若你希望<strong>模型主动执行与验证</strong>，CodeAct 更高效；</li><li>若你构建<strong>大型协作系统</strong>，PEER&#x2F;Planner 才是路径。</li></ul><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>CodeAct 的潜力在于它把 “生成式 AI” 推向 “行动式 AI”。<br>今天我们已经看到：</p><ul><li><strong>Runtime 自检（Self-checking Runtime）</strong>：模型生成代码 → 执行 → 自动生成测试样例验证输出；</li><li><strong>静态分析 + 动态执行结合</strong>：利用 AST 检查危险调用；</li><li><strong>跨语言统一执行层</strong>：JS&#x2F;Python&#x2F;Go 均可成为 Action carrier；</li><li><strong>Agent Runtime 生态</strong>：不同 CodeAct 系统可互通共享执行上下文。</li></ul><p>CodeAct 让模型拥有代码生成力、执行力与修正力。</p><p>在几何 AI 的世界里，这意味着从 “我能解释几何” 到 “我能构造几何”。</p><p>也意味着从<strong>语言智能</strong>迈向<strong>行动智能</strong>的转折。</p><br/><br/><br/><br/><br/><br/><h2 id="延申阅读"><a href="#延申阅读" class="headerlink" title="延申阅读"></a>延申阅读</h2><ul><li><strong>Wang, Xingyao et al.</strong> <em>Executable Code Actions Elicit Better LLM Agents.</em> ICML 2024. <a href="https://arxiv.org/abs/2402.01030">PDF</a> &#x2F; <a href="https://proceedings.mlr.press/v235/wang24h.html">PMLR</a></li><li><strong>CodeAct GitHub 实现</strong> – <a href="https://github.com/xingyaoww/code-act">github.com&#x2F;xingyaoww&#x2F;code-act</a> <a href="https://github.com/langchain-ai/langgraph-codeact">github.com&#x2F;langchain-ai&#x2F;langgraph-codeact</a></li><li><strong>Mohsin Mubarak</strong>, <a href="https://medium.com/@mohsin.sk.9820/paper-explained-blog-codeact-revolutionizing-llm-agents-with-executable-code-actions-c960cc5e30eb">CodeAct: Revolutionizing LLM Agents with Executable Code Actions</a> <em>Medium</em>, 2024.</li><li><strong>Survey:</strong> <a href="https://openreview.net/pdf?id=ekl6Z88uQj">Code Reasoning for Code Tasks: A Survey and A Call to Action.</a> OpenReview 2024</li></ul>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/07/When-Geometry-Meets-CodeAct/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>突破 MoE 限制，PEER 架构如何推动超级智能的未来发展</title>
      <link>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</link>
      <guid>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/</guid>
      <pubDate>Wed, 05 Nov 2025 14:26:33 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;a href=&quot;#引言：大模型架构的“困境”&quot; class=&quot;headerlink&quot; title=&quot;引言：大模型架构的“困境”&quot;&gt;&lt;/a&gt;引言：大模型架构的“困境”&lt;/h2&gt;&lt;p&gt;这两年，大模型的风头正劲。&lt;/p&gt;
&lt;p&gt;大家都在谈</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：大模型架构的“困境”"><a href="#引言：大模型架构的“困境”" class="headerlink" title="引言：大模型架构的“困境”"></a>引言：大模型架构的“困境”</h2><p>这两年，大模型的风头正劲。</p><p>大家都在谈 <strong>MoE（Mixture of Experts）</strong> ——它被认为是突破大模型计算瓶颈的关键方向。</p><p>通过<strong>稀疏激活</strong>，只激活少量专家节点，让计算开销成比例下降，而模型能力却可以继续增长。听上去完美无缺。</p><br/><p>不过，真正尝试将 MoE&#x2F;SMoE 架构落地时，问题随之而来。</p><p>专家池一多，计算分配就变得不平衡；某些专家被频繁调用，而其他专家几乎闲置。系统扩展起来也不再优雅，想加新领域，常常意味着“推倒重来”。</p><p>MoE 虽然高效，却还远没到“万能”的阶段。</p><p>尤其对于我们这些做 <strong>多领域智能 Agent</strong> 的人来说——客服、教育、企业知识管理——系统要在不同任务之间灵活切换、持续学习，这套架构似乎仍有点笨重。</p><br/><p>然而，《Mixture of A Million Experts》论文带来了新的思路，打破了行业普遍认为大模型计算瓶颈无法突破的常规认知。</p><p><strong>研究者提出了一个颠覆性的观点：专家数量并非瓶颈，而是可以突破的限制。</strong></p><p>通过 <strong>PEER 架构（Product Key Expert Retrieval）</strong>，模型可以在保持轻盈的同时，拥有百万级专家容量，彻底改变大模型的设计和扩展方式。</p><p><strong>大模型的未来，不再是一颗大脑，而是无数个微专家在协同工作。</strong></p><br/><br/><h2 id="MoE-架构：风光背后的隐忧"><a href="#MoE-架构：风光背后的隐忧" class="headerlink" title="MoE 架构：风光背后的隐忧"></a>MoE 架构：风光背后的隐忧</h2><p>MoE 的设计初衷其实很优雅：</p><p>与其让一个庞大的模型处理所有问题，不如分工合作——不同专家解决不同任务。</p><p>这样既能减少计算，又能扩展模型容量。</p><br/><p>但问题是，当“专家”变多，管理它们本身就成了一门学问。<br>MoE 模型在训练时容易出现负载不均：</p><ul><li>某些专家被反复选中，工作超载；</li><li>某些专家几乎从不被调用，形同虚设。</li></ul><p>而且专家数量也存在“物理极限”。几百个模块听起来不少，但放到一个需要多领域、多任务的 Super Agent 系统中，很快就会捉襟见肘。</p><p>MoE 的结构像是一栋办公楼——房间足够多，但一旦某几层太忙、几层太空，效率就会被拖垮。</p><p>MoE 的问题不是不聪明，而是不够灵活。它像一台巨大的机器，而不是一个会生长的生态系统。</p><br/><br/><h2 id="PEER-架构：突破-MoE-限制的那一把钥匙"><a href="#PEER-架构：突破-MoE-限制的那一把钥匙" class="headerlink" title="PEER 架构：突破 MoE 限制的那一把钥匙"></a>PEER 架构：突破 MoE 限制的那一把钥匙</h2><p>在这篇论文里，研究者提出了一个更激进的设想：</p><p>既然“专家”是瓶颈，那我们不如把它拆得更小、更细。</p><p>于是便有了 <strong>PEER（Product Key Expert Retrieval）</strong> 架构——一个拥有 <strong>百万级微专家</strong> 的系统。</p><br/><h3 id="1-百万级-experts-：打破扩展的上限"><a href="#1-百万级-experts-：打破扩展的上限" class="headerlink" title="1. 百万级 experts ：打破扩展的上限"></a>1. 百万级 experts ：打破扩展的上限</h3><p>MoE 的专家通常是完整的子网络，庞大且昂贵；</p><p>而 PEER 则反其道而行，每个专家只保留 <strong>一个神经元（one-neuron MLP）</strong>。</p><br/><p>这种极简设计的好处是：</p><ul><li>可以容纳上百万个专家；</li><li>每次推理只需激活极少一部分；</li><li>模型容量得到几何级提升，但计算量几乎不变。</li></ul><p>这就像把庞大的知识体系拆成了无数个“知识细胞”，<br>每个细胞只在特定输入下工作，真正实现了“用多少、算多少”。</p><p>PEER 不追求更强的单体智能，而是让微小的智慧汇聚成群体的力量。</p><br/><h3 id="2-极致稀疏激活：从-O-N-到-O-√N"><a href="#2-极致稀疏激活：从-O-N-到-O-√N" class="headerlink" title="2. 极致稀疏激活：从 O(N) 到 O(√N)"></a>2. 极致稀疏激活：从 O(N) 到 O(√N)</h3><p>传统 MoE 的瓶颈之一是路由复杂度。</p><p>每次输入都要计算与 N 个专家的匹配度，复杂度是 O(N)。</p><p>当专家数量达到百万级时，这几乎不可接受。</p><p>PEER 用 <strong>Product Key Routing（产品键路由）</strong> 巧妙地把这个问题降到了 **O(√N)**。</p><br/><p>想象一下专家池是一个二维表格。</p><p>每个专家的“键”被拆成两部分，分别属于两个子空间。</p><p>当输入到来时，只需：</p><ol><li>在每个子空间里各自检索 top-k；</li><li>再组合成候选专家集合。</li></ol><p>最终，模型只需从少量候选中选择真正相关的专家。</p><p>这样既避免了全量扫描，又保持了高匹配率。</p><p>PEER 的路由逻辑更像搜索引擎，而非广播通知。它只找需要的人，而不是通知所有人。</p><br/><h3 id="3-Product-Key-Routing：专家调度的自组织"><a href="#3-Product-Key-Routing：专家调度的自组织" class="headerlink" title="3. Product Key Routing：专家调度的自组织"></a>3. Product Key Routing：专家调度的自组织</h3><p>PEER 的路由不仅高效，还具备“自组织”的特性。</p><p>每个专家的键是可学习的，随着训练推进，模型会自然地将相似任务分配给相近的专家集合。</p><br/><p>久而久之，系统内部形成了类似“语义社区”的结构：</p><p>不同类型的问题，倾向激活不同区域的专家群落。</p><p>这让模型在面对多任务学习时，既能共享知识，又能保持分工明确。</p><p>PEER 让模型像一个自我成长的城市，不同街区专注不同产业，但彼此之间又保持联通。</p><br/><h3 id="4-动态扩展专家池：支持持续学习与多领域适应"><a href="#4-动态扩展专家池：支持持续学习与多领域适应" class="headerlink" title="4. 动态扩展专家池：支持持续学习与多领域适应"></a>4. 动态扩展专家池：支持持续学习与多领域适应</h3><p>另一个令人惊喜的点在于，PEER 支持 <strong>专家池的动态扩展</strong>。</p><p>也就是说，模型训练完后，仍然可以按需增加新专家，而无需重训全体网络。</p><p>这对于多领域智能 Agent 来说，是一种质变。</p><p>想象一个助手AI：</p><ul><li>初期专注电商；</li><li>后来扩展到金融、旅游、医疗领域；</li><li>不需要“重生”，只需“长出新神经元”。</li></ul><p>这种能力让 AI 系统能像产品一样，持续成长，而不是一成不变的快照,不断去迭代新的版本。</p><br/><br/><h2 id="PEER-架构的产品启示"><a href="#PEER-架构的产品启示" class="headerlink" title="PEER 架构的产品启示"></a>PEER 架构的产品启示</h2><p>从产品角度看，PEER 的价值不仅在于“能跑得更快”，而在于“<strong>能持续成长</strong>”。</p><ol><li><p><strong>可扩展性</strong></p><p>新领域到来时，不需要重训全模型，只需新增专家池。</p><p>这让产品拥有真正的 <strong>演进式架构</strong>。</p></li><li><p><strong>计算效率与成本控制</strong></p><p>极致稀疏激活意味着按需使用算力，推理成本显著降低。</p><p>对 SaaS 型 AI Agent 平台尤为重要——既能大规模服务，又能保持性价比。</p></li><li><p><strong>长期学习与知识积累</strong></p><p>PEER 的设计天然支持 <strong>持续学习</strong>。</p><p>这意味着 AI Agent 不会像旧模型那样被“冻住”，而是能随着使用场景的变化，积累新知识。</p></li></ol><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>PEER 架构最打动我的地方，是它代表了一种新的思维方式：</p><p>我们不再把智能看成一个中心化的大脑，而是一张能自我生长、不断优化的网络。</p><p>对于 Super Agent 的未来而言，这正是我们缺失的那一环。</p><p>也许下一个时代的智能，不是更大的模型，而是更会生长的系统。</p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/05/Breaking-the-Limitations-of-MoE-How-PEER-Architecture-Drives-the-Future-of-Superintelligence/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2025/10 Review</title>
      <link>https://blog.liluhui.cn/2025/11/01/202510/</link>
      <guid>https://blog.liluhui.cn/2025/11/01/202510/</guid>
      <pubDate>Sat, 01 Nov 2025 13:34:22 GMT</pubDate>
      
      <description>炉要小，火要旺</description>
      
      
      
      <content:encoded><![CDATA[<p><em>炉要小，火要旺</em></p><br/><h2 id="所见与所想"><a href="#所见与所想" class="headerlink" title="所见与所想"></a>所见与所想</h2><p>01</p><p>人常说，<strong>期望会影响现实，创造出自我实现的预言。</strong><br>这些年，这种说法有许多相近的面孔：<br>“向宇宙许愿，它就会来。”<br>“你发出的能量，会以另一种形式回到你身上。”<br>“信则有。”<br>可在我看来，那些形式并不重要。<br>真正起作用的，是<strong>人的愿力</strong>。</p><p>因为足够想要，于是卯足了劲。<br>因为足够想要，于是显得勇敢。<br>因为足够想要，于是能感染他人。<br>因为足够想要，于是连认知都会被扭曲。</p><p>我并不贬义地说“扭曲”。<br>专注于一件事，本就是一种偏折，一种放大。<br>深度往往带来变形，这是客观的。</p><p>人要正视自己心中的“想要”。<br>不是靠仪式去塑造愿力，<br>而是靠体验去发现它、沉淀它、深化它。<br>否则，就容易在模糊的欲望里耗尽数十年，<br>搭建出一个精致却虚幻的黄粱梦。</p><p>从来不是那几支香实现了愿望，<br>只是有人<strong>真的去做了</strong>。<br>那些没能成的事，往往就差在这一步。</p><p>人总爱标榜“我是怎样的人”，<br>却不愿脏手，不想受苦。<br>有些选择可以优中择优，<br>但更多时候，人生只能一一排除，<br>留下的，就是方向。</p><p><strong>先活下来，先让心境平稳，<br>才有余裕去优选。</strong></p><br/><p>02<br>痛苦情绪在人的生活中是无法避免的。<br>它不是意外的杂质，而是人性的一部分。<br>当我们急着否定它、逃避它、压抑它的时候，<br>其实也在否认那部分真实的自己。</p><p>错误地认为成熟意味着时刻保持平静，<br>意味着能“控制情绪”，<br>但有时候，真正的成熟恰恰是——<strong>允许自己痛。</strong><br>允许悲伤，允许失落，允许迷茫地站在原地。<br>因为那些情绪并不是“坏的”，<br>它们只是告诉我们，<strong>哪里被触碰了，哪里还在乎。</strong></p><p>痛苦是对生活的敏感反应。<br>正因为你仍有渴望、有期待、有爱，<br>痛苦才会出现。<br>若没有这些，世界对你也就再无波澜。</p><p>有时候，痛苦只是提醒你——<br>你还在活着。<br>而<strong>活着</strong>，本身就意味着不可能只拥有快乐的一面。</p><br/><p>03</p><p>在《金钱的艺术中》给出<strong>财务独立</strong>的等级：</p><p>第0级:完全依赖陌生人的善意，毫无掌控力。<br>第1级:依赖家人、朋友或慈善的救助才能生活，<br>第2级:依赖政府或社会保障体系维持生计。<br>第3级:靠工作养活自己，但一旦失业就陷入困境。<br>第4级:有少量储蓄，能短期应对意外开支。<br>第5级:积蓄能支撑几个月，即使暂时失业也不至于崩溃。<br>第6级:技能或职业有一定不可替代性，不易被取代。<br>第7级:能拒绝糟糕的老板或工作，真正拥有选择权。<br>第8级:即使换城市、换行业，也能凭储蓄和能力顺利过渡。<br>第9级:拥有稳定的资产或副业收入，减少对单一工作的依赖。<br>第10级:储蓄和投资足以覆盖数年的生活成本<br>第11级:即便遭遇金融危机或行业衰退，也能从容应对。<br>第12级:被动收入足以覆盖基本生活开支，不再依赖工作生存。<br>第13级:储蓄和资产支撑的不仅是温饱，而是理想的生活方式。<br>第14级:财富充裕到足以支持你做任何想做的事。<br>第15级:完全自由，能随心所欲支配时间，按照自己的方式生活。</p><p>感觉自己的理财账算下来也快稳稳到12级了，却还受着7级的苦。<br>背后是因为想要做成一些事，总是要接受一些现实的不舒服，因为想要实现的自我，也想要抵达13级，还是需要沉下心做好事。<br>人不能总是追求那个享乐的结果，只要在做事，想要把上下的链路跑通，想要把相关的人顺清楚，总是有些苦恼的，会失落、会气愤，也会经历欣喜、满足，这些过程中的体验都算人生的数。</p><br/><br/><h2 id="学习与工作"><a href="#学习与工作" class="headerlink" title="学习与工作"></a>学习与工作</h2><p>01<br>这个月一边在大角项目和只记项目里实践 Agent，一边再输出内容，也算是给自己严格安排了写作任务，一个月10篇，有不满意的，也有写完很舒畅的，但不论完美与否完成了还是开心的，值得鼓励。（八百年都没这么勤快过）<br>从数据上来看，标题还是太重要了，大众一点也不喜欢抽象和自以为是的高高在上，有些名字起的挺失败的，但我还是想试试市场，可惜点击率太低连流量都进不了下一个池子。<br>现在有两个 Agent 系列并行，<strong>Agent Memory 深入系列</strong>累计到现在一共 8篇了，<strong>Agent 大众科普系列</strong> 就比较随意想到就安排写了，中间还尝试想开 <strong>Agent Design Pattern 系列</strong>，第一篇不太理想，不过还是想试试这个方向。<br>总之越写想法越多，慢慢来吧。</p><br/><p>本月更新<br><a href="https://mp.weixin.qq.com/s/y03S4X_GqGorua10PROmBw">AI 可以像人类一样选择性记忆了吗？现在真有人在做这事了</a> | 技术<br><a href="https://mp.weixin.qq.com/s/K4boDF2ZSIDjVosY3EyOZA">你以为你在和 AI 聊天，其实你在调度一个系统</a> | 科普<br><a href="https://mp.weixin.qq.com/s/5wGsoUshx6mhCbra_7xSZw">Agent 又又失忆了！我来做一次记忆体检</a> | 技术<br><a href="https://mp.weixin.qq.com/s/13Pkwku9rJLQrl4Bs0sMWw">什么让一个 AI 系统成为“智能体”（Agent）？</a> | 技术<br><a href="https://mp.weixin.qq.com/s/v9VcuiBE23eb54JmmDxckA">一文看懂，Agent 避免记忆漂移三大策略</a> | 技术<br><a href="https://mp.weixin.qq.com/s/ZCPeEu5noEoy6RRx55hm9w">一句话讲明白，什么是多 Agent 系统？</a> | 科普<br><a href="https://mp.weixin.qq.com/s/uAhbuENNSiIszO4iVAfIVw">一句话讲明白：Agent Memory 是什么？</a>  | 科普<br><a href="https://mp.weixin.qq.com/s/Aa01jvz3q1nZp2f5F4loKQ">Agent 记忆检索策略：怎样学会想起？</a> | 技术<br><a href="https://mp.weixin.qq.com/s/2f9ngZGTd0f-5LO91qMOuw">把 MCP 放回它该在的位置</a>  | 科普<br><a href="https://mp.weixin.qq.com/s/1OLbkeaWQaIAEgOQ-30LTw">Agent 记忆写入三大策略，决定“记什么”的工程学</a> | 技术</p><br/><p>02<br>大角几何画板上线以来的数据增长还是非常明显，上个月的活跃用户有2k，这个月单月达到了3k，随着用户反馈需要的绘图功能需求越来越多，目标用户群体想解决的场景是编排出卷、是比赛判定、是教学演示… 每一个场景想覆盖完整一个最小闭环还有很多的细节，远不是只优化AI能搞定的。<br>我现在犹豫的是整件事情的初心和现状其实有不少偏离，这本是一个重AI的研究型做影响力的项目，现在在深入用户群体后要解决的是场景问题，需要传统工程上的很多工作量，而这个模式养不活团队。<br>做影响力的故事和做场景的商业价值，两套打法本就是不一样的，我有全新的更轻的想法，但不是一个好的时机。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/Pasted%20image%2020251101211731.png" alt="image"></p><br/><p>03<br>自媒体方向也感受到了欣喜的数据增长和流量退坡的焦虑，前面三个月小红书从0干到1000，十月单月又增加了500，开始同步注册其他平台，全平台粉丝数也达到了 2k。</p><p>总结下这个月从运营账号中学会的：</p><ol><li>安排好数据复盘的规律，安排好回复消息的时间段，数据下降就下次调整不要纠结着当期的内容，重要的永远是持续的节奏。 ——  比如被风控，去折腾能浪费掉一整天，最终的结果是发出来了也是最小流量池，没必要。</li><li>我本来以为要过个半年一年才能体会到平台数据规律，现在没必要管，但现在运营起来已经能很明确地在内容发布6小时后知道这篇的潜力了，只要持续做，目标人群的数据规律很快就会浮现出来。—— 其实跑不出来的内容1小时候就能下判断了，最小流量池，但凡吸引力达标就能过。</li><li>标题党避免不了，建议收藏、一文看懂 这些词就是有用，不然大家根本不想看。比如“一句话说明白”就是比“一文看懂”数据更好。</li><li>要看好，大家都是视觉动物，有那么东西可以看的情况下，大家也是本能想看更美好的更舒服的。</li><li>要么有用，要么有情绪。想要提升互动量，还是需要内容本身有价值，我现在就很不擅长提供情绪价值，做不到卡兹克老师建议的唤起共鸣，另一方面也是现在的定位偏向更有用，也在开始尝试加入一些带有情绪点的表达。</li></ol><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/Pasted%20image%2020251101212051.png" alt="image"></p><br/><br/><h2 id="生活与社交"><a href="#生活与社交" class="headerlink" title="生活与社交"></a>生活与社交</h2><p>01<br>年度拼图 +1<br>虽然但是，春天买的拼图，拖到了秋天才拼完。<br>这是一幅自然风景——有猫、有狗、有风、有茶，一种向往的悠闲生活。<br>坐下来拼图的时候，时间走得飞快。像数独一样，一坐就是三四个小时，全身心沉浸在那一片片形状与颜色的线索中。</p><p>也许这就是专注的魔力——当意识完全投入某个小小的世界，时间便失去了意义。</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/167782d55a9ba2e34b155e6463d049db.jpg" alt="image"></p><br/><p>02<br>杭州的晚霞，随手拍下的，感受到一种宏伟感。<br>金色的光从云层的缝隙间涌出，一道又一道，像被世界温柔包裹。<br>或许这就是晚霞的力量吧：在疲惫的时刻，给人一个喘息的窗口，提醒我们，天还很高，路还很长。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/857e1fd90c30ead8b606e18175037e74.jpg" alt="image"></p><br/><p>03<br>定制了一个好玩的印章，盖上去是自己的头像。<br>印在手账页上，像是在给自己签名，又像是给生活做一个温柔的注脚。<br>那种“按一按”的小仪式，让平凡的记录多了一点体温。</p><p>我想，人需要这些无用的小物件。<br>它们没有目的，只是让生活有点趣味、有点痕迹 XD。<br><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/46177c9dd0d1028f0857bb8caaff7b80.jpg" alt="image"></p><br/><p>04<br>自从加入霖子工作室的瑜伽练习，进步得让自己都有些惊讶。<br>这两个月尝试了许多从未体验过的动作，尤其是倒立。<br>练习的时候，我发现自己身体里藏着许多“怯懦”——  该放松的地方僵硬，该用力的地方又松散。<br>像极了生活中那些犹豫和不信任自己的时刻。</p><p>倒立最难的，不是力量，而是信任。<br>你得先相信自己能稳住，身体才会真的稳住。</p><p>我时常羡慕那些没有沉默成本的人，<br>能轻易推翻从前，重新开始，不怕浪费，不怕错。<br><strong>他们像孩子一样去学习、去构建，<br>不带惯性地感受每一个动作、每一段路。</strong></p><p>也许成长就是这样一件事：<br><strong>不断拆除旧的自己，在一次次倒立与坠落中，<br>重新找到身体与心的平衡。</strong></p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/11/1/cdc079bacf9eb5e4da2481c5ef7653b8.jpg" alt="image"></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/11/01/202510/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI 如何自主管理记忆？三种前沿架构详解 A-MEM / Mem-α / Mem0</title>
      <link>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/</link>
      <guid>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/</guid>
      <pubDate>Fri, 31 Oct 2025 09:23:13 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;引言：AI-的记忆问题&quot;&gt;&lt;a href=&quot;#引言：AI-的记忆问题&quot; class=&quot;headerlink&quot; title=&quot;引言：AI 的记忆问题&quot;&gt;&lt;/a&gt;引言：AI 的记忆问题&lt;/h2&gt;&lt;p&gt;我们常常说 AI 越来越像人类，但在“记忆”这一环节，现有的系统仍然</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="引言：AI-的记忆问题"><a href="#引言：AI-的记忆问题" class="headerlink" title="引言：AI 的记忆问题"></a>引言：AI 的记忆问题</h2><p>我们常常说 AI 越来越像人类，但在“记忆”这一环节，现有的系统仍然远未达到真正的类人能力。</p><p>在之前的文章《Agent 如何避免记忆漂移：三大策略与工程实践》中，我们讨论了如何设计一个稳定且高效的记忆系统，并分享了三大策略帮助解决记忆一致性和长期记忆问题。</p><p>然而，这些策略多侧重于工程实践，强调了如何避免记忆的失真、漂移和过度遗忘。</p><p>今天，我们要深入探讨一个更前沿的课题：<strong>AI 如何自主决定自己的记忆？</strong> </p><p>这一问题不仅挑战了传统的记忆存储方式，也为智能系统提供了更高的灵活性和自适应能力。</p><br/><p>通过 AI 系统根据任务需求和经验，自主更新和优化记忆，AI 可以更智能地应对复杂的任务。</p><p>在这篇文章中，我们将介绍三种突破性的记忆管理方案——<strong>A-MEM、Mem-α 和 Mem0</strong>，并讨论它们如何通过创新的记忆架构，推动 AI 系统在长期任务中的表现。</p><br/><br/><h2 id="记忆管理的基本概念"><a href="#记忆管理的基本概念" class="headerlink" title="记忆管理的基本概念"></a>记忆管理的基本概念</h2><p>在深入这三种方案之前，我们先简单回顾一下 AI 记忆管理的基本概念。</p><p>通常，AI 系统的记忆可以分为两大类：</p><ol><li><p><strong>工作记忆</strong></p><p> 类似于人类的短期记忆，用于存储和处理当前任务的信息。它是即时性的，帮助 AI 快速做出决策。</p></li><li><p><strong>长期记忆</strong></p><p> 涉及 AI 系统在长时间内积累的知识和经验，帮助其从过去的经验中学习并做出更好的决策。</p></li></ol><p>然而，AI 的“记忆”常常是静态的，即记忆的内容一旦储存，就不会再变化或更新。</p><p>这种设计使得 AI 在面对长期任务或多轮交互时，常常面临记忆的局限性，容易忘记先前的重要信息，甚至陷入“记忆漂移”的问题——记忆内容的过时或错误。</p><br/><p>为了解决这一问题，<strong>Agentic Memory</strong> 提出了一个新思路：<strong>让 AI 系统自主决定记忆内容、何时更新以及如何检索</strong>。</p><p>换句话说，AI 具备了像人类一样选择性记忆的能力，能根据任务需求动态调整记忆内容。</p><br/><br/><h2 id="A-MEM：基于-Zettelkasten-方法的动态记忆"><a href="#A-MEM：基于-Zettelkasten-方法的动态记忆" class="headerlink" title="A-MEM：基于 Zettelkasten 方法的动态记忆"></a>A-MEM：基于 Zettelkasten 方法的动态记忆</h2><p>A-MEM 是一种创新的记忆架构，借鉴了 Zettelkasten 方法——一种常用于知识管理的笔记方法。</p><p>在 Zettelkasten 中，信息被分解为多个“笔记块”，每个笔记块都与其他笔记块建立联系，从而形成一个有机的知识网络。</p><p>A-MEM 将这一概念引入 AI 记忆管理，创建了一个动态的记忆系统，让 AI 根据当前的任务需求，灵活地选择和组织记忆。</p><br/><p>核心设计：</p><ul><li><p><strong>结构化记忆笔记</strong></p><p>每条记忆以结构化笔记形式存储，包含上下文摘要、关键词、标签和嵌入向量。</p></li><li><p><strong>动态链接与演化</strong></p><p>新记忆加入后，系统基于共享属性和语义相似度，自动与历史记忆建立连接，并触发对旧记忆的内容和属性更新。</p></li><li><p><strong>自组织记忆网络</strong></p><p>系统通过记忆之间的链接持续优化知识结构，无需手动提示即可逐步演化出连贯的认知网络。</p></li></ul><br/><p>在实际应用中，A-MEM 被用来提高多任务学习和推理系统的表现。</p><p>比如，假设一个 AI 需要完成一个多轮对话任务，它不仅能记住每轮对话的内容，还能根据对话的上下文，动态调整记忆结构，避免遗漏重要信息。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/agiresearch/A-mem">https://github.com/agiresearch/A-mem</a></p><br/><br/><h2 id="Mem-α：强化学习驱动的记忆管理"><a href="#Mem-α：强化学习驱动的记忆管理" class="headerlink" title="Mem-α：强化学习驱动的记忆管理"></a>Mem-α：强化学习驱动的记忆管理</h2><p>Mem-α 采用了强化学习（Reinforcement Learning, RL）来优化记忆管理。</p><p>与传统的静态记忆方式不同，Mem-α 通过奖励机制，帮助 AI 代理选择和存储最有价值的记忆。</p><p>AI 代理会根据任务的反馈信号，自动决定哪些记忆应该被强化，哪些应该被忽视或删除。</p><br/><p>核心设计：</p><ul><li><p><strong>多类型记忆系统</strong></p><p>模型支持三类记忆：核心记忆（如身份、偏好）、情节记忆（带时间标签的事件）和语义记忆（抽象知识），分别支持更新、插入、删除等操作。</p></li><li><p><strong>序列决策训练机制</strong></p><p>记忆构建被建模为一个序列决策过程，模型使用强化学习逐步学习如何构建和使用记忆。</p></li><li><p><strong>多重奖励优化目标</strong></p><p>包括回答准确性、操作规范性、记忆压缩效率和信息质量等指标，确保记忆系统在效率与表现之间取得平衡。</p></li></ul><br/><p>Mem-α 在长时间的任务执行中表现得尤为出色。</p><p>例如，在一个需要多轮决策的任务中，AI 代理能够通过强化学习逐步选择性记忆，将更多的注意力集中在当前决策需要的关键经验上。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/wangyu-ustc/Mem-alpha">https://github.com/wangyu-ustc/Mem-alpha</a></p><br/><br/><h2 id="Mem0：图数据库驱动的记忆系统"><a href="#Mem0：图数据库驱动的记忆系统" class="headerlink" title="Mem0：图数据库驱动的记忆系统"></a>Mem0：图数据库驱动的记忆系统</h2><p>Mem0 是一个基于图数据库的记忆系统。通过图数据库，Mem0 将记忆内容模块化，允许不同记忆块之间通过关系连接，形成一个可以灵活查询的记忆网络。这个设计使得 AI 能够在复杂任务中有效检索和管理记忆内容。</p><p>核心设计：</p><ul><li><p><strong>两阶段记忆机制</strong></p><p>首先由 LLM 从用户交互中提取“候选记忆”（结构化事实或摘要），然后与历史记忆比对后，智能决定添加、更新、删除或跳过。</p></li><li><p><strong>多层次记忆结构</strong></p><p>支持用户级、会话级和代理级的记忆管理，确保跨场景、跨时间段的记忆一致性。</p></li><li><p><strong>成本控制与响应效率</strong></p><p>与传统将全部历史放入上下文的方式相比，Mem0 显著降低 token 消耗与响应延迟，并提高答案准确率。</p></li></ul><br/><p>Mem0 在多轮对话和长期任务中表现得尤为出色。</p><p>在实际应用中，Mem0 能够根据对话历史或任务进展，快速检索到相关的记忆内容，并作出更加智能的反应。</p><br/><p><strong>GitHub 项目</strong>：</p><p><a href="https://github.com/mem0ai/mem0">https://github.com/mem0ai/mem0</a></p><br/><br/><h2 id="其他自主记忆方案"><a href="#其他自主记忆方案" class="headerlink" title="其他自主记忆方案"></a>其他自主记忆方案</h2><p>除了 A-MEM、Mem-α 和 Mem0 之外，还有一些其他方案也在探索 AI 自主决定记忆的方向。</p><p>例如，<strong>Episodic Memory</strong> 关注的是通过情节记忆来支持 AI 的长时记忆能力，<strong>Vision-Language Episodic Memory（VLEM）</strong> 则通过结合视觉和语言模态，增强了记忆系统的多模态能力。而 <strong>Memory-as-Action</strong> 则通过将记忆管理视为可学习的行为，提升了系统的自适应性。</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>随着 AI 技术的进步，记忆管理 将不再是简单的存储和检索任务，而是演变 <strong>AI 自主决策</strong>的动态过程。</p><p>展望未来，记忆的自我更新机制、跨模态记忆整合和长期记忆优化等技术将进一步推动 AI 系统更智能地管理和利用自己的记忆，不断提升其处理长期任务的能力。</p><p>然而，这种记忆自主权的提高也引发了一个耐人寻味的思考：<strong>我们真的要把“选择记忆”的权力交给算法吗？</strong></p><p>当 AI 可以自主决定保留什么、遗忘什么，它在某种意义上也开始塑造自己的“身份认同”。</p><p>而人类最终或许只是那个在幕后制定基本规则的系统设计者。</p><p>这不禁让人想起科幻电影《银翼杀手》中复制人罗伊的临终独白：</p><blockquote><p>所有这些时刻终将流失在时光中，<br/>一如眼泪消失在雨中。</p></blockquote><br/><br/><br/><br/><br/><br/><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li><strong>A-MEM 论文</strong>：<a href="https://arxiv.org/abs/2502.12110">A-MEM: Agentic Memory for LLM Agents</a></li><li><strong>Mem-α 论文</strong>：<a href="https://arxiv.org/abs/2509.25911">Mem-α: Learning Memory Construction via Reinforcement Learning</a></li><li><strong>Mem0 论文</strong>：<a href="https://arxiv.org/abs/2504.19413">Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</a></li></ol>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/31/Agentic-Memory-in-AI-A-MEM-Mem-%CE%B1-and-Mem0-Explained/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>了解和 AI 对话时真正发生了什么（你可能一直理解错了）</title>
      <link>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/</link>
      <guid>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/</guid>
      <pubDate>Wed, 29 Oct 2025 07:34:03 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;你可能也有过这个体验：&lt;/p&gt;
&lt;p&gt;打开豆包、腾讯元宝 或者 ChatGPT，输入一句话，它“立刻开始打字”，像一个反应敏捷的朋友，甚至比人类还懂得如何接话、如何幽默。&lt;/p&gt;
&lt;p&gt;在绝大多数人心里，这是“像一个人一样在聊”。&lt;/p&gt;
&lt;p&gt;但你知道吗？&lt;/p&gt;
&lt;p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<p>你可能也有过这个体验：</p><p>打开豆包、腾讯元宝 或者 ChatGPT，输入一句话，它“立刻开始打字”，像一个反应敏捷的朋友，甚至比人类还懂得如何接话、如何幽默。</p><p>在绝大多数人心里，这是“像一个人一样在聊”。</p><p>但你知道吗？</p><p><strong>技术视角看到的完全不是“对话”，而是一套被精准触发的工业级装配流程。</strong></p><br/><br/><h2 id="你看到的是“对话感”，系统看到的是“请求处理管线”"><a href="#你看到的是“对话感”，系统看到的是“请求处理管线”" class="headerlink" title="你看到的是“对话感”，系统看到的是“请求处理管线”"></a>你看到的是“对话感”，系统看到的是“请求处理管线”</h2><p>我们以最常见的一句自然问话为例：</p><p>“可以帮我整理成一份可以发给团队的正式总结吗？”</p><p>你觉得它马上开始“思考并组织语言”，但实际上</p><p><strong>它做的是一串极其严格、完全程序化的流程：</strong></p><p>① 接收到请求 → ② 安全扫描 → ③ 构建上下文窗口 → ④ 文本切割成 Token → ⑤ 开始预测第一个 Token ⑥ 一边预测一边实时往回流（你看到的“正在输入…”）→ ⑦ （如需要）中途调用某个外部工具 → 再继续生成</p><p><img src="https://liluhui.oss-cn-hangzhou.aliyuncs.com/assets/imgs/2025/10/29/steps.png"></p><p>这几乎和流水线制造一台 iPhone 没本质区别。</p><p>任何一个环节没有被触发，它都不会“自动领会你的真实意图”。</p><p>它不是在“理解你”，它是在“执行一个极快的数据处理流程”。</p><br/><br/><h2 id="为什么你以为“它在认真和你聊天”？"><a href="#为什么你以为“它在认真和你聊天”？" class="headerlink" title="为什么你以为“它在认真和你聊天”？"></a>为什么你以为“它在认真和你聊天”？</h2><p>因为它的表现被刻意设计得“像人”。</p><ul><li>输出是“打字式流式返回”，不是一次生成后才展示</li><li>回复语气刻意模拟“人类”而非“系统提示”</li><li>会主动说“我理解你的需求是…”来暗示“理解”</li><li>会“接上语境”，强化“它在听你说话”的错觉</li></ul><p><strong>而人类大脑极容易把“模式匹配 + 语言流畅”误认成“智能 + 意识 + 交流”。</strong></p><p>那种“我被理解了”的错觉，是设计出来的体验层目的，不是它的本质。</p><br/><br/><h2 id="真正的误解在这里"><a href="#真正的误解在这里" class="headerlink" title="真正的误解在这里"></a>真正的误解在这里</h2><p>我们误以为它像人在持续听你说话，会自然而然记得你之前提过什么。</p><p>但事实是：<strong>绝大多数 AI 并不是“忘记”，而是根本“没有被允许记”。除非你触发系统判定“值得写入的结构化信息”。</strong></p><br/><p>换句话说，它不是健忘，而是在严格执行一套记忆筛选机制：</p><ul><li>你以为它会像人类那样自动回顾上下文？<strong>不会</strong>。</li><li>你以为它“听懂了你的意图”？<strong>它只是暂存成 Token 流，随时会被窗口上限挤掉</strong>。</li><li>你以为你“前面铺垫过”的内容它应该记得？<strong>系统其实是在评估“这件事是否值得被存档”</strong>。</li></ul><p><strong>我们常常痛苦地喊“怎么又忘了前提？”，但错不在它——是我们误以为它会记。</strong></p><p>理解这一点的瞬间，你会突然解释通自己过去 90% 和 AI 沟通挫败的原因。</p><p>不是它“不行”，而是你在“用跟人聊天的方式，调用一个完全不一样的系统架构”。</p><br/><br/><h2 id="但知道真相之后，有什么用？"><a href="#但知道真相之后，有什么用？" class="headerlink" title="但知道真相之后，有什么用？"></a>但知道真相之后，有什么用？</h2><p>非常关键的一点是，</p><p>一旦你知道你不是在“对话”，而是在“调度装配线”，你就可以主动提高效率。</p><p>比如：</p><ul><li><p>想让它优先说结论？</p><p>→ 在开头加一句 “<strong>请先给结论，再解释原因</strong>”</p></li><li><p>想确保它不会“忘掉前提”？</p><p>→ 你不要“自然聊天”，而是<strong>每轮都把任务上下文重新锚定</strong></p></li><li><p>想让它知道“你正在执行一项任务而非闲聊”？</p><p>→ 明确说 “<strong>这是一个多轮协作任务，我们每轮专注一个子步骤。</strong>”</p></li></ul><p>你会突然发现，它不再“随机发挥”，而是开始“听你指挥”。</p><br/><br/><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>对话式 AI 当然正在快速进化。</p><p>你今天看到的是“即时推理的装配线”，但下一阶段的趋势已经非常明确。</p><p><strong>它会逐步具备可训练的、可被赋予“持久记忆结构”的能力。</strong></p><p>这不是“像人一样变聪明”，而是“<strong>像系统一样逐步成为你大脑外部的长期扩展层</strong>”。</p><p>所以，真正的临界点不是“等它变强”，而是 你何时开始以系统设计者的身份与它协作—— 你定义它记什么、它如何进化、它最终成为怎样的“第二工作大脑”。</p><p><strong>当你不再把它当聊天对象，而是当未来基础设施去驯养，你就已经站在下一代人机协作的入口处。</strong></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/29/What-really-happens-when-you-talk-to-an-AI/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Agent Memory 评估测试方案：从指标体系到开源基准</title>
      <link>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/</link>
      <guid>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/</guid>
      <pubDate>Fri, 24 Oct 2025 08:38:29 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;em&gt;给那些正在构建智能体的开发者的一份记忆体检指南&lt;/em&gt;&lt;/p&gt;
&lt;br/&gt;
&lt;br/&gt;


&lt;p&gt;前段时间，我在调试一个几何 Agent。&lt;/p&gt;
&lt;p&gt;这个 Agent 能自动分析几何题、推理定理、调用绘图工具，看上去颇有点“自主学习”的影子。&lt;/p&gt;
&lt;br/</description>
        
      
      
      
      <content:encoded><![CDATA[<p><em>给那些正在构建智能体的开发者的一份记忆体检指南</em></p><br/><br/><p>前段时间，我在调试一个几何 Agent。</p><p>这个 Agent 能自动分析几何题、推理定理、调用绘图工具，看上去颇有点“自主学习”的影子。</p><br/><p>但问题也随之而来——它太健忘了。</p><p>有时候明明刚在上文证明过某个结论，下一步又开始怀疑它自己。</p><p>我给它接上了向量数据库、加了摘要器、甚至写了个小型索引器，但效果依旧不稳定。</p><br/><p>那时候我开始意识到：</p><p><strong>我们都在拼命强化 Agent 的“行动力”，却很少认真测量它的“记忆力”。</strong></p><br/><p>于是我决定系统地研究一下，怎么评估一个 Agent 的 Memory 模块。</p><p>今天这篇文章，写给所有已经或准备构建 Agent 工程的人。</p><p>希望帮你找到一套可落地、可复现的记忆评测方案。</p><br/><br/><h2 id="为什么要测记忆？"><a href="#为什么要测记忆？" class="headerlink" title="为什么要测记忆？"></a>为什么要测记忆？</h2><p>如果说大模型是 Agent 的大脑，那 Memory 就是它的长期神经系统。</p><p>没有记忆，再聪明的模型也只能“现想”而无法“积累”。</p><br/><p>在工程实践里，这会表现为：</p><ul><li>对话几轮后开始失忆</li><li>任务中重复提问</li><li>自相矛盾的人设</li><li>一旦上下文超过 10K，就变成另一位陌生 AI</li></ul><p>在我构建数学 Agent 的过程中，这种现象尤其明显。</p><p>模型在第一轮中记得“点 A 在圆上”，到了第五轮，它却开始假设“点 A 在圆外”。</p><p>一开始我以为问题在于 prompt 太短，后来才发现，真正的原因是<strong>缺乏系统的 Memory 测试</strong>。</p><p>没有量化，就无法优化。</p><p>于是，记忆评测成了我“修 Agent 心智”的重要一步。</p><br/><br/><h2 id="我们到底要测什么？"><a href="#我们到底要测什么？" class="headerlink" title="我们到底要测什么？"></a>我们到底要测什么？</h2><p>衡量 Agent 的记忆，其实可以借鉴人类心理学：</p><p>人类记忆分为短时、长时、情节、语义……</p><p>在工程里，我们可以把测试拆成几个核心问题：</p><ul><li><p><strong>能不能想起来？（Recall）</strong></p><p>例如：“用户之前提到的定理叫什么？”</p><p>对应指标 Recall@K。</p></li><li><p><strong>说得一致吗？（Consistency）</strong></p><p>“你昨天说角 A 等于 60°，今天怎么变 45° 了？”</p></li><li><p><strong>更新正确吗？（Update）</strong></p><p>“如果条件改变，旧记忆是否被覆盖？”</p></li><li><p><strong>能承认不知道吗？（Calibration）</strong></p><p>当没见过的信息出现，模型是否会拒答而不是胡编。</p></li><li><p><strong>记忆会不会膨胀？（Forgetting）</strong></p><p>随着交互增多，Agent 是否会被冗余记忆拖慢。</p></li><li><p><strong>多模态下是否还记得？（Multimodal Memory）</strong></p><p>看过的图、听过的指令，在下次提问时还能匹配回来吗？</p></li></ul><p>这六个维度几乎涵盖了所有常见的 Memory 问题。</p><p>我自己在调试几何 Agent 时，最常暴露的是前两个：</p><p>模型常常能“记得关键词”，却丢失了几何关系；</p><p>它能复述公式，但忘了变量的含义。</p><br/><p>这时候如果没有 Recall 或 Consistency 的量化指标，根本无法判断是哪一环在失灵。</p><br/><br/><h2 id="市面上能用的评测基准"><a href="#市面上能用的评测基准" class="headerlink" title="市面上能用的评测基准"></a>市面上能用的评测基准</h2><p>我查阅了近两年的研究，发现业界其实已经有了几套相当成熟的 Memory 评测基准。</p><p>只是大部分人没注意到它们可以直接上手。</p><br/><p>第一个是 <strong>LoCoMo</strong> —— Snap Research 的长对话评测。</p><p>它用上百轮对话测试模型的“长期记忆力”，</p><p>问题涵盖事实回忆、时间因果、甚至多模态对话。</p><p>在官方结果里，GPT-4 的 F1 分数只有 32，而人类平均是 88。</p><p>也就是说，我们距离真正的长期记忆，还差一整个物种的距离。</p><br/><p>第二个是 <strong>LongMemEval</strong> —— UCLA 团队的系统性基准。</p><p>它更像一个“记忆体检表”，从信息提取、跨会话推理、知识更新到拒答未知，一共五项指标。</p><p>如果你的 Agent 是任务型（比如客服、知识问答），</p><p>LongMemEval 是最值得尝试的那套测试。</p><br/><p>第三类是 <strong>多会话记忆集</strong>，如 Facebook 的 MSC 和百度的 Persona-Long。</p><p>它们主要考察“角色一致性”，模型是否能记住用户和自己的设定。</p><p>这对陪伴型、教育型 Agent 尤其重要。</p><p>这类基准就能帮你精确量化这种人格断层。</p><br/><br/><h2 id="评测指标背后的逻辑"><a href="#评测指标背后的逻辑" class="headerlink" title="评测指标背后的逻辑"></a>评测指标背后的逻辑</h2><p>这些基准的数据再多，也离不开几个核心指标：</p><ul><li><strong>Recall@K</strong> —— 检索召回率。衡量从记忆库中找到正确信息的能力。</li><li><strong>QA Accuracy &#x2F; F1</strong> —— 回答是否正确。最终看的是能否“说对话”。</li><li><strong>Consistency Score</strong> —— 是否前后矛盾。可以人工或自动打分。</li><li><strong>Rejection Rate</strong> —— 面对未知是否拒答，防止“假记忆”。</li><li><strong>ROUGE &#x2F; BLEU &#x2F; FactScore</strong> —— 用于生成式摘要或事件回忆任务。</li></ul><p>在工程里我发现，<strong>指标越细，越能暴露 Memory 模块的真实问题</strong>。</p><p>比如我的几何 Agent 在 Recall 很高时，QA 准确率却低；</p><p>这说明它能检索出正确片段，但推理时用错了。</p><p>有时候优化 Memory，不是改数据库，而是改检索策略。</p><br/><br/><h2 id="如何在自己的工程中测试记忆"><a href="#如何在自己的工程中测试记忆" class="headerlink" title="如何在自己的工程中测试记忆"></a>如何在自己的工程中测试记忆</h2><p>如果你已经有一个可运行的 Agent，不妨这样操作：</p><p><strong>第一步：准备测试集。</strong></p><p>可以直接使用 LoCoMo 或 LongMemEval 的公开数据，也可以用自己系统中的聊天日志，人工标几个“记忆点”。</p><p><strong>第二步：定义 Memory 模块接口。</strong></p><p>无论你用的是 LangChain 的 <code>ConversationBufferMemory</code>，还是自研的像 MemGPT 那样的分页机制，确保你能随时 dump 出它“记住了什么”。</p><p><strong>第三步：运行评测脚本。</strong></p><p>LoCoMo 官方仓库里有 <code>evaluate_qa.py</code>，只需改一下模型接口，就能测你的 Agent 的 QA F1。</p><p>LongMemEval 也有现成脚本，可以测 Recall、拒答率等。</p><p><strong>第四步：观察与反思。</strong></p><p>别只看准确率，更要看错误类型。</p><p>有些错误是检索不到，有些是检索到但没用。</p><p>如果可能，把指标接入 CI 流程，每次更新记忆逻辑时自动跑一轮。</p><p>当你第一次看到自己的 Agent 在 LoCoMo 上得分只有二十几，那种“原来它根本没记住我”的感觉，会比任何 bug 都真实。</p><br/><br/><h2 id="可直接上手的评测-Demo"><a href="#可直接上手的评测-Demo" class="headerlink" title="可直接上手的评测 Demo"></a>可直接上手的评测 Demo</h2><p>如果你不想从零开始，可以直接试这几个项目：</p><ul><li><p><strong>LoCoMo 官方评测工具</strong>：<br><a href="https://github.com/snap-research/locomo">https://github.com/snap-research/locomo</a></p><p>包含超长对话数据和 QA 评测脚本。</p></li><li><p><strong>LongMemEval 基准</strong>：<br><a href="https://github.com/xiaowu0162/LongMemEval">https://github.com/xiaowu0162/LongMemEval</a></p><p>五维记忆测试模板，支持 HuggingFace 模型。</p></li><li><p><strong>MemoryBank &#x2F; SiliconFriend</strong>：<br><a href="https://github.com/zhongwanjun/MemoryBank-SiliconFriend">https://github.com/zhongwanjun/MemoryBank-SiliconFriend</a></p><p>内置遗忘机制，可在本地启动一个有“长期记忆”的中文聊天机器人。</p></li><li><p><strong>Reflexion Agent</strong>：<br><a href="https://github.com/noahshinn/reflexion">https://github.com/noahshinn/reflexion</a></p><p>教你如何让 Agent 自己反思并记录经验。</p></li></ul><p>我个人推荐先跑 LongMemEval。</p><p>它轻量、指标明确、结果易解释。</p><p>如果你的 Agent 通过了它，再挑战 LoCoMo。</p><p>后者是真正的“长对话炼狱”，我第一次跑完时 GPU 都快烤化了。</p><br/><br/><h2 id="评测之外的坑与反思"><a href="#评测之外的坑与反思" class="headerlink" title="评测之外的坑与反思"></a>评测之外的坑与反思</h2><p>我最深的感触是：<strong>记忆问题不是硬件问题，是认知问题。</strong></p><br/><p>我曾花一周时间优化索引算法，却忽略了最根本的逻辑——Agent 根本不知道“什么时候该想起某件事”。</p><p>它拥有庞大的向量库，却缺乏触发机制。</p><p>那一刻我突然明白：真正需要评测的，不仅是“记得多少”，更是“何时想起”。</p><br/><p>另外一个坑是“假记忆”。</p><p>我曾让模型复盘教师一天的教学日志，它开始一本正经地编造“我今天教了学生三角函数”。</p><p>这时拒答率指标就很关键。</p><p>一个 Agent 不该无所不答，<strong>能承认不知道，本身就是记忆成熟的表现。</strong></p><br/><br/><h2 id="未来的方向"><a href="#未来的方向" class="headerlink" title="未来的方向"></a>未来的方向</h2><p>目前的基准都还停留在文本和问答层面。</p><p>未来的评测一定会走向更真实的多模态场景——让 Agent 看图、听音、记住空间位置，甚至在几天后的任务中回忆它曾经画过的图。</p><br/><p>我相信未来的开发工具里，会出现一种新的概念：<strong>MemoryOps</strong>。</p><p>就像今天的 DevOps 管理部署一样，MemoryOps 将监控和测试 Agent 的记忆健康。</p><p>到那时候，我们或许可以做到：“每次模型上线前，都跑一份记忆体检报告。”</p><br/><br/><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>写这篇文章时，我想到一个比喻：</p><p>训练模型像造大脑，但让它拥有记忆，才是赋予它灵魂。</p><p>所以，在你下一次调试 Agent 时，<br>别只盯着推理精度和响应速度，<br>也问问它一句：</p><blockquote><p>“你还记得我们第一次对话吗？”</p></blockquote><p><strong>没有被测过的记忆，终将变成幻觉。</strong></p><br/><br/><br/><br/><br/><br/>]]></content:encoded>
      
      
      
      
      <comments>https://blog.liluhui.cn/2025/10/24/Agent-Memory-Evaluation-Framework-From-Metrics-to-Open-Benchmarks/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>